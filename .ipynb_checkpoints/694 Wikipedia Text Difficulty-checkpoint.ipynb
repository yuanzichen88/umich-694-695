{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import altair as alt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.1.2-cp38-cp38-win_amd64.whl (24.0 MB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-6.0.0-py3-none-any.whl (58 kB)\n",
      "Collecting Cython==0.29.23\n",
      "  Downloading Cython-0.29.23-cp38-cp38-win_amd64.whl (1.7 MB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\mryua\\anaconda3\\lib\\site-packages (from gensim) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\mryua\\anaconda3\\lib\\site-packages (from gensim) (1.18.5)\n",
      "Installing collected packages: smart-open, Cython, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.21\n",
      "    Uninstalling Cython-0.29.21:\n",
      "      Successfully uninstalled Cython-0.29.21\n",
      "Successfully installed Cython-0.29.23 gensim-4.1.2 smart-open-6.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mryua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED=694"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File WikiLarge_Train.csv does not exist: 'WikiLarge_Train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ba752c3c988a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'WikiLarge_Train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# the dataset label is well balanced\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File WikiLarge_Train.csv does not exist: 'WikiLarge_Train.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('WikiLarge_Train.csv')\n",
    "len(df[df['label']==1])/len(df) # the dataset label is well balanced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['original_text']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=10,stop_words='english',ngram_range=(1,2))\n",
    "X_train_transform = vectorizer.fit_transform(X_train)\n",
    "X_test_transform  = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<333414x57516 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4053454 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec5997cf2ab42a1ac6fba0f64eb508d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=333414.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7176982c33d3462893b119ad9e76621e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=83354.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_text_train=[]\n",
    "tokenized_text_test=[]\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "for text in tqdm(X_train):\n",
    "    tokens_in_text = word_tokenize(text)\n",
    "    tokens_in_text = [word for word in tokens_in_text if word.lower() not in stopWords]\n",
    "    tokenized_text_train.append(tokens_in_text)\n",
    "    \n",
    "for text in tqdm(X_test):\n",
    "    tokens_in_text = word_tokenize(text)\n",
    "    tokens_in_text = [word for word in tokens_in_text if word.lower() not in stopWords]\n",
    "    tokenized_text_test.append(tokens_in_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14257763, 24263135)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(vector_size=100,window=2,min_count=100,seed= RANDOM_SEED,workers=4)\n",
    "model.build_vocab(tokenized_text_train)\n",
    "model.train(tokenized_text_train,total_examples=model.corpus_count,epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dense_features(tokenized_text,word_vectors):\n",
    "    dense_list=[]\n",
    "    words=[]\n",
    "    for _ in tokenized_text: \n",
    "        words =[word for word in _ if word in word_vectors.key_to_index]\n",
    "        \n",
    "        if len(words) >0:\n",
    "            dense_list.append(np.mean(word_vectors[words],axis=0))\n",
    "            \n",
    "        else: \n",
    "            dense_list.append(np.zeros(word_vectors.vector_size))\n",
    "            \n",
    "    return np.array(dense_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wv = generate_dense_features(tokenized_text_train,word_vectors)\n",
    "X_test_wv = generate_dense_features(tokenized_text_test,word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Word's Difficulty Considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic english words\n",
    "dale_chall = pd.read_csv('dale_chall.txt',delimiter='\\t',header=None,names=['word'])\n",
    "dale = set(dale_chall['word'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concreteness rating\n",
    "concrete_df = pd.read_csv('Concreteness_ratings_Brysbaert_et_al_BRM.txt',delimiter='\\t')\n",
    "concreteset=(concrete_df['Word'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Alternative.spelling</th>\n",
       "      <th>Freq_pm</th>\n",
       "      <th>Dom_PoS_SUBTLEX</th>\n",
       "      <th>Nletters</th>\n",
       "      <th>Nphon</th>\n",
       "      <th>Nsyll</th>\n",
       "      <th>Lemma_highest_PoS</th>\n",
       "      <th>AoA_Kup</th>\n",
       "      <th>Perc_known</th>\n",
       "      <th>AoA_Kup_lem</th>\n",
       "      <th>Perc_known_lem</th>\n",
       "      <th>AoA_Bird_lem</th>\n",
       "      <th>AoA_Bristol_lem</th>\n",
       "      <th>AoA_Cort_lem</th>\n",
       "      <th>AoA_Schock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>20415.27</td>\n",
       "      <td>Article</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>2.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aardvark</td>\n",
       "      <td>aardvark</td>\n",
       "      <td>0.41</td>\n",
       "      <td>Noun</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>aardvark</td>\n",
       "      <td>9.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>9.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abacus</td>\n",
       "      <td>abacus</td>\n",
       "      <td>0.24</td>\n",
       "      <td>Noun</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>abacus</td>\n",
       "      <td>8.69</td>\n",
       "      <td>0.65</td>\n",
       "      <td>8.69</td>\n",
       "      <td>0.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abacuses</td>\n",
       "      <td>abacuses</td>\n",
       "      <td>0.02</td>\n",
       "      <td>Noun</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>abacus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.69</td>\n",
       "      <td>0.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abalone</td>\n",
       "      <td>abalone</td>\n",
       "      <td>0.51</td>\n",
       "      <td>Verb</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>abalone</td>\n",
       "      <td>12.23</td>\n",
       "      <td>0.72</td>\n",
       "      <td>12.23</td>\n",
       "      <td>0.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word Alternative.spelling   Freq_pm Dom_PoS_SUBTLEX  Nletters  Nphon  \\\n",
       "0         a                    a  20415.27         Article         1      1   \n",
       "1  aardvark             aardvark      0.41            Noun         8      7   \n",
       "2    abacus               abacus      0.24            Noun         6      6   \n",
       "3  abacuses             abacuses      0.02            Noun         8      9   \n",
       "4   abalone              abalone      0.51            Verb         7      7   \n",
       "\n",
       "   Nsyll Lemma_highest_PoS  AoA_Kup  Perc_known  AoA_Kup_lem  Perc_known_lem  \\\n",
       "0      1                 a     2.89        1.00         2.89            1.00   \n",
       "1      2          aardvark     9.89        1.00         9.89            1.00   \n",
       "2      3            abacus     8.69        0.65         8.69            0.65   \n",
       "3      4            abacus      NaN         NaN         8.69            0.65   \n",
       "4      4           abalone    12.23        0.72        12.23            0.72   \n",
       "\n",
       "   AoA_Bird_lem  AoA_Bristol_lem  AoA_Cort_lem  AoA_Schock  \n",
       "0          3.16              NaN           NaN         NaN  \n",
       "1           NaN              NaN           NaN         NaN  \n",
       "2           NaN              NaN           NaN         NaN  \n",
       "3           NaN              NaN           NaN         NaN  \n",
       "4           NaN              NaN           NaN         NaN  "
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AoA\n",
    "#Perc_known_lem, AoA_Kup_lem\n",
    "AoA = pd.read_csv('AoA_51715_words.csv',encoding = 'unicode_escape')\n",
    "AoA_set = set(AoA['Word'].values)\n",
    "AoA.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word = set(word_vectors.index_to_key) #around 6k words in the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2623"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_word.intersection(concreteset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-282-e7a08ad5c3a7>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['AoA_Kup_lem'][i] = 3\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "word_list = []\n",
    "for word in model_word: \n",
    "    word_list.append((word,lemmatizer.lemmatize(word.lower())))\n",
    "df = pd.DataFrame(word_list,columns=['Original','word'])\n",
    "df = df.merge(AoA,left_on='word',right_on='Word',how='left')\n",
    "df = df[['Original','word','Perc_known','AoA_Kup_lem']]\n",
    "word_not_matched = set(df[df['Perc_known'].isnull()].word.values)\n",
    "\n",
    "for i in range(len(df)):   \n",
    "    if df['word'][i][0] in set(('0','1','2','3','4','5','6','7','8','9')) or len(df['word'][i])==1:\n",
    "        df['AoA_Kup_lem'][i] = 3\n",
    "mean_value = df['AoA_Kup_lem'].mean()\n",
    "df['AoA_Kup_lem'].fillna(value=mean_value,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>word</th>\n",
       "      <th>Perc_known</th>\n",
       "      <th>AoA_Kup_lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>troops</td>\n",
       "      <td>troop</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weapon</td>\n",
       "      <td>weapon</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Original    word  Perc_known  AoA_Kup_lem\n",
       "0   troops   troop         1.0         8.35\n",
       "1   weapon  weapon         1.0         6.95"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.loc[df['Original']==['troops','weapons']]\n",
    "df[df['Original'].isin(['troops','weapon'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_perc_known(tokenized_text,df):\n",
    "    avg_perc_know=None\n",
    "    perc_know_list=[]\n",
    "    for _ in tokenized_text: \n",
    "        words =[word for word in _ if word in word_vectors.key_to_index]\n",
    "        \n",
    "        if len(words) >0:\n",
    "            avg_perc_know = np.mean(df[df['Original'].isin(words)]['AoA_Kup_lem'])\n",
    "            perc_know_list.append(avg_perc_know)\n",
    "        else: \n",
    "            \n",
    "            perc_know_list.append(0)\n",
    "            \n",
    "    return perc_know_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(X_train_wv)\n",
    "df_train['year'] = generate_perc_known(tokenized_text_train,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(X_test_wv)\n",
    "df_test['year'] = generate_perc_known(tokenized_text_test,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.311034</td>\n",
       "      <td>-0.044982</td>\n",
       "      <td>-0.188059</td>\n",
       "      <td>-0.036853</td>\n",
       "      <td>0.053964</td>\n",
       "      <td>-0.075969</td>\n",
       "      <td>-0.573179</td>\n",
       "      <td>0.142119</td>\n",
       "      <td>0.038524</td>\n",
       "      <td>0.011229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140362</td>\n",
       "      <td>0.229184</td>\n",
       "      <td>-0.046725</td>\n",
       "      <td>-0.456503</td>\n",
       "      <td>-0.757063</td>\n",
       "      <td>-0.267589</td>\n",
       "      <td>0.390128</td>\n",
       "      <td>0.055417</td>\n",
       "      <td>0.151433</td>\n",
       "      <td>5.156867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.100743</td>\n",
       "      <td>-0.236878</td>\n",
       "      <td>-0.186871</td>\n",
       "      <td>-0.475519</td>\n",
       "      <td>0.342335</td>\n",
       "      <td>0.070849</td>\n",
       "      <td>-0.316106</td>\n",
       "      <td>-0.209397</td>\n",
       "      <td>0.046328</td>\n",
       "      <td>0.050351</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.414860</td>\n",
       "      <td>-0.051380</td>\n",
       "      <td>-0.257673</td>\n",
       "      <td>0.067684</td>\n",
       "      <td>-0.184270</td>\n",
       "      <td>-0.094727</td>\n",
       "      <td>0.147250</td>\n",
       "      <td>0.273635</td>\n",
       "      <td>-0.004352</td>\n",
       "      <td>6.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.083303</td>\n",
       "      <td>-0.488813</td>\n",
       "      <td>0.342551</td>\n",
       "      <td>0.181661</td>\n",
       "      <td>-0.427759</td>\n",
       "      <td>-0.216358</td>\n",
       "      <td>-1.041624</td>\n",
       "      <td>-0.118173</td>\n",
       "      <td>-0.336361</td>\n",
       "      <td>-0.128601</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152526</td>\n",
       "      <td>0.411480</td>\n",
       "      <td>0.557212</td>\n",
       "      <td>-0.095473</td>\n",
       "      <td>-0.539461</td>\n",
       "      <td>-0.470343</td>\n",
       "      <td>-0.006639</td>\n",
       "      <td>0.140359</td>\n",
       "      <td>0.200645</td>\n",
       "      <td>7.175730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.037667</td>\n",
       "      <td>-0.479091</td>\n",
       "      <td>-0.312405</td>\n",
       "      <td>-0.369330</td>\n",
       "      <td>-0.456307</td>\n",
       "      <td>0.190385</td>\n",
       "      <td>0.258977</td>\n",
       "      <td>0.020776</td>\n",
       "      <td>-0.173443</td>\n",
       "      <td>0.177531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.201434</td>\n",
       "      <td>-0.380838</td>\n",
       "      <td>-0.132189</td>\n",
       "      <td>-0.206441</td>\n",
       "      <td>-0.454263</td>\n",
       "      <td>0.163056</td>\n",
       "      <td>-0.168300</td>\n",
       "      <td>0.022108</td>\n",
       "      <td>-0.615601</td>\n",
       "      <td>6.441667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.142891</td>\n",
       "      <td>-0.380996</td>\n",
       "      <td>-0.318780</td>\n",
       "      <td>-0.123600</td>\n",
       "      <td>-0.229757</td>\n",
       "      <td>0.281040</td>\n",
       "      <td>-0.462546</td>\n",
       "      <td>0.183915</td>\n",
       "      <td>-0.537223</td>\n",
       "      <td>0.394007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.420755</td>\n",
       "      <td>0.118566</td>\n",
       "      <td>-0.368935</td>\n",
       "      <td>-0.032998</td>\n",
       "      <td>-0.360825</td>\n",
       "      <td>0.115795</td>\n",
       "      <td>0.066207</td>\n",
       "      <td>0.037782</td>\n",
       "      <td>-0.053558</td>\n",
       "      <td>7.493333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83349</th>\n",
       "      <td>-0.119182</td>\n",
       "      <td>-0.004474</td>\n",
       "      <td>-0.032353</td>\n",
       "      <td>-0.331377</td>\n",
       "      <td>-0.243901</td>\n",
       "      <td>0.046180</td>\n",
       "      <td>-0.114272</td>\n",
       "      <td>-0.024914</td>\n",
       "      <td>-0.006346</td>\n",
       "      <td>0.419893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.159321</td>\n",
       "      <td>-0.389408</td>\n",
       "      <td>-0.268235</td>\n",
       "      <td>-0.397462</td>\n",
       "      <td>-0.291322</td>\n",
       "      <td>0.215912</td>\n",
       "      <td>0.134387</td>\n",
       "      <td>0.355377</td>\n",
       "      <td>-0.154329</td>\n",
       "      <td>7.422743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83350</th>\n",
       "      <td>-0.088119</td>\n",
       "      <td>-0.660035</td>\n",
       "      <td>-0.288880</td>\n",
       "      <td>-0.008632</td>\n",
       "      <td>0.287710</td>\n",
       "      <td>-0.047573</td>\n",
       "      <td>-0.319038</td>\n",
       "      <td>-0.172171</td>\n",
       "      <td>-0.037463</td>\n",
       "      <td>0.367047</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007213</td>\n",
       "      <td>0.120631</td>\n",
       "      <td>-0.100991</td>\n",
       "      <td>-0.154148</td>\n",
       "      <td>-0.306328</td>\n",
       "      <td>0.017333</td>\n",
       "      <td>0.272786</td>\n",
       "      <td>0.166727</td>\n",
       "      <td>-0.075412</td>\n",
       "      <td>7.026164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83351</th>\n",
       "      <td>-0.088393</td>\n",
       "      <td>-0.268005</td>\n",
       "      <td>-0.301353</td>\n",
       "      <td>-0.239260</td>\n",
       "      <td>-0.152188</td>\n",
       "      <td>-0.250156</td>\n",
       "      <td>-0.128421</td>\n",
       "      <td>-0.140322</td>\n",
       "      <td>0.039296</td>\n",
       "      <td>0.029445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025348</td>\n",
       "      <td>-0.259513</td>\n",
       "      <td>-0.283999</td>\n",
       "      <td>-0.144472</td>\n",
       "      <td>-0.217558</td>\n",
       "      <td>0.018160</td>\n",
       "      <td>0.017978</td>\n",
       "      <td>0.210068</td>\n",
       "      <td>0.010174</td>\n",
       "      <td>6.400714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83352</th>\n",
       "      <td>0.021214</td>\n",
       "      <td>-0.067990</td>\n",
       "      <td>-0.190103</td>\n",
       "      <td>0.006632</td>\n",
       "      <td>-0.135657</td>\n",
       "      <td>0.150053</td>\n",
       "      <td>-0.263083</td>\n",
       "      <td>-0.235765</td>\n",
       "      <td>-0.327293</td>\n",
       "      <td>0.037924</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059949</td>\n",
       "      <td>0.160235</td>\n",
       "      <td>-0.085067</td>\n",
       "      <td>-0.065448</td>\n",
       "      <td>-0.389915</td>\n",
       "      <td>-0.297451</td>\n",
       "      <td>0.177708</td>\n",
       "      <td>-0.012893</td>\n",
       "      <td>-0.207058</td>\n",
       "      <td>5.905209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83353</th>\n",
       "      <td>0.345428</td>\n",
       "      <td>0.452051</td>\n",
       "      <td>-0.229750</td>\n",
       "      <td>-0.064214</td>\n",
       "      <td>-0.499776</td>\n",
       "      <td>-0.231279</td>\n",
       "      <td>0.188128</td>\n",
       "      <td>1.074919</td>\n",
       "      <td>0.261664</td>\n",
       "      <td>1.293295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.747728</td>\n",
       "      <td>0.470486</td>\n",
       "      <td>-0.162657</td>\n",
       "      <td>-0.528358</td>\n",
       "      <td>-0.231820</td>\n",
       "      <td>-0.360472</td>\n",
       "      <td>0.119667</td>\n",
       "      <td>1.069931</td>\n",
       "      <td>-0.259268</td>\n",
       "      <td>6.668391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83354 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0      0.311034 -0.044982 -0.188059 -0.036853  0.053964 -0.075969 -0.573179   \n",
       "1     -0.100743 -0.236878 -0.186871 -0.475519  0.342335  0.070849 -0.316106   \n",
       "2      0.083303 -0.488813  0.342551  0.181661 -0.427759 -0.216358 -1.041624   \n",
       "3     -0.037667 -0.479091 -0.312405 -0.369330 -0.456307  0.190385  0.258977   \n",
       "4      0.142891 -0.380996 -0.318780 -0.123600 -0.229757  0.281040 -0.462546   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "83349 -0.119182 -0.004474 -0.032353 -0.331377 -0.243901  0.046180 -0.114272   \n",
       "83350 -0.088119 -0.660035 -0.288880 -0.008632  0.287710 -0.047573 -0.319038   \n",
       "83351 -0.088393 -0.268005 -0.301353 -0.239260 -0.152188 -0.250156 -0.128421   \n",
       "83352  0.021214 -0.067990 -0.190103  0.006632 -0.135657  0.150053 -0.263083   \n",
       "83353  0.345428  0.452051 -0.229750 -0.064214 -0.499776 -0.231279  0.188128   \n",
       "\n",
       "              7         8         9  ...        91        92        93  \\\n",
       "0      0.142119  0.038524  0.011229  ...  0.140362  0.229184 -0.046725   \n",
       "1     -0.209397  0.046328  0.050351  ... -0.414860 -0.051380 -0.257673   \n",
       "2     -0.118173 -0.336361 -0.128601  ...  0.152526  0.411480  0.557212   \n",
       "3      0.020776 -0.173443  0.177531  ...  0.201434 -0.380838 -0.132189   \n",
       "4      0.183915 -0.537223  0.394007  ...  0.420755  0.118566 -0.368935   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "83349 -0.024914 -0.006346  0.419893  ...  0.159321 -0.389408 -0.268235   \n",
       "83350 -0.172171 -0.037463  0.367047  ... -0.007213  0.120631 -0.100991   \n",
       "83351 -0.140322  0.039296  0.029445  ...  0.025348 -0.259513 -0.283999   \n",
       "83352 -0.235765 -0.327293  0.037924  ... -0.059949  0.160235 -0.085067   \n",
       "83353  1.074919  0.261664  1.293295  ...  0.747728  0.470486 -0.162657   \n",
       "\n",
       "             94        95        96        97        98        99      year  \n",
       "0     -0.456503 -0.757063 -0.267589  0.390128  0.055417  0.151433  5.156867  \n",
       "1      0.067684 -0.184270 -0.094727  0.147250  0.273635 -0.004352  6.040000  \n",
       "2     -0.095473 -0.539461 -0.470343 -0.006639  0.140359  0.200645  7.175730  \n",
       "3     -0.206441 -0.454263  0.163056 -0.168300  0.022108 -0.615601  6.441667  \n",
       "4     -0.032998 -0.360825  0.115795  0.066207  0.037782 -0.053558  7.493333  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "83349 -0.397462 -0.291322  0.215912  0.134387  0.355377 -0.154329  7.422743  \n",
       "83350 -0.154148 -0.306328  0.017333  0.272786  0.166727 -0.075412  7.026164  \n",
       "83351 -0.144472 -0.217558  0.018160  0.017978  0.210068  0.010174  6.400714  \n",
       "83352 -0.065448 -0.389915 -0.297451  0.177708 -0.012893 -0.207058  5.905209  \n",
       "83353 -0.528358 -0.231820 -0.360472  0.119667  1.069931 -0.259268  6.668391  \n",
       "\n",
       "[83354 rows x 101 columns]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=RANDOM_SEED,max_iter=1000).fit(df_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6353384360678551"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,lr.predict(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_bow = DummyClassifier(strategy='uniform',random_state=RANDOM_SEED).fit(X_train_transform,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5011277203253593"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, dummy_bow.predict(X_test_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_wv = DummyClassifier(strategy='uniform',random_state=RANDOM_SEED).fit(X_train_wv,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5011277203253593"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,dummy_wv.predict(X_test_wv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_bow = LogisticRegression(random_state=RANDOM_SEED,max_iter=1000).fit(X_train_transform,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6847301869136454"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,lr_bow.predict(X_test_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_wv = LogisticRegression(random_state=RANDOM_SEED,max_iter=1000).fit(X_train_wv,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6249130215706504"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,lr_wv.predict(X_test_wv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_bow = RandomForestClassifier(n_estimators=500,max_depth=5,random_state=RANDOM_SEED).fit(X_train_transform,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6215898457182619"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,rf_bow.predict(X_test_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_wv = RandomForestClassifier(n_estimators=100,max_depth=5,random_state=RANDOM_SEED).fit(X_train_wv,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6205940926650191"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,rf_wv.predict(X_test_wv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2,random_state=RANDOM_SEED).fit(X_train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>y_label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>304501</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1979-80 Buffalo Sabres NHL 32 1880 74 1 4 2.36...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162313</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Diseases Lentils in culture Lentils are mentio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336845</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Railroads , like the Lehigh Valley Railroad , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150625</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>An example of this would be an individual anim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40240</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Both the Matanuska and Susitna Rivers have maj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259178</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>After the Germans invaded Norway in April 1940...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365838</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>July 28 - Henry Bennet , 1st Earl of Arlington...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131932</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Pancake restaurants are popular family restaur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146867</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A cycling domestique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121958</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>David Boreanaz 's first paid acting appearance...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>333414 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cluster  y_label                                               text\n",
       "304501        0        0  1979-80 Buffalo Sabres NHL 32 1880 74 1 4 2.36...\n",
       "162313        0        1  Diseases Lentils in culture Lentils are mentio...\n",
       "336845        0        0  Railroads , like the Lehigh Valley Railroad , ...\n",
       "150625        0        1  An example of this would be an individual anim...\n",
       "40240         0        1  Both the Matanuska and Susitna Rivers have maj...\n",
       "...         ...      ...                                                ...\n",
       "259178        0        0  After the Germans invaded Norway in April 1940...\n",
       "365838        1        0  July 28 - Henry Bennet , 1st Earl of Arlington...\n",
       "131932        0        1  Pancake restaurants are popular family restaur...\n",
       "146867        0        1                               A cycling domestique\n",
       "121958        0        1  David Boreanaz 's first paid acting appearance...\n",
       "\n",
       "[333414 rows x 3 columns]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_df = pd.DataFrame({'cluster':kmeans.labels_,'y_label':y_train,'text':X_train})\n",
    "cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=2, random_state=694)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
