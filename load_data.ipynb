{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import altair as alt\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.18.5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make sure numpy version is < 1.20\n",
    "np.version.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\mryua\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\mryua\\anaconda3\\lib\\site-packages (from gensim) (1.5.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\mryua\\anaconda3\\lib\\site-packages (from gensim) (6.0.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\mryua\\anaconda3\\lib\\site-packages (from gensim) (1.18.5)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\mryua\\anaconda3\\lib\\site-packages (from gensim) (0.29.23)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mryua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mryua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mryua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED=694"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There is manuscript evidence that Austen conti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In a remarkable comparative analysis , Mandaea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Before Persephone was released to Hermes , who...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cogeneration plants are commonly found in dist...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Geneva -LRB- , ; , ; , ; ; -RRB- is the second...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416763</th>\n",
       "      <td>A Duke Nukem 3D version has been sold for Xbox...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416764</th>\n",
       "      <td>However , it is becoming replaced as a method ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416765</th>\n",
       "      <td>There are hand gestures in both Hindu and Budd...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416766</th>\n",
       "      <td>If it is necessary to use colors , try to choo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416767</th>\n",
       "      <td>Calgary Stampeders ,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>416768 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            original_text  label\n",
       "0       There is manuscript evidence that Austen conti...      1\n",
       "1       In a remarkable comparative analysis , Mandaea...      1\n",
       "2       Before Persephone was released to Hermes , who...      1\n",
       "3       Cogeneration plants are commonly found in dist...      1\n",
       "4       Geneva -LRB- , ; , ; , ; ; -RRB- is the second...      1\n",
       "...                                                   ...    ...\n",
       "416763  A Duke Nukem 3D version has been sold for Xbox...      0\n",
       "416764  However , it is becoming replaced as a method ...      0\n",
       "416765  There are hand gestures in both Hindu and Budd...      0\n",
       "416766  If it is necessary to use colors , try to choo...      0\n",
       "416767                               Calgary Stampeders ,      0\n",
       "\n",
       "[416768 rows x 2 columns]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = 'Data/WikiLarge_Train.csv'\n",
    "df = pd.read_csv(train_path, skiprows=0, skipfooter=0, engine='python')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['label']==1])/len(df) # the dataset label is well balanced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He studied in Armenia and Istanbul , then at Wisconsin University which he finished in 1915 .'"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[50]['original_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117.921906192414"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['original_text'].apply(lambda x: len(x)).mean()\n",
    "# This means all texts are considered short text, which allows us to use dense representations, \n",
    "# as dense representations work well with short text.\n",
    "# Gensim.KeyedVectors.load('assets/wikipedia.100.word-vecs.kv')??? How to generate and use this???\n",
    "# Maybe we should train word2vec model on the entire corpus. Just training data? TOP 100 word-vectors(features)\n",
    "# Alternatively we could use bag-of-words model, which is term-document matrix representation, having much more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['original_text']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-2011</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-2011</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-2000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1997</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.636</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119087</th>\n",
       "      <td>119087</td>\n",
       "      <td>#NAME?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119088</th>\n",
       "      <td>119088</td>\n",
       "      <td>#NAME?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119089</th>\n",
       "      <td>119089</td>\n",
       "      <td>#NAME?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119090</th>\n",
       "      <td>119090</td>\n",
       "      <td>#NAME?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119091</th>\n",
       "      <td>119091</td>\n",
       "      <td>#NAME?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119092 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id original_text  label\n",
       "0            0         -2011    NaN\n",
       "1            1         -2011    NaN\n",
       "2            2         -2000    NaN\n",
       "3            3         -1997    NaN\n",
       "4            4         1.636    NaN\n",
       "...        ...           ...    ...\n",
       "119087  119087        #NAME?    NaN\n",
       "119088  119088        #NAME?    NaN\n",
       "119089  119089        #NAME?    NaN\n",
       "119090  119090        #NAME?    NaN\n",
       "119091  119091        #NAME?    NaN\n",
       "\n",
       "[119092 rows x 3 columns]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_path = 'Data/WikiLarge_Test.csv'\n",
    "test_df = pd.read_csv(test_path, skiprows=0, skipfooter=0, engine='python')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                           10000\n",
       "original_text    An atheist would say that this argument proves...\n",
       "label                                                          NaN\n",
       "Name: 10000, dtype: object"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.iloc[10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119087</th>\n",
       "      <td>119087</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119088</th>\n",
       "      <td>119088</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119089</th>\n",
       "      <td>119089</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119090</th>\n",
       "      <td>119090</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119091</th>\n",
       "      <td>119091</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119092 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  label\n",
       "0            0      0\n",
       "1            1      0\n",
       "2            2      1\n",
       "3            3      1\n",
       "4            4      0\n",
       "...        ...    ...\n",
       "119087  119087      0\n",
       "119088  119088      1\n",
       "119089  119089      1\n",
       "119090  119090      1\n",
       "119091  119091      1\n",
       "\n",
       "[119092 rows x 2 columns]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samplesubmission_path = 'Data/sampleSubmission.csv'\n",
    "samplesubmission_df = pd.read_csv(samplesubmission_path, skiprows=0, skipfooter=0, engine='python')\n",
    "samplesubmission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, the dataframes we are working with are:\n",
    "\n",
    "dalechall_df, concreteness_df, aoawords_df, train_df, test_df, samplesubmission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=10,stop_words='english',ngram_range=(1,2))\n",
    "X_train_transform = vectorizer.fit_transform(X_train)\n",
    "X_test_transform  = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<333414x57773 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4071111 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "== dale_chall.txt ==\n",
    "\n",
    "This is the Dale Chall 3000 Word List, which is one definition of words that are considered \"basic\" English.\n",
    "\n",
    "A summary is at https://www.readabilityformulas.com/articles/dale-chall-readability-word-list.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic english words\n",
    "dalechall_path = 'Data/dale_chall.txt'\n",
    "dale_chall = pd.read_csv(dalechall_path,delimiter='\\t',header=None,names=['word'])\n",
    "dale = set(dale_chall['word'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2946"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 2946 words in dale can be combined with the nltk stopwords.\n",
    "### We could maybe assign an arbitrary score to each dale_chall word. - for reference only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english')) | dale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2986"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304501    1979-80 Buffalo Sabres NHL 32 1880 74 1 4 2.36...\n",
       "162313    Diseases Lentils in culture Lentils are mentio...\n",
       "336845    Railroads , like the Lehigh Valley Railroad , ...\n",
       "150625    An example of this would be an individual anim...\n",
       "40240     Both the Matanuska and Susitna Rivers have maj...\n",
       "                                ...                        \n",
       "259178    After the Germans invaded Norway in April 1940...\n",
       "365838    July 28 - Henry Bennet , 1st Earl of Arlington...\n",
       "131932    Pancake restaurants are popular family restaur...\n",
       "146867                                 A cycling domestique\n",
       "121958    David Boreanaz 's first paid acting appearance...\n",
       "Name: original_text, Length: 333414, dtype: object"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1979-80 Buffalo Sabres NHL 32 1880 74 1 4 2.36 20 8 4 0 0.000'"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[304501]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['buffalo', 'sabres', 'nhl']"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim.utils.simple_preprocess(X_train[304501])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gensim.parsing.preprocessing.STOPWORDS\n",
    "#stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text_train=[]\n",
    "tokenized_text_test=[]\n",
    "stopWords = set(stopwords.words('english')) | dale\n",
    "# This cell will run 4 minutes\n",
    "import gensim\n",
    "from nltk.stem.porter import *\n",
    "def lemmatize_stemming(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    #Un-hash next line to use stemming\n",
    "    #return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "    #Un-hash next line to NOT use stemming\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            #Un-hash next line to use lemmatization/stemming\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            #Un-hash next line to NOT use lemmatization/stemming\n",
    "            #result.append(token)\n",
    "            \n",
    "    return result\n",
    "\n",
    "tokenized_text_train = [preprocess(text) for text in X_train]\n",
    "tokenized_text_test=[preprocess(text) for text in X_test]\n",
    "\n",
    "#for text in tqdm(X_train):\n",
    "#    tokens_in_text = word_tokenize(text)\n",
    "#    tokens_in_text = [word for word in tokens_in_text if word.lower() not in stopWords]\n",
    "#    tokenized_text_train.append(tokens_in_text)\n",
    "    \n",
    "#for text in tqdm(X_test):\n",
    "#    tokens_in_text = word_tokenize(text)\n",
    "#    tokens_in_text = [word for word in tokens_in_text if word.lower() not in stopWords]\n",
    "#    tokenized_text_test.append(tokens_in_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_text_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11285460, 15511955)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(vector_size=100,window=2,min_count=100,seed= RANDOM_SEED,workers=4)\n",
    "model.build_vocab(tokenized_text_train)\n",
    "model.train(tokenized_text_train,total_examples=model.corpus_count,epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-297-1ce1b1396e88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mword_vectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mvocab\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    659\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 661\u001b[1;33m         raise AttributeError(\n\u001b[0m\u001b[0;32m    662\u001b[0m             \u001b[1;34m\"The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m             \u001b[1;34m\"Use KeyedVector's .key_to_index dict, .index_to_key list, and methods \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"
     ]
    }
   ],
   "source": [
    "word_vectors.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = word_vectors.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 0,\n",
       " 'bear': 1,\n",
       " 'france': 2,\n",
       " 'city': 3,\n",
       " 'know': 4,\n",
       " 'unite': 5,\n",
       " 'department': 6,\n",
       " 'play': 7,\n",
       " 'region': 8,\n",
       " 'commune': 9,\n",
       " 'time': 10,\n",
       " 'north': 11,\n",
       " 'world': 12,\n",
       " 'football': 13,\n",
       " 'american': 14,\n",
       " 'call': 15,\n",
       " 'include': 16,\n",
       " 'people': 17,\n",
       " 'south': 18,\n",
       " 'game': 19,\n",
       " 'work': 20,\n",
       " 'team': 21,\n",
       " 'national': 22,\n",
       " 'form': 23,\n",
       " 'county': 24,\n",
       " 'player': 25,\n",
       " 'district': 26,\n",
       " 'write': 27,\n",
       " 'release': 28,\n",
       " 'year': 29,\n",
       " 'years': 30,\n",
       " 'english': 31,\n",
       " 'name': 32,\n",
       " 'second': 33,\n",
       " 'live': 34,\n",
       " 'locate': 35,\n",
       " 'number': 36,\n",
       " 'west': 37,\n",
       " 'music': 38,\n",
       " 'band': 39,\n",
       " 'film': 40,\n",
       " 'area': 41,\n",
       " 'group': 42,\n",
       " 'record': 43,\n",
       " 'town': 44,\n",
       " 'university': 45,\n",
       " 'series': 46,\n",
       " 'later': 47,\n",
       " 'september': 48,\n",
       " 'season': 49,\n",
       " 'build': 50,\n",
       " 'british': 51,\n",
       " 'january': 52,\n",
       " 'base': 53,\n",
       " 'march': 54,\n",
       " 'calais': 55,\n",
       " 'east': 56,\n",
       " 'album': 57,\n",
       " 'school': 58,\n",
       " 'begin': 59,\n",
       " 'german': 60,\n",
       " 'october': 61,\n",
       " 'start': 62,\n",
       " 'july': 63,\n",
       " 'league': 64,\n",
       " 'take': 65,\n",
       " 'august': 66,\n",
       " 'book': 67,\n",
       " 'river': 68,\n",
       " 'june': 69,\n",
       " 'give': 70,\n",
       " 'mean': 71,\n",
       " 'place': 72,\n",
       " 'december': 73,\n",
       " 'april': 74,\n",
       " 'family': 75,\n",
       " 'november': 76,\n",
       " 'england': 77,\n",
       " 'like': 78,\n",
       " 'best': 79,\n",
       " 'february': 80,\n",
       " 'century': 81,\n",
       " 'northern': 82,\n",
       " 'follow': 83,\n",
       " 'award': 84,\n",
       " 'come': 85,\n",
       " 'early': 86,\n",
       " 'french': 87,\n",
       " 'history': 88,\n",
       " 'london': 89,\n",
       " 'king': 90,\n",
       " 'population': 91,\n",
       " 'refer': 92,\n",
       " 'leave': 93,\n",
       " 'capital': 94,\n",
       " 'john': 95,\n",
       " 'largest': 96,\n",
       " 'province': 97,\n",
       " 'western': 98,\n",
       " 'line': 99,\n",
       " 'term': 100,\n",
       " 'ndash': 101,\n",
       " 'create': 102,\n",
       " 'government': 103,\n",
       " 'rock': 104,\n",
       " 'small': 105,\n",
       " 'germany': 106,\n",
       " 'central': 107,\n",
       " 'main': 108,\n",
       " 'life': 109,\n",
       " 'major': 110,\n",
       " 'club': 111,\n",
       " 'usually': 112,\n",
       " 'president': 113,\n",
       " 'language': 114,\n",
       " 'die': 115,\n",
       " 'japanese': 116,\n",
       " 'word': 117,\n",
       " 'single': 118,\n",
       " 'water': 119,\n",
       " 'long': 120,\n",
       " 'international': 121,\n",
       " 'high': 122,\n",
       " 'star': 123,\n",
       " 'force': 124,\n",
       " 'island': 125,\n",
       " 'hold': 126,\n",
       " 'change': 127,\n",
       " 'large': 128,\n",
       " 'company': 129,\n",
       " 'produce': 130,\n",
       " 'party': 131,\n",
       " 'house': 132,\n",
       " 'type': 133,\n",
       " 'television': 134,\n",
       " 'near': 135,\n",
       " 'different': 136,\n",
       " 'species': 137,\n",
       " 'open': 138,\n",
       " 'say': 139,\n",
       " 'common': 140,\n",
       " 'great': 141,\n",
       " 'york': 142,\n",
       " 'home': 143,\n",
       " 'study': 144,\n",
       " 'church': 145,\n",
       " 'southern': 146,\n",
       " 'center': 147,\n",
       " 'saint': 148,\n",
       " 'general': 149,\n",
       " 'country': 150,\n",
       " 'feature': 151,\n",
       " 'million': 152,\n",
       " 'kingdom': 153,\n",
       " 'title': 154,\n",
       " 'power': 155,\n",
       " 'professional': 156,\n",
       " 'publish': 157,\n",
       " 'found': 158,\n",
       " 'go': 159,\n",
       " 'death': 160,\n",
       " 'municipality': 161,\n",
       " 'land': 162,\n",
       " 'member': 163,\n",
       " 'song': 164,\n",
       " 'move': 165,\n",
       " 'serve': 166,\n",
       " 'point': 167,\n",
       " 'currently': 168,\n",
       " 'character': 169,\n",
       " 'order': 170,\n",
       " 'station': 171,\n",
       " 'union': 172,\n",
       " 'australia': 173,\n",
       " 'list': 174,\n",
       " 'lead': 175,\n",
       " 'service': 176,\n",
       " 'america': 177,\n",
       " 'championship': 178,\n",
       " 'make': 179,\n",
       " 'japan': 180,\n",
       " 'appear': 181,\n",
       " 'modern': 182,\n",
       " 'famous': 183,\n",
       " 'develop': 184,\n",
       " 'body': 185,\n",
       " 'eastern': 186,\n",
       " 'father': 187,\n",
       " 'important': 188,\n",
       " 'members': 189,\n",
       " 'storm': 190,\n",
       " 'cause': 191,\n",
       " 'nord': 192,\n",
       " 'popular': 193,\n",
       " 'park': 194,\n",
       " 'video': 195,\n",
       " 'tropical': 196,\n",
       " 'right': 197,\n",
       " 'roman': 198,\n",
       " 'race': 199,\n",
       " 'republic': 200,\n",
       " 'show': 201,\n",
       " 'italian': 202,\n",
       " 'plant': 203,\n",
       " 'hurricane': 204,\n",
       " 'white': 205,\n",
       " 'period': 206,\n",
       " 'aisne': 207,\n",
       " 'europe': 208,\n",
       " 'college': 209,\n",
       " 'public': 210,\n",
       " 'black': 211,\n",
       " 'consider': 212,\n",
       " 'official': 213,\n",
       " 'contain': 214,\n",
       " 'head': 215,\n",
       " 'page': 216,\n",
       " 'pakistan': 217,\n",
       " 'design': 218,\n",
       " 'children': 219,\n",
       " 'example': 220,\n",
       " 'color': 221,\n",
       " 'countries': 222,\n",
       " 'singer': 223,\n",
       " 'spanish': 224,\n",
       " 'result': 225,\n",
       " 'jam': 226,\n",
       " 'grow': 227,\n",
       " 'reference': 228,\n",
       " 'short': 229,\n",
       " 'join': 230,\n",
       " 'help': 231,\n",
       " 'original': 232,\n",
       " 'program': 233,\n",
       " 'greek': 234,\n",
       " 'remain': 235,\n",
       " 'sell': 236,\n",
       " 'present': 237,\n",
       " 'european': 238,\n",
       " 'royal': 239,\n",
       " 'india': 240,\n",
       " 'movie': 241,\n",
       " 'rule': 242,\n",
       " 'match': 243,\n",
       " 'kill': 244,\n",
       " 'calvados': 245,\n",
       " 'total': 246,\n",
       " 'william': 247,\n",
       " 'level': 248,\n",
       " 'accord': 249,\n",
       " 'california': 250,\n",
       " 'career': 251,\n",
       " 'minister': 252,\n",
       " 'control': 253,\n",
       " 'date': 254,\n",
       " 'support': 255,\n",
       " 'basse': 256,\n",
       " 'normandie': 257,\n",
       " 'council': 258,\n",
       " 'battle': 259,\n",
       " 'allow': 260,\n",
       " 'person': 261,\n",
       " 'receive': 262,\n",
       " 'late': 263,\n",
       " 'sport': 264,\n",
       " 'switzerland': 265,\n",
       " 'perform': 266,\n",
       " 'free': 267,\n",
       " 'return': 268,\n",
       " 'current': 269,\n",
       " 'sign': 270,\n",
       " 'george': 271,\n",
       " 'hockey': 272,\n",
       " 'northwest': 273,\n",
       " 'pay': 274,\n",
       " 'train': 275,\n",
       " 'article': 276,\n",
       " 'canton': 277,\n",
       " 'commonly': 278,\n",
       " 'young': 279,\n",
       " 'border': 280,\n",
       " 'islands': 281,\n",
       " 'reach': 282,\n",
       " 'footballer': 283,\n",
       " 'local': 284,\n",
       " 'empire': 285,\n",
       " 'canada': 286,\n",
       " 'wrestle': 287,\n",
       " 'today': 288,\n",
       " 'earth': 289,\n",
       " 'division': 290,\n",
       " 'class': 291,\n",
       " 'think': 292,\n",
       " 'lake': 293,\n",
       " 'link': 294,\n",
       " 'cover': 295,\n",
       " 'establish': 296,\n",
       " 'range': 297,\n",
       " 'marry': 298,\n",
       " 'actor': 299,\n",
       " 'originally': 300,\n",
       " 'part': 301,\n",
       " 'role': 302,\n",
       " 'field': 303,\n",
       " 'charles': 304,\n",
       " 'continue': 305,\n",
       " 'mother': 306,\n",
       " 'army': 307,\n",
       " 'speak': 308,\n",
       " 'light': 309,\n",
       " 'metal': 310,\n",
       " 'political': 311,\n",
       " 'consist': 312,\n",
       " 'blue': 313,\n",
       " 'china': 314,\n",
       " 'version': 315,\n",
       " 'human': 316,\n",
       " 'similar': 317,\n",
       " 'days': 318,\n",
       " 'emperor': 319,\n",
       " 'pass': 320,\n",
       " 'provide': 321,\n",
       " 'areas': 322,\n",
       " 'queen': 323,\n",
       " 'process': 324,\n",
       " 'grand': 325,\n",
       " 'australian': 326,\n",
       " 'italy': 327,\n",
       " 'village': 328,\n",
       " 'direct': 329,\n",
       " 'coast': 330,\n",
       " 'lose': 331,\n",
       " 'final': 332,\n",
       " 'network': 333,\n",
       " 'close': 334,\n",
       " 'science': 335,\n",
       " 'have': 336,\n",
       " 'source': 337,\n",
       " 'loire': 338,\n",
       " 'size': 339,\n",
       " 'host': 340,\n",
       " 'describe': 341,\n",
       " 'association': 342,\n",
       " 'christian': 343,\n",
       " 'operate': 344,\n",
       " 'attack': 345,\n",
       " 'seat': 346,\n",
       " 'see': 347,\n",
       " 'henry': 348,\n",
       " 'ancient': 349,\n",
       " 'style': 350,\n",
       " 'end': 351,\n",
       " 'miles': 352,\n",
       " 'military': 353,\n",
       " 'story': 354,\n",
       " 'gironde': 355,\n",
       " 'wind': 356,\n",
       " 'women': 357,\n",
       " 'scotland': 358,\n",
       " 'turn': 359,\n",
       " 'event': 360,\n",
       " 'position': 361,\n",
       " 'events': 362,\n",
       " 'energy': 363,\n",
       " 'canadian': 364,\n",
       " 'middle': 365,\n",
       " 'road': 366,\n",
       " 'complete': 367,\n",
       " 'green': 368,\n",
       " 'discover': 369,\n",
       " 'radio': 370,\n",
       " 'ireland': 371,\n",
       " 'author': 372,\n",
       " 'africa': 373,\n",
       " 'picardie': 374,\n",
       " 'mark': 375,\n",
       " 'space': 376,\n",
       " 'note': 377,\n",
       " 'cross': 378,\n",
       " 'report': 379,\n",
       " 'champion': 380,\n",
       " 'languages': 381,\n",
       " 'defeat': 382,\n",
       " 'natural': 383,\n",
       " 'half': 384,\n",
       " 'railway': 385,\n",
       " 'paul': 386,\n",
       " 'leader': 387,\n",
       " 'separate': 388,\n",
       " 'replace': 389,\n",
       " 'love': 390,\n",
       " 'seven': 391,\n",
       " 'rise': 392,\n",
       " 'wife': 393,\n",
       " 'office': 394,\n",
       " 'prime': 395,\n",
       " 'florida': 396,\n",
       " 'aquitaine': 397,\n",
       " 'good': 398,\n",
       " 'brazilian': 399,\n",
       " 'calendar': 400,\n",
       " 'generally': 401,\n",
       " 'little': 402,\n",
       " 'atlantic': 403,\n",
       " 'tour': 404,\n",
       " 'formula': 405,\n",
       " 'development': 406,\n",
       " 'census': 407,\n",
       " 'latin': 408,\n",
       " 'exist': 409,\n",
       " 'track': 410,\n",
       " 'director': 411,\n",
       " 'david': 412,\n",
       " 'hall': 413,\n",
       " 'square': 414,\n",
       " 'fight': 415,\n",
       " 'robert': 416,\n",
       " 'food': 417,\n",
       " 'view': 418,\n",
       " 'tree': 419,\n",
       " 'believe': 420,\n",
       " 'court': 421,\n",
       " 'relate': 422,\n",
       " 'surface': 423,\n",
       " 'represent': 424,\n",
       " 'chinese': 425,\n",
       " 'case': 426,\n",
       " 'bank': 427,\n",
       " 'trade': 428,\n",
       " 'orchestra': 429,\n",
       " 'russian': 430,\n",
       " 'parliament': 431,\n",
       " 'hand': 432,\n",
       " 'stand': 433,\n",
       " 'writer': 434,\n",
       " 'project': 435,\n",
       " 'hill': 436,\n",
       " 'valley': 437,\n",
       " 'site': 438,\n",
       " 'ocean': 439,\n",
       " 'songs': 440,\n",
       " 'prize': 441,\n",
       " 'model': 442,\n",
       " 'special': 443,\n",
       " 'occur': 444,\n",
       " 'systems': 445,\n",
       " 'divide': 446,\n",
       " 'asia': 447,\n",
       " 'paris': 448,\n",
       " 'information': 449,\n",
       " 'image': 450,\n",
       " 'daughter': 451,\n",
       " 'letter': 452,\n",
       " 'southwest': 453,\n",
       " 'port': 454,\n",
       " 'brother': 455,\n",
       " 'moon': 456,\n",
       " 'chemical': 457,\n",
       " 'break': 458,\n",
       " 'tell': 459,\n",
       " 'need': 460,\n",
       " 'plan': 461,\n",
       " 'run': 462,\n",
       " 'software': 463,\n",
       " 'object': 464,\n",
       " 'win': 465,\n",
       " 'theory': 466,\n",
       " 'meet': 467,\n",
       " 'better': 468,\n",
       " 'fall': 469,\n",
       " 'highest': 470,\n",
       " 'cities': 471,\n",
       " 'organization': 472,\n",
       " 'actress': 473,\n",
       " 'bring': 474,\n",
       " 'night': 475,\n",
       " 'want': 476,\n",
       " 'movement': 477,\n",
       " 'elect': 478,\n",
       " 'scottish': 479,\n",
       " 'stadium': 480,\n",
       " 'increase': 481,\n",
       " 'composer': 482,\n",
       " 'summer': 483,\n",
       " 'fourth': 484,\n",
       " 'opera': 485,\n",
       " 'research': 486,\n",
       " 'ring': 487,\n",
       " 'indian': 488,\n",
       " 'share': 489,\n",
       " 'street': 490,\n",
       " 'novel': 491,\n",
       " 'culture': 492,\n",
       " 'use': 493,\n",
       " 'dutch': 494,\n",
       " 'studio': 495,\n",
       " 'berlin': 496,\n",
       " 'influence': 497,\n",
       " 'independent': 498,\n",
       " 'debut': 499,\n",
       " 'post': 500,\n",
       " 'officially': 501,\n",
       " 'entertainment': 502,\n",
       " 'players': 503,\n",
       " 'send': 504,\n",
       " 'richard': 505,\n",
       " 'community': 506,\n",
       " 'structure': 507,\n",
       " 'claim': 508,\n",
       " 'sound': 509,\n",
       " 'lower': 510,\n",
       " 'simply': 511,\n",
       " 'vote': 512,\n",
       " 'introduce': 513,\n",
       " 'spain': 514,\n",
       " 'ship': 515,\n",
       " 'social': 516,\n",
       " 'issue': 517,\n",
       " 'involve': 518,\n",
       " 'prince': 519,\n",
       " 'market': 520,\n",
       " 'catholic': 521,\n",
       " 'instead': 522,\n",
       " 'iowa': 523,\n",
       " 'federal': 524,\n",
       " 'drive': 525,\n",
       " 'sing': 526,\n",
       " 'animals': 527,\n",
       " 'especially': 528,\n",
       " 'northwestern': 529,\n",
       " 'scale': 530,\n",
       " 'disney': 531,\n",
       " 'genus': 532,\n",
       " 'michael': 533,\n",
       " 'thomas': 534,\n",
       " 'duke': 535,\n",
       " 'count': 536,\n",
       " 'retire': 537,\n",
       " 'guitar': 538,\n",
       " 'paint': 539,\n",
       " 'soviet': 540,\n",
       " 'away': 541,\n",
       " 'act': 542,\n",
       " 'brand': 543,\n",
       " 'peter': 544,\n",
       " 'wales': 545,\n",
       " 'territory': 546,\n",
       " 'traditional': 547,\n",
       " 'mass': 548,\n",
       " 'bird': 549,\n",
       " 'add': 550,\n",
       " 'britain': 551,\n",
       " 'enter': 552,\n",
       " 'stop': 553,\n",
       " 'airport': 554,\n",
       " 'mountain': 555,\n",
       " 'production': 556,\n",
       " 'travel': 557,\n",
       " 'voice': 558,\n",
       " 'blood': 559,\n",
       " 'fish': 560,\n",
       " 'wikipedia': 561,\n",
       " 'look': 562,\n",
       " 'approximately': 563,\n",
       " 'stage': 564,\n",
       " 'native': 565,\n",
       " 'child': 566,\n",
       " 'announce': 567,\n",
       " 'louis': 568,\n",
       " 'effect': 569,\n",
       " 'stone': 570,\n",
       " 'length': 571,\n",
       " 'shape': 572,\n",
       " 'gold': 573,\n",
       " 'bridge': 574,\n",
       " 'politician': 575,\n",
       " 'unit': 576,\n",
       " 'teach': 577,\n",
       " 'figure': 578,\n",
       " 'finish': 579,\n",
       " 'mexico': 580,\n",
       " 'standard': 581,\n",
       " 'civil': 582,\n",
       " 'engineer': 583,\n",
       " 'oldest': 584,\n",
       " 'define': 585,\n",
       " 'branch': 586,\n",
       " 'units': 587,\n",
       " 'available': 588,\n",
       " 'data': 589,\n",
       " 'winter': 590,\n",
       " 'damage': 591,\n",
       " 'society': 592,\n",
       " 'chart': 593,\n",
       " 'outside': 594,\n",
       " 'lord': 595,\n",
       " 'successful': 596,\n",
       " 'associate': 597,\n",
       " 'channel': 598,\n",
       " 'picture': 599,\n",
       " 'heat': 600,\n",
       " 'distance': 601,\n",
       " 'musical': 602,\n",
       " 'decide': 603,\n",
       " 'washington': 604,\n",
       " 'governor': 605,\n",
       " 'rank': 606,\n",
       " 'academy': 607,\n",
       " 'action': 608,\n",
       " 'read': 609,\n",
       " 'limit': 610,\n",
       " 'round': 611,\n",
       " 'lie': 612,\n",
       " 'minor': 613,\n",
       " 'pacific': 614,\n",
       " 'super': 615,\n",
       " 'virginia': 616,\n",
       " 'grind': 617,\n",
       " 'real': 618,\n",
       " 'orbit': 619,\n",
       " 'average': 620,\n",
       " 'upper': 621,\n",
       " 'edward': 622,\n",
       " 'require': 623,\n",
       " 'piece': 624,\n",
       " 'draw': 625,\n",
       " 'strong': 626,\n",
       " 'instrument': 627,\n",
       " 'episode': 628,\n",
       " 'copy': 629,\n",
       " 'education': 630,\n",
       " 'choose': 631,\n",
       " 'flow': 632,\n",
       " 'brown': 633,\n",
       " 'websites': 634,\n",
       " 'police': 635,\n",
       " 'chief': 636,\n",
       " 'try': 637,\n",
       " 'manager': 638,\n",
       " 'smaller': 639,\n",
       " 'windows': 640,\n",
       " 'section': 641,\n",
       " 'chicago': 642,\n",
       " 'premier': 643,\n",
       " 'secretary': 644,\n",
       " 'martin': 645,\n",
       " 'artist': 646,\n",
       " 'mainly': 647,\n",
       " 'dance': 648,\n",
       " 'baseball': 649,\n",
       " 'score': 650,\n",
       " 'russia': 651,\n",
       " 'news': 652,\n",
       " 'dynasty': 653,\n",
       " 'attempt': 654,\n",
       " 'larger': 655,\n",
       " 'function': 656,\n",
       " 'estimate': 657,\n",
       " 'texas': 658,\n",
       " 'mountains': 659,\n",
       " 'parent': 660,\n",
       " 'mary': 661,\n",
       " 'store': 662,\n",
       " 'arts': 663,\n",
       " 'flower': 664,\n",
       " 'angeles': 665,\n",
       " 'business': 666,\n",
       " 'wear': 667,\n",
       " 'compose': 668,\n",
       " 'label': 669,\n",
       " 'wall': 670,\n",
       " 'soon': 671,\n",
       " 'cathedral': 672,\n",
       " 'carry': 673,\n",
       " 'nintendo': 674,\n",
       " 'festival': 675,\n",
       " 'performance': 676,\n",
       " 'administrative': 677,\n",
       " 'peak': 678,\n",
       " 'victoria': 679,\n",
       " 'albums': 680,\n",
       " 'fiction': 681,\n",
       " 'woman': 682,\n",
       " 'metropolitan': 683,\n",
       " 'learn': 684,\n",
       " 'students': 685,\n",
       " 'connect': 686,\n",
       " 'nations': 687,\n",
       " 'alpes': 688,\n",
       " 'irish': 689,\n",
       " 'partement': 690,\n",
       " 'magazine': 691,\n",
       " 'nuclear': 692,\n",
       " 'forest': 693,\n",
       " 'producer': 694,\n",
       " 'success': 695,\n",
       " 'flag': 696,\n",
       " 'wide': 697,\n",
       " 'ball': 698,\n",
       " 'conductor': 699,\n",
       " 'display': 700,\n",
       " 'face': 701,\n",
       " 'months': 702,\n",
       " 'typically': 703,\n",
       " 'cell': 704,\n",
       " 'measure': 705,\n",
       " 'surround': 706,\n",
       " 'female': 707,\n",
       " 'centre': 708,\n",
       " 'beach': 709,\n",
       " 'board': 710,\n",
       " 'own': 711,\n",
       " 'recognize': 712,\n",
       " 'material': 713,\n",
       " 'block': 714,\n",
       " 'value': 715,\n",
       " 'certain': 716,\n",
       " 'nobel': 717,\n",
       " 'able': 718,\n",
       " 'talk': 719,\n",
       " 'addition': 720,\n",
       " 'conduct': 721,\n",
       " 'african': 722,\n",
       " 'reason': 723,\n",
       " 'speed': 724,\n",
       " 'destroy': 725,\n",
       " 'animal': 726,\n",
       " 'austria': 727,\n",
       " 'electric': 728,\n",
       " 'arm': 729,\n",
       " 'peace': 730,\n",
       " 'derive': 731,\n",
       " 'museum': 732,\n",
       " 'text': 733,\n",
       " 'birth': 734,\n",
       " 'brazil': 735,\n",
       " 'castle': 736,\n",
       " 'money': 737,\n",
       " 'dark': 738,\n",
       " 'things': 739,\n",
       " 'coach': 740,\n",
       " 'greater': 741,\n",
       " 'concert': 742,\n",
       " 'rail': 743,\n",
       " 'suggest': 744,\n",
       " 'religious': 745,\n",
       " 'spring': 746,\n",
       " 'widely': 747,\n",
       " 'heavy': 748,\n",
       " 'cells': 749,\n",
       " 'pope': 750,\n",
       " 'wing': 751,\n",
       " 'belong': 752,\n",
       " 'edition': 753,\n",
       " 'election': 754,\n",
       " 'engine': 755,\n",
       " 'sister': 756,\n",
       " 'hard': 757,\n",
       " 'visit': 758,\n",
       " 'physical': 759,\n",
       " 'institute': 760,\n",
       " 'offer': 761,\n",
       " 'test': 762,\n",
       " 'netherlands': 763,\n",
       " 'compound': 764,\n",
       " 'belgian': 765,\n",
       " 'pressure': 766,\n",
       " 'purpose': 767,\n",
       " 'higher': 768,\n",
       " 'oklahoma': 769,\n",
       " 'independence': 770,\n",
       " 'garden': 771,\n",
       " 'wave': 772,\n",
       " 'eventually': 773,\n",
       " 'mario': 774,\n",
       " 'practice': 775,\n",
       " 'picardy': 776,\n",
       " 'lady': 777,\n",
       " 'primary': 778,\n",
       " 'charge': 779,\n",
       " 'honor': 780,\n",
       " 'camp': 781,\n",
       " 'golden': 782,\n",
       " 'press': 783,\n",
       " 'officer': 784,\n",
       " 'spend': 785,\n",
       " 'borough': 786,\n",
       " 'mount': 787,\n",
       " 'condition': 788,\n",
       " 'combine': 789,\n",
       " 'gain': 790,\n",
       " 'private': 791,\n",
       " 'media': 792,\n",
       " 'extend': 793,\n",
       " 'romania': 794,\n",
       " 'guitarist': 795,\n",
       " 'zealand': 796,\n",
       " 'musician': 797,\n",
       " 'double': 798,\n",
       " 'joseph': 799,\n",
       " 'code': 800,\n",
       " 'treaty': 801,\n",
       " 'symbol': 802,\n",
       " 'situate': 803,\n",
       " 'jupiter': 804,\n",
       " 'rest': 805,\n",
       " 'category': 806,\n",
       " 'spell': 807,\n",
       " 'possible': 808,\n",
       " 'professor': 809,\n",
       " 'comedy': 810,\n",
       " 'towns': 811,\n",
       " 'raise': 812,\n",
       " 'heart': 813,\n",
       " 'sarthe': 814,\n",
       " 'survive': 815,\n",
       " 'doctor': 816,\n",
       " 'website': 817,\n",
       " 'holy': 818,\n",
       " 'string': 819,\n",
       " 'illinois': 820,\n",
       " 'songwriter': 821,\n",
       " 'attend': 822,\n",
       " 'paper': 823,\n",
       " 'declare': 824,\n",
       " 'worldwide': 825,\n",
       " 'particularly': 826,\n",
       " 'nature': 827,\n",
       " 'broadcast': 828,\n",
       " 'subject': 829,\n",
       " 'status': 830,\n",
       " 'active': 831,\n",
       " 'regions': 832,\n",
       " 'air': 833,\n",
       " 'internet': 834,\n",
       " 'machine': 835,\n",
       " 'economic': 836,\n",
       " 'historical': 837,\n",
       " 'location': 838,\n",
       " 'master': 839,\n",
       " 'humans': 840,\n",
       " 'fifth': 841,\n",
       " 'apply': 842,\n",
       " 'inside': 843,\n",
       " 'animate': 844,\n",
       " 'dead': 845,\n",
       " 'cold': 846,\n",
       " 'nickname': 847,\n",
       " 'temperature': 848,\n",
       " 'austrian': 849,\n",
       " 'card': 850,\n",
       " 'classical': 851,\n",
       " 'democratic': 852,\n",
       " 'roll': 853,\n",
       " 'manchester': 854,\n",
       " 'multiple': 855,\n",
       " 'stories': 856,\n",
       " 'elements': 857,\n",
       " 'yellow': 858,\n",
       " 'planet': 859,\n",
       " 'alternative': 860,\n",
       " 'particular': 861,\n",
       " 'bass': 862,\n",
       " 'happen': 863,\n",
       " 'industry': 864,\n",
       " 'poland': 865,\n",
       " 'urban': 866,\n",
       " 'health': 867,\n",
       " 'revolution': 868,\n",
       " 'jesus': 869,\n",
       " 'olympic': 870,\n",
       " 'harry': 871,\n",
       " 'draft': 872,\n",
       " 'appoint': 873,\n",
       " 'nation': 874,\n",
       " 'jewish': 875,\n",
       " 'feet': 876,\n",
       " 'primarily': 877,\n",
       " 'experience': 878,\n",
       " 'access': 879,\n",
       " 'future': 880,\n",
       " 'launch': 881,\n",
       " 'elizabeth': 882,\n",
       " 'nearly': 883,\n",
       " 'carolina': 884,\n",
       " 'individual': 885,\n",
       " 'adopt': 886,\n",
       " 'foreign': 887,\n",
       " 'prix': 888,\n",
       " 'rat': 889,\n",
       " 'chess': 890,\n",
       " 'recent': 891,\n",
       " 'true': 892,\n",
       " 'friend': 893,\n",
       " 'regular': 894,\n",
       " 'korea': 895,\n",
       " 'statistics': 896,\n",
       " 'file': 897,\n",
       " 'older': 898,\n",
       " 'advance': 899,\n",
       " 'depression': 900,\n",
       " 'captain': 901,\n",
       " 'capture': 902,\n",
       " 'graduate': 903,\n",
       " 'piano': 904,\n",
       " 'keep': 905,\n",
       " 'literature': 906,\n",
       " 'disease': 907,\n",
       " 'zone': 908,\n",
       " 'shoot': 909,\n",
       " 'founder': 910,\n",
       " 'complex': 911,\n",
       " 'account': 912,\n",
       " 'inhabitants': 913,\n",
       " 'southeast': 914,\n",
       " 'peninsula': 915,\n",
       " 'earlier': 916,\n",
       " 'ask': 917,\n",
       " 'commercial': 918,\n",
       " 'medical': 919,\n",
       " 'basketball': 920,\n",
       " 'greatest': 921,\n",
       " 'user': 922,\n",
       " 'conference': 923,\n",
       " 'despite': 924,\n",
       " 'poet': 925,\n",
       " 'protect': 926,\n",
       " 'age': 927,\n",
       " 'license': 928,\n",
       " 'simple': 929,\n",
       " 'southeastern': 930,\n",
       " 'previously': 931,\n",
       " 'solar': 932,\n",
       " 'fame': 933,\n",
       " 'soldier': 934,\n",
       " 'degree': 935,\n",
       " 'rome': 936,\n",
       " 'egypt': 937,\n",
       " 'cook': 938,\n",
       " 'southwestern': 939,\n",
       " 'male': 940,\n",
       " 'succeed': 941,\n",
       " 'hours': 942,\n",
       " 'directly': 943,\n",
       " 'swedish': 944,\n",
       " 'collection': 945,\n",
       " 'horse': 946,\n",
       " 'iron': 947,\n",
       " 'variety': 948,\n",
       " 'library': 949,\n",
       " 'principal': 950,\n",
       " 'crown': 951,\n",
       " 'probably': 952,\n",
       " 'brothers': 953,\n",
       " 'digital': 954,\n",
       " 'armenian': 955,\n",
       " 'korean': 956,\n",
       " 'discovery': 957,\n",
       " 'print': 958,\n",
       " 'previous': 959,\n",
       " 'construction': 960,\n",
       " 'accept': 961,\n",
       " 'notable': 962,\n",
       " 'alexander': 963,\n",
       " 'table': 964,\n",
       " 'santa': 965,\n",
       " 'friends': 966,\n",
       " 'weather': 967,\n",
       " 'competition': 968,\n",
       " 'medicine': 969,\n",
       " 'theme': 970,\n",
       " 'idea': 971,\n",
       " 'personal': 972,\n",
       " 'war': 973,\n",
       " 'past': 974,\n",
       " 'longer': 975,\n",
       " 'scott': 976,\n",
       " 'deal': 977,\n",
       " 'grant': 978,\n",
       " 'review': 979,\n",
       " 'remove': 980,\n",
       " 'drama': 981,\n",
       " 'vocals': 982,\n",
       " 'reduce': 983,\n",
       " 'focus': 984,\n",
       " 'comprise': 985,\n",
       " 'campaign': 986,\n",
       " 'challenge': 987,\n",
       " 'circuit': 988,\n",
       " 'biggest': 989,\n",
       " 'rename': 990,\n",
       " 'stay': 991,\n",
       " 'saturn': 992,\n",
       " 'organize': 993,\n",
       " 'beat': 994,\n",
       " 'counties': 995,\n",
       " 'compete': 996,\n",
       " 'foundation': 997,\n",
       " 'entire': 998,\n",
       " 'annual': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors[0] == word_vectors['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4294"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_vector = word_vectors.index_to_key\n",
    "len(words_in_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word's Difficulty Considered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "== Concreteness_ratings_Brysbaert_et_al_BRM.txt ==\n",
    "\n",
    "This file contains concreteness ratings for 40 thousand English lemma words gathered via Amazon Mechanical Turk. The ratings come from a larger list of 63 thousand words and represent all English words known to 85% of the raters.\n",
    "\n",
    "The file contains eight columns:\n",
    "1. The word\n",
    "2. Whether it is a single word or a two-word expression \n",
    "3. The mean concreteness rating\n",
    "4. The standard deviation of the concreteness ratings\n",
    "5. The number of persons indicating they did not know the word\n",
    "6. The total number of persons who rated the word\n",
    "7. Percentage participants who knew the word\n",
    "8. The SUBTLEX-US frequency count (on a total of 51 million; Brysbaert & New, 2009) \n",
    "9. The dominant part-of-speech usage\n",
    "\n",
    "Original source: http://crr.ugent.be/archives/1330\n",
    "\n",
    "Brysbaert, M., Warriner, A.B., & Kuperman, V. (2014). Concreteness ratings for 40 thousand generally known English word lemmas. Behavior Research Methods, 46, 904-911.\n",
    "http://crr.ugent.be/papers/Brysbaert_Warriner_Kuperman_BRM_Concreteness_ratings.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concreteness rating - the higher Conc.M, the easier the word is.\n",
    "concreteness_path = 'Data/Concreteness_ratings_Brysbaert_et_al_BRM.txt'\n",
    "concrete_df = pd.read_csv(concreteness_path,delimiter='\\t', keep_default_na=False)\n",
    "concreteset=(concrete_df['Word'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Bigram</th>\n",
       "      <th>Conc.M</th>\n",
       "      <th>Conc.SD</th>\n",
       "      <th>Unknown</th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent_known</th>\n",
       "      <th>SUBTLEX</th>\n",
       "      <th>Dom_Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roadsweeper</td>\n",
       "      <td>0</td>\n",
       "      <td>4.85</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>traindriver</td>\n",
       "      <td>0</td>\n",
       "      <td>4.54</td>\n",
       "      <td>0.71</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tush</td>\n",
       "      <td>0</td>\n",
       "      <td>4.45</td>\n",
       "      <td>1.01</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>0.88</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hairdress</td>\n",
       "      <td>0</td>\n",
       "      <td>3.93</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pharmaceutics</td>\n",
       "      <td>0</td>\n",
       "      <td>3.77</td>\n",
       "      <td>1.41</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39949</th>\n",
       "      <td>unenvied</td>\n",
       "      <td>0</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0</td>\n",
       "      <td>#N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39950</th>\n",
       "      <td>agnostically</td>\n",
       "      <td>0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0</td>\n",
       "      <td>#N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39951</th>\n",
       "      <td>conceptualistic</td>\n",
       "      <td>0</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.50</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0</td>\n",
       "      <td>#N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39952</th>\n",
       "      <td>conventionalism</td>\n",
       "      <td>0</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0</td>\n",
       "      <td>#N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39953</th>\n",
       "      <td>essentialness</td>\n",
       "      <td>0</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0</td>\n",
       "      <td>#N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39954 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Word  Bigram  Conc.M  Conc.SD  Unknown  Total  \\\n",
       "0          roadsweeper       0    4.85     0.37        1     27   \n",
       "1          traindriver       0    4.54     0.71        3     29   \n",
       "2                 tush       0    4.45     1.01        3     25   \n",
       "3            hairdress       0    3.93     1.28        0     29   \n",
       "4        pharmaceutics       0    3.77     1.41        4     26   \n",
       "...                ...     ...     ...      ...      ...    ...   \n",
       "39949         unenvied       0    1.21     0.62        1     30   \n",
       "39950     agnostically       0    1.20     0.50        2     27   \n",
       "39951  conceptualistic       0    1.18     0.50        4     26   \n",
       "39952  conventionalism       0    1.18     0.48        1     29   \n",
       "39953    essentialness       0    1.04     0.20        2     26   \n",
       "\n",
       "       Percent_known  SUBTLEX Dom_Pos  \n",
       "0               0.96        0       0  \n",
       "1               0.90        0       0  \n",
       "2               0.88       66       0  \n",
       "3               1.00        1       0  \n",
       "4               0.85        0       0  \n",
       "...              ...      ...     ...  \n",
       "39949           0.97        0    #N/A  \n",
       "39950           0.93        0    #N/A  \n",
       "39951           0.85        0    #N/A  \n",
       "39952           0.97        0    #N/A  \n",
       "39953           0.92        0    #N/A  \n",
       "\n",
       "[39954 rows x 9 columns]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    37058\n",
       "1     2896\n",
       "Name: Bigram, dtype: int64"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_df.Bigram.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Bigram</th>\n",
       "      <th>Conc.M</th>\n",
       "      <th>Conc.SD</th>\n",
       "      <th>Unknown</th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent_known</th>\n",
       "      <th>SUBTLEX</th>\n",
       "      <th>Dom_Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28707</th>\n",
       "      <td>baking soda</td>\n",
       "      <td>1</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>#N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28709</th>\n",
       "      <td>baseball bat</td>\n",
       "      <td>1</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>#N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28710</th>\n",
       "      <td>bath towel</td>\n",
       "      <td>1</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>#N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28711</th>\n",
       "      <td>beach ball</td>\n",
       "      <td>1</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>#N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28712</th>\n",
       "      <td>bed sheet</td>\n",
       "      <td>1</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>#N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39619</th>\n",
       "      <td>tantamount to</td>\n",
       "      <td>1</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.85</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0</td>\n",
       "      <td>#N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39857</th>\n",
       "      <td>chance on</td>\n",
       "      <td>1</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0</td>\n",
       "      <td>#N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39871</th>\n",
       "      <td>free rein</td>\n",
       "      <td>1</td>\n",
       "      <td>1.37</td>\n",
       "      <td>0.63</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0</td>\n",
       "      <td>#N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39899</th>\n",
       "      <td>by chance</td>\n",
       "      <td>1</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0</td>\n",
       "      <td>#N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39947</th>\n",
       "      <td>in principle</td>\n",
       "      <td>1</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.41</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0</td>\n",
       "      <td>#N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2896 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Word  Bigram  Conc.M  Conc.SD  Unknown  Total  Percent_known  \\\n",
       "28707    baking soda       1    5.00     0.00        0     30           1.00   \n",
       "28709   baseball bat       1    5.00     0.00        0     29           1.00   \n",
       "28710     bath towel       1    5.00     0.00        0     29           1.00   \n",
       "28711     beach ball       1    5.00     0.00        0     28           1.00   \n",
       "28712      bed sheet       1    5.00     0.00        0     28           1.00   \n",
       "...              ...     ...     ...      ...      ...    ...            ...   \n",
       "39619  tantamount to       1    1.52     0.85        4     27           0.85   \n",
       "39857      chance on       1    1.38     0.75        2     28           0.93   \n",
       "39871      free rein       1    1.37     0.63        2     29           0.93   \n",
       "39899      by chance       1    1.34     0.72        1     30           0.97   \n",
       "39947   in principle       1    1.21     0.41        4     28           0.86   \n",
       "\n",
       "       SUBTLEX Dom_Pos  \n",
       "28707        0    #N/A  \n",
       "28709        0    #N/A  \n",
       "28710        0    #N/A  \n",
       "28711        0    #N/A  \n",
       "28712        0    #N/A  \n",
       "...        ...     ...  \n",
       "39619        0    #N/A  \n",
       "39857        0    #N/A  \n",
       "39871        0    #N/A  \n",
       "39899        0    #N/A  \n",
       "39947        0    #N/A  \n",
       "\n",
       "[2896 rows x 9 columns]"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_df[concrete_df.Bigram==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Bigram</th>\n",
       "      <th>Conc.M</th>\n",
       "      <th>Conc.SD</th>\n",
       "      <th>Unknown</th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent_known</th>\n",
       "      <th>SUBTLEX</th>\n",
       "      <th>Dom_Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Word, Bigram, Conc.M, Conc.SD, Unknown, Total, Percent_known, SUBTLEX, Dom_Pos]\n",
       "Index: []"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There is no Nan value in Conc.M column\n",
    "concrete_df[concrete_df['Conc.M'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are we gonna consider bigrams in this dataset, given it's only a small fraction ~ 8% in size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.04"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(concrete_df['Conc.M'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(concrete_df['Conc.M'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concreteness values range from 1 - 5, we could possible use the inverse value of concreteness to scale it to a 0-1 range and give easier words less weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_words = list(concrete_df['Word'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39954"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(concrete_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_complement = [word for word in words_in_vector if word not in concrete_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1187"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(concrete_complement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['france',\n",
       " 'american',\n",
       " 'years',\n",
       " 'english',\n",
       " 'british',\n",
       " 'calais',\n",
       " 'german',\n",
       " 'england',\n",
       " 'french',\n",
       " 'london',\n",
       " 'largest',\n",
       " 'ndash',\n",
       " 'germany',\n",
       " 'japanese',\n",
       " 'york',\n",
       " 'australia',\n",
       " 'america',\n",
       " 'members',\n",
       " 'nord',\n",
       " 'italian']"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_complement[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_intersect = [word for word in words_in_vector if word in concrete_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3107"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(concrete_intersect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'state'"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_intersect[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.4483625e+00, -2.2964539e-02,  1.8962008e-03,  5.4766673e-01,\n",
       "       -2.6734391e-01, -6.7329913e-01,  3.6649238e-02,  7.5110155e-01,\n",
       "        2.6593966e-02,  3.6272058e-01,  1.8063091e-01, -8.1770726e-02,\n",
       "       -1.2644223e+00, -1.1407950e+00,  1.3186027e+00,  1.5527658e-01,\n",
       "       -4.0354431e-01,  3.4420010e-01,  7.5570363e-01,  1.0251225e+00,\n",
       "       -3.3162057e-01,  5.4691845e-01,  2.8648919e-01,  4.1449976e-01,\n",
       "        4.7424209e-01, -8.8924634e-01,  1.3474070e+00, -5.9115124e-01,\n",
       "       -2.3832204e-02,  2.1102323e-01,  1.1408305e+00, -2.3086752e-01,\n",
       "        3.8275310e-01,  6.7911857e-01, -1.9161204e-01,  2.0487516e-01,\n",
       "        4.2912993e-01, -1.9336376e-01,  7.2722673e-02, -1.0755360e+00,\n",
       "       -1.4390081e+00, -5.9247285e-01,  3.6843863e-01, -2.0793953e+00,\n",
       "        5.6404817e-01, -1.5470855e-01,  8.4685934e-01,  7.3713817e-02,\n",
       "       -1.0202421e+00,  3.3701974e-01, -1.2711020e+00,  7.2744918e-01,\n",
       "       -7.4224278e-02,  1.1051776e+00, -1.0541314e+00, -5.4030412e-01,\n",
       "        1.1490265e+00,  7.6801157e-01,  3.5300469e-01, -6.7077130e-02,\n",
       "       -6.5011984e-01,  1.4341727e-01, -1.3758168e+00,  3.1443551e-02,\n",
       "       -2.3014335e-01, -3.3961418e-01,  5.1504678e-01,  2.5453347e-01,\n",
       "        3.7255052e-01,  9.5435417e-01,  3.7471649e-01, -2.6675457e-01,\n",
       "        5.1894271e-01, -5.4225886e-01,  6.6476667e-01,  6.3090223e-01,\n",
       "        8.7733161e-01, -8.0502927e-01, -3.7340087e-01,  9.1642064e-01,\n",
       "        1.0521773e+00,  7.2462112e-01, -1.1677450e+00,  5.7258666e-02,\n",
       "       -4.9416608e-01, -1.6915272e-01, -1.1965002e+00, -1.1763625e+00,\n",
       "       -1.0882083e+00,  7.8588203e-03,  1.7722680e-01, -7.0745629e-01,\n",
       "        4.1059697e-01, -8.5689449e-01, -1.0432867e+00,  5.0230306e-01,\n",
       "       -7.6437533e-02,  2.8377321e-01, -1.9868830e-01, -9.7835362e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.52])"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_df[concrete_df['Word']=='state']['Conc.M'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in concrete_intersect:\n",
    "    word_vectors[word] = word_vectors[word] * 1/concrete_df[concrete_df['Word']==word]['Conc.M'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "== AoA_51715_words.csv ==\n",
    "\n",
    "This file contains \"Age of Acquisition\" (AoA) estimates for about 51k English words, which refers to the approximate age (in years) when a word was learned. Early words, being more basic, have lower average AoA.\n",
    "\n",
    "The main columns you will be interested in are \"Word\" and \"AoA_Kup_lem\". But the others may be useful too.\n",
    "\n",
    "The file contains these columns:\n",
    "\n",
    "Word :: The word in question\n",
    "Alternative.spelling :: if the Word may be spelled frequently in another form\t\n",
    "Freq_pm\t:: Freq of the Word in general English (larger -> more common)\n",
    "Dom_PoS_SUBTLEX\t:: Dominant part of speech in general usage\n",
    "Nletters :: number of letters \n",
    "Nphon :: number of phonemes\n",
    "Nsyll :: number of syllables\n",
    "Lemma_highest_PoS :: the \"lemmatized\" or \"root\" form of the word (in the dominant part of speech. e.g. The root form of the verb \"abates\" is \"abate\".\n",
    "AoA_Kup\t:: The AoA from a previous study by Kuperman et al.\n",
    "Perc_known :: Percent of people who knew the word in the Kuperman et al. study\n",
    "AoA_Kup_lem :: Estimated AoA based on Kuperman et al. study lemmatized words. THIS IS THE MAIN COLUMN OF INTEREST.\n",
    "Perc_known_lem\t:: Estimated percentage of people who would know this form of the word in the Kuperman study.\n",
    "AoA_Bird_lem :: AoA reported in previous study by Bird (2001) \n",
    "AoA_Bristol_lem\t:: AoA reported in previous study from Bristol Univ. (2006)\n",
    "AoA_Cort_lem :: AoA reported in previous study by Cortese & Khanna (2008)\n",
    "AoA_Schock :: AoA reported in previous study by Schock (2012)\n",
    "\n",
    "Original source : http://crr.ugent.be/archives/806"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Alternative.spelling</th>\n",
       "      <th>Freq_pm</th>\n",
       "      <th>Dom_PoS_SUBTLEX</th>\n",
       "      <th>Nletters</th>\n",
       "      <th>Nphon</th>\n",
       "      <th>Nsyll</th>\n",
       "      <th>Lemma_highest_PoS</th>\n",
       "      <th>AoA_Kup</th>\n",
       "      <th>Perc_known</th>\n",
       "      <th>AoA_Kup_lem</th>\n",
       "      <th>Perc_known_lem</th>\n",
       "      <th>AoA_Bird_lem</th>\n",
       "      <th>AoA_Bristol_lem</th>\n",
       "      <th>AoA_Cort_lem</th>\n",
       "      <th>AoA_Schock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>20415.27</td>\n",
       "      <td>Article</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>2.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aardvark</td>\n",
       "      <td>aardvark</td>\n",
       "      <td>0.41</td>\n",
       "      <td>Noun</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>aardvark</td>\n",
       "      <td>9.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>9.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abacus</td>\n",
       "      <td>abacus</td>\n",
       "      <td>0.24</td>\n",
       "      <td>Noun</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>abacus</td>\n",
       "      <td>8.69</td>\n",
       "      <td>0.65</td>\n",
       "      <td>8.69</td>\n",
       "      <td>0.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abacuses</td>\n",
       "      <td>abacuses</td>\n",
       "      <td>0.02</td>\n",
       "      <td>Noun</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>abacus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.69</td>\n",
       "      <td>0.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abalone</td>\n",
       "      <td>abalone</td>\n",
       "      <td>0.51</td>\n",
       "      <td>Verb</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>abalone</td>\n",
       "      <td>12.23</td>\n",
       "      <td>0.72</td>\n",
       "      <td>12.23</td>\n",
       "      <td>0.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word Alternative.spelling   Freq_pm Dom_PoS_SUBTLEX  Nletters  Nphon  \\\n",
       "0         a                    a  20415.27         Article         1      1   \n",
       "1  aardvark             aardvark      0.41            Noun         8      7   \n",
       "2    abacus               abacus      0.24            Noun         6      6   \n",
       "3  abacuses             abacuses      0.02            Noun         8      9   \n",
       "4   abalone              abalone      0.51            Verb         7      7   \n",
       "\n",
       "   Nsyll Lemma_highest_PoS  AoA_Kup  Perc_known  AoA_Kup_lem  Perc_known_lem  \\\n",
       "0      1                 a     2.89        1.00         2.89            1.00   \n",
       "1      2          aardvark     9.89        1.00         9.89            1.00   \n",
       "2      3            abacus     8.69        0.65         8.69            0.65   \n",
       "3      4            abacus      NaN         NaN         8.69            0.65   \n",
       "4      4           abalone    12.23        0.72        12.23            0.72   \n",
       "\n",
       "   AoA_Bird_lem  AoA_Bristol_lem  AoA_Cort_lem  AoA_Schock  \n",
       "0          3.16              NaN           NaN         NaN  \n",
       "1           NaN              NaN           NaN         NaN  \n",
       "2           NaN              NaN           NaN         NaN  \n",
       "3           NaN              NaN           NaN         NaN  \n",
       "4           NaN              NaN           NaN         NaN  "
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AoA\n",
    "#Perc_known_lem, AoA_Kup_lem\n",
    "aoawords_path = 'Data/AoA_51715_words.csv'\n",
    "AoA = pd.read_csv(aoawords_path,encoding = 'unicode_escape')\n",
    "AoA_set = set(AoA['Word'].values)\n",
    "AoA.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51715"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AoA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.58"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AoA.AoA_Kup_lem.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AoA.AoA_Kup_lem.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Alternative.spelling</th>\n",
       "      <th>Freq_pm</th>\n",
       "      <th>Dom_PoS_SUBTLEX</th>\n",
       "      <th>Nletters</th>\n",
       "      <th>Nphon</th>\n",
       "      <th>Nsyll</th>\n",
       "      <th>Lemma_highest_PoS</th>\n",
       "      <th>AoA_Kup</th>\n",
       "      <th>Perc_known</th>\n",
       "      <th>AoA_Kup_lem</th>\n",
       "      <th>Perc_known_lem</th>\n",
       "      <th>AoA_Bird_lem</th>\n",
       "      <th>AoA_Bristol_lem</th>\n",
       "      <th>AoA_Cort_lem</th>\n",
       "      <th>AoA_Schock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14878</th>\n",
       "      <td>eisteddfod</td>\n",
       "      <td>eisteddfod</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>eisteddfod</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>architrave</td>\n",
       "      <td>architrave</td>\n",
       "      <td>0.04</td>\n",
       "      <td>Noun</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>architrave</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6274</th>\n",
       "      <td>calceolaria</td>\n",
       "      <td>calceolaria</td>\n",
       "      <td>0.02</td>\n",
       "      <td>Noun</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>calceolaria</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32931</th>\n",
       "      <td>penury</td>\n",
       "      <td>penury</td>\n",
       "      <td>0.02</td>\n",
       "      <td>Noun</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>penury</td>\n",
       "      <td>20.6</td>\n",
       "      <td>0.28</td>\n",
       "      <td>20.6</td>\n",
       "      <td>0.28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25243</th>\n",
       "      <td>kendo</td>\n",
       "      <td>kendo</td>\n",
       "      <td>0.37</td>\n",
       "      <td>Noun</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>kendo</td>\n",
       "      <td>20.5</td>\n",
       "      <td>0.11</td>\n",
       "      <td>20.5</td>\n",
       "      <td>0.11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38932</th>\n",
       "      <td>rogation</td>\n",
       "      <td>rogation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>rogation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42089</th>\n",
       "      <td>smilax</td>\n",
       "      <td>smilax</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>smilax</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46368</th>\n",
       "      <td>thulium</td>\n",
       "      <td>thulium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>thulium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50862</th>\n",
       "      <td>wickiup</td>\n",
       "      <td>wickiup</td>\n",
       "      <td>0.27</td>\n",
       "      <td>Noun</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>wickiup</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50941</th>\n",
       "      <td>williwaw</td>\n",
       "      <td>williwaw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>williwaw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51715 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Word Alternative.spelling  Freq_pm Dom_PoS_SUBTLEX  Nletters  \\\n",
       "14878   eisteddfod           eisteddfod      NaN             NaN        10   \n",
       "2084    architrave           architrave     0.04            Noun        10   \n",
       "6274   calceolaria          calceolaria     0.02            Noun        11   \n",
       "32931       penury               penury     0.02            Noun         6   \n",
       "25243        kendo                kendo     0.37            Noun         5   \n",
       "...            ...                  ...      ...             ...       ...   \n",
       "38932     rogation             rogation      NaN             NaN         8   \n",
       "42089       smilax               smilax      NaN             NaN         6   \n",
       "46368      thulium              thulium      NaN             NaN         7   \n",
       "50862      wickiup              wickiup     0.27            Noun         7   \n",
       "50941     williwaw             williwaw      NaN             NaN         8   \n",
       "\n",
       "       Nphon  Nsyll Lemma_highest_PoS  AoA_Kup  Perc_known  AoA_Kup_lem  \\\n",
       "14878      8      3        eisteddfod     25.0        0.05         25.0   \n",
       "2084       8      3        architrave     21.0        0.05         21.0   \n",
       "6274      11      6       calceolaria     21.0        0.11         21.0   \n",
       "32931      7      3            penury     20.6        0.28         20.6   \n",
       "25243      5      2             kendo     20.5        0.11         20.5   \n",
       "...      ...    ...               ...      ...         ...          ...   \n",
       "38932      7      3          rogation      NaN        0.00          NaN   \n",
       "42089      7      2            smilax      NaN        0.00          NaN   \n",
       "46368      6      3           thulium      NaN        0.00          NaN   \n",
       "50862      6      3           wickiup      NaN        0.00          NaN   \n",
       "50941      6      3          williwaw      NaN        0.00          NaN   \n",
       "\n",
       "       Perc_known_lem  AoA_Bird_lem  AoA_Bristol_lem  AoA_Cort_lem  AoA_Schock  \n",
       "14878            0.05           NaN              NaN           NaN         NaN  \n",
       "2084             0.05           NaN              NaN           NaN         NaN  \n",
       "6274             0.11           NaN              NaN           NaN         NaN  \n",
       "32931            0.28           NaN              NaN           NaN         NaN  \n",
       "25243            0.11           NaN              NaN           NaN         NaN  \n",
       "...               ...           ...              ...           ...         ...  \n",
       "38932            0.00           NaN              NaN           NaN         NaN  \n",
       "42089            0.00           NaN              NaN           NaN         NaN  \n",
       "46368            0.00           NaN              NaN           NaN         NaN  \n",
       "50862            0.00           NaN              NaN           NaN         NaN  \n",
       "50941            0.00           NaN              NaN           NaN         NaN  \n",
       "\n",
       "[51715 rows x 16 columns]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AoA.sort_values(['AoA_Kup_lem'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AoA[AoA['AoA_Kup_lem'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Alternative.spelling</th>\n",
       "      <th>Freq_pm</th>\n",
       "      <th>Dom_PoS_SUBTLEX</th>\n",
       "      <th>Nletters</th>\n",
       "      <th>Nphon</th>\n",
       "      <th>Nsyll</th>\n",
       "      <th>Lemma_highest_PoS</th>\n",
       "      <th>AoA_Kup</th>\n",
       "      <th>Perc_known</th>\n",
       "      <th>AoA_Kup_lem</th>\n",
       "      <th>Perc_known_lem</th>\n",
       "      <th>AoA_Bird_lem</th>\n",
       "      <th>AoA_Bristol_lem</th>\n",
       "      <th>AoA_Cort_lem</th>\n",
       "      <th>AoA_Schock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>actinium</td>\n",
       "      <td>actinium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>actinium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1322</th>\n",
       "      <td>ambuscade</td>\n",
       "      <td>ambuscade</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>ambuscade</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306</th>\n",
       "      <td>ashlar</td>\n",
       "      <td>ashlar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>ashlar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5095</th>\n",
       "      <td>bosky</td>\n",
       "      <td>bosky</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>bosky</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6404</th>\n",
       "      <td>canaille</td>\n",
       "      <td>canaille</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>canaille</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9004</th>\n",
       "      <td>compeer</td>\n",
       "      <td>compeer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>compeer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9005</th>\n",
       "      <td>compeers</td>\n",
       "      <td>compeers</td>\n",
       "      <td>0.02</td>\n",
       "      <td>Noun</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>compeer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16000</th>\n",
       "      <td>europium</td>\n",
       "      <td>europium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>europium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19065</th>\n",
       "      <td>gallimaufry</td>\n",
       "      <td>gallimaufry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>gallimaufry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22498</th>\n",
       "      <td>hutment</td>\n",
       "      <td>hutment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>hutment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25196</th>\n",
       "      <td>karakul</td>\n",
       "      <td>karakul</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>karakul</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25219</th>\n",
       "      <td>kedge</td>\n",
       "      <td>kedge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>kedge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25575</th>\n",
       "      <td>kyat</td>\n",
       "      <td>kyat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>kyat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32754</th>\n",
       "      <td>peculation</td>\n",
       "      <td>peculation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>peculation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34588</th>\n",
       "      <td>pother</td>\n",
       "      <td>pother</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>pother</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38932</th>\n",
       "      <td>rogation</td>\n",
       "      <td>rogation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>rogation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42089</th>\n",
       "      <td>smilax</td>\n",
       "      <td>smilax</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>smilax</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46368</th>\n",
       "      <td>thulium</td>\n",
       "      <td>thulium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>thulium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50862</th>\n",
       "      <td>wickiup</td>\n",
       "      <td>wickiup</td>\n",
       "      <td>0.27</td>\n",
       "      <td>Noun</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>wickiup</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50941</th>\n",
       "      <td>williwaw</td>\n",
       "      <td>williwaw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>williwaw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Word Alternative.spelling  Freq_pm Dom_PoS_SUBTLEX  Nletters  \\\n",
       "442       actinium             actinium      NaN             NaN         8   \n",
       "1322     ambuscade            ambuscade      NaN             NaN         9   \n",
       "2306        ashlar               ashlar      NaN             NaN         6   \n",
       "5095         bosky                bosky      NaN             NaN         5   \n",
       "6404      canaille             canaille      NaN             NaN         8   \n",
       "9004       compeer              compeer      NaN             NaN         7   \n",
       "9005      compeers             compeers     0.02            Noun         8   \n",
       "16000     europium             europium      NaN             NaN         8   \n",
       "19065  gallimaufry          gallimaufry      NaN             NaN        11   \n",
       "22498      hutment              hutment      NaN             NaN         7   \n",
       "25196      karakul              karakul      NaN             NaN         7   \n",
       "25219        kedge                kedge      NaN             NaN         5   \n",
       "25575         kyat                 kyat      NaN             NaN         4   \n",
       "32754   peculation           peculation      NaN             NaN        10   \n",
       "34588       pother               pother      NaN             NaN         6   \n",
       "38932     rogation             rogation      NaN             NaN         8   \n",
       "42089       smilax               smilax      NaN             NaN         6   \n",
       "46368      thulium              thulium      NaN             NaN         7   \n",
       "50862      wickiup              wickiup     0.27            Noun         7   \n",
       "50941     williwaw             williwaw      NaN             NaN         8   \n",
       "\n",
       "       Nphon  Nsyll Lemma_highest_PoS  AoA_Kup  Perc_known  AoA_Kup_lem  \\\n",
       "442        8      4          actinium      NaN         0.0          NaN   \n",
       "1322       8      3         ambuscade      NaN         0.0          NaN   \n",
       "2306       5      2            ashlar      NaN         0.0          NaN   \n",
       "5095       4      2             bosky      NaN         0.0          NaN   \n",
       "6404       5      2          canaille      NaN         0.0          NaN   \n",
       "9004       6      3           compeer      NaN         0.0          NaN   \n",
       "9005       7      3           compeer      NaN         NaN          NaN   \n",
       "16000      8      4          europium      NaN         0.0          NaN   \n",
       "19065      9      4       gallimaufry      NaN         0.0          NaN   \n",
       "22498      7      2           hutment      NaN         0.0          NaN   \n",
       "25196      7      3           karakul      NaN         0.0          NaN   \n",
       "25219      3      1             kedge      NaN         0.0          NaN   \n",
       "25575      4      2              kyat      NaN         0.0          NaN   \n",
       "32754     10      4        peculation      NaN         0.0          NaN   \n",
       "34588      5      2            pother      NaN         0.0          NaN   \n",
       "38932      7      3          rogation      NaN         0.0          NaN   \n",
       "42089      7      2            smilax      NaN         0.0          NaN   \n",
       "46368      6      3           thulium      NaN         0.0          NaN   \n",
       "50862      6      3           wickiup      NaN         0.0          NaN   \n",
       "50941      6      3          williwaw      NaN         0.0          NaN   \n",
       "\n",
       "       Perc_known_lem  AoA_Bird_lem  AoA_Bristol_lem  AoA_Cort_lem  AoA_Schock  \n",
       "442               0.0           NaN              NaN           NaN         NaN  \n",
       "1322              0.0           NaN              NaN           NaN         NaN  \n",
       "2306              0.0           NaN              NaN           NaN         NaN  \n",
       "5095              0.0           NaN              NaN           NaN         NaN  \n",
       "6404              0.0           NaN              NaN           NaN         NaN  \n",
       "9004              0.0           NaN              NaN           NaN         NaN  \n",
       "9005              0.0           NaN              NaN           NaN         NaN  \n",
       "16000             0.0           NaN              NaN           NaN         NaN  \n",
       "19065             0.0           NaN              NaN           NaN         NaN  \n",
       "22498             0.0           NaN              NaN           NaN         NaN  \n",
       "25196             0.0           NaN              NaN           NaN         NaN  \n",
       "25219             0.0           NaN              NaN           NaN         NaN  \n",
       "25575             0.0           NaN              NaN           NaN         NaN  \n",
       "32754             0.0           NaN              NaN           NaN         NaN  \n",
       "34588             0.0           NaN              NaN           NaN         NaN  \n",
       "38932             0.0           NaN              NaN           NaN         NaN  \n",
       "42089             0.0           NaN              NaN           NaN         NaN  \n",
       "46368             0.0           NaN              NaN           NaN         NaN  \n",
       "50862             0.0           NaN              NaN           NaN         NaN  \n",
       "50941             0.0           NaN              NaN           NaN         NaN  "
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AoA[AoA['AoA_Kup_lem'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to impute all Nan values in AoA_Kup_lem as the max AoA value 25, as they appear to be hard words.\n",
    "AoA['AoA_Kup_lem'].fillna(value=AoA['AoA_Kup_lem'].max(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Alternative.spelling</th>\n",
       "      <th>Freq_pm</th>\n",
       "      <th>Dom_PoS_SUBTLEX</th>\n",
       "      <th>Nletters</th>\n",
       "      <th>Nphon</th>\n",
       "      <th>Nsyll</th>\n",
       "      <th>Lemma_highest_PoS</th>\n",
       "      <th>AoA_Kup</th>\n",
       "      <th>Perc_known</th>\n",
       "      <th>AoA_Kup_lem</th>\n",
       "      <th>Perc_known_lem</th>\n",
       "      <th>AoA_Bird_lem</th>\n",
       "      <th>AoA_Bristol_lem</th>\n",
       "      <th>AoA_Cort_lem</th>\n",
       "      <th>AoA_Schock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2306</th>\n",
       "      <td>ashlar</td>\n",
       "      <td>ashlar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>ashlar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38932</th>\n",
       "      <td>rogation</td>\n",
       "      <td>rogation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>rogation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46368</th>\n",
       "      <td>thulium</td>\n",
       "      <td>thulium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>thulium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14878</th>\n",
       "      <td>eisteddfod</td>\n",
       "      <td>eisteddfod</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>eisteddfod</td>\n",
       "      <td>25.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>25.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5095</th>\n",
       "      <td>bosky</td>\n",
       "      <td>bosky</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>bosky</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27395</th>\n",
       "      <td>mamma</td>\n",
       "      <td>mamma</td>\n",
       "      <td>3.02</td>\n",
       "      <td>Noun</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>mama</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27393</th>\n",
       "      <td>mamas</td>\n",
       "      <td>mamas</td>\n",
       "      <td>0.71</td>\n",
       "      <td>Noun</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>mama</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27392</th>\n",
       "      <td>mama</td>\n",
       "      <td>mama</td>\n",
       "      <td>103.71</td>\n",
       "      <td>Noun</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>mama</td>\n",
       "      <td>1.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29050</th>\n",
       "      <td>mommas</td>\n",
       "      <td>mommas</td>\n",
       "      <td>0.10</td>\n",
       "      <td>Noun</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>momma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29049</th>\n",
       "      <td>momma</td>\n",
       "      <td>momma</td>\n",
       "      <td>8.08</td>\n",
       "      <td>Noun</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>momma</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51715 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Word Alternative.spelling  Freq_pm Dom_PoS_SUBTLEX  Nletters  \\\n",
       "2306       ashlar               ashlar      NaN             NaN         6   \n",
       "38932    rogation             rogation      NaN             NaN         8   \n",
       "46368     thulium              thulium      NaN             NaN         7   \n",
       "14878  eisteddfod           eisteddfod      NaN             NaN        10   \n",
       "5095        bosky                bosky      NaN             NaN         5   \n",
       "...           ...                  ...      ...             ...       ...   \n",
       "27395       mamma                mamma     3.02            Noun         5   \n",
       "27393       mamas                mamas     0.71            Noun         5   \n",
       "27392        mama                 mama   103.71            Noun         4   \n",
       "29050      mommas               mommas     0.10            Noun         6   \n",
       "29049       momma                momma     8.08            Noun         5   \n",
       "\n",
       "       Nphon  Nsyll Lemma_highest_PoS  AoA_Kup  Perc_known  AoA_Kup_lem  \\\n",
       "2306       5      2            ashlar      NaN        0.00        25.00   \n",
       "38932      7      3          rogation      NaN        0.00        25.00   \n",
       "46368      6      3           thulium      NaN        0.00        25.00   \n",
       "14878      8      3        eisteddfod    25.00        0.05        25.00   \n",
       "5095       4      2             bosky      NaN        0.00        25.00   \n",
       "...      ...    ...               ...      ...         ...          ...   \n",
       "27395      4      2              mama      NaN         NaN         1.89   \n",
       "27393      5      2              mama      NaN         NaN         1.89   \n",
       "27392      4      2              mama     1.89        1.00         1.89   \n",
       "29050      5      2             momma      NaN         NaN         1.58   \n",
       "29049      4      2             momma     1.58        1.00         1.58   \n",
       "\n",
       "       Perc_known_lem  AoA_Bird_lem  AoA_Bristol_lem  AoA_Cort_lem  AoA_Schock  \n",
       "2306             0.00           NaN              NaN           NaN         NaN  \n",
       "38932            0.00           NaN              NaN           NaN         NaN  \n",
       "46368            0.00           NaN              NaN           NaN         NaN  \n",
       "14878            0.05           NaN              NaN           NaN         NaN  \n",
       "5095             0.00           NaN              NaN           NaN         NaN  \n",
       "...               ...           ...              ...           ...         ...  \n",
       "27395            1.00           NaN              NaN           NaN         NaN  \n",
       "27393            1.00           NaN              NaN           NaN         NaN  \n",
       "27392            1.00           NaN              NaN           NaN         NaN  \n",
       "29050            1.00           NaN              NaN           NaN         NaN  \n",
       "29049            1.00           NaN              NaN           NaN         NaN  \n",
       "\n",
       "[51715 rows x 16 columns]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AoA.sort_values(['AoA_Kup_lem'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AoA values range from 0 - 25, which means the smaller the AoA value, the easier the word is. We could possibly use the AoA value to give easier words less weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoa_words = list(AoA['Word'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51715"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aoa_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoa_complement = [word for word in words_in_vector if word not in aoa_words]\n",
    "aoa_intersect = [word for word in words_in_vector if word in aoa_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "988"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aoa_complement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['france',\n",
       " 'american',\n",
       " 'english',\n",
       " 'september',\n",
       " 'british',\n",
       " 'calais',\n",
       " 'german',\n",
       " 'october',\n",
       " 'july',\n",
       " 'june',\n",
       " 'april',\n",
       " 'england',\n",
       " 'february',\n",
       " 'french',\n",
       " 'london',\n",
       " 'john',\n",
       " 'ndash',\n",
       " 'germany',\n",
       " 'usually',\n",
       " 'japanese',\n",
       " 'york',\n",
       " 'australia',\n",
       " 'america',\n",
       " 'nord',\n",
       " 'italian',\n",
       " 'aisne',\n",
       " 'europe',\n",
       " 'pakistan',\n",
       " 'spanish',\n",
       " 'greek',\n",
       " 'european',\n",
       " 'india',\n",
       " 'calvados',\n",
       " 'william',\n",
       " 'california',\n",
       " 'basse',\n",
       " 'normandie',\n",
       " 'switzerland',\n",
       " 'george',\n",
       " 'canton',\n",
       " 'commonly',\n",
       " 'canada',\n",
       " 'charles',\n",
       " 'china',\n",
       " 'australian',\n",
       " 'italy',\n",
       " 'loire',\n",
       " 'christian',\n",
       " 'henry',\n",
       " 'gironde',\n",
       " 'scotland',\n",
       " 'canadian',\n",
       " 'ireland',\n",
       " 'africa',\n",
       " 'picardie',\n",
       " 'paul',\n",
       " 'florida',\n",
       " 'aquitaine',\n",
       " 'brazilian',\n",
       " 'generally',\n",
       " 'atlantic',\n",
       " 'latin',\n",
       " 'david',\n",
       " 'robert',\n",
       " 'chinese',\n",
       " 'russian',\n",
       " 'asia',\n",
       " 'paris',\n",
       " 'scottish',\n",
       " 'indian',\n",
       " 'dutch',\n",
       " 'berlin',\n",
       " 'officially',\n",
       " 'richard',\n",
       " 'lower',\n",
       " 'spain',\n",
       " 'iowa',\n",
       " 'especially',\n",
       " 'disney',\n",
       " 'michael',\n",
       " 'thomas',\n",
       " 'peter',\n",
       " 'wales',\n",
       " 'britain',\n",
       " 'wikipedia',\n",
       " 'approximately',\n",
       " 'louis',\n",
       " 'mexico',\n",
       " 'washington',\n",
       " 'pacific',\n",
       " 'virginia',\n",
       " 'edward',\n",
       " 'chicago',\n",
       " 'martin',\n",
       " 'mainly',\n",
       " 'russia',\n",
       " 'texas',\n",
       " 'mary',\n",
       " 'angeles',\n",
       " 'nintendo',\n",
       " 'victoria',\n",
       " 'alpes',\n",
       " 'irish',\n",
       " 'partement',\n",
       " 'typically',\n",
       " 'nobel',\n",
       " 'african',\n",
       " 'austria',\n",
       " 'brazil',\n",
       " 'widely',\n",
       " 'netherlands',\n",
       " 'belgian',\n",
       " 'oklahoma',\n",
       " 'mario',\n",
       " 'picardy',\n",
       " 'romania',\n",
       " 'zealand',\n",
       " 'joseph',\n",
       " 'jupiter',\n",
       " 'sarthe',\n",
       " 'illinois',\n",
       " 'particularly',\n",
       " 'austrian',\n",
       " 'manchester',\n",
       " 'poland',\n",
       " 'jesus',\n",
       " 'olympic',\n",
       " 'harry',\n",
       " 'jewish',\n",
       " 'elizabeth',\n",
       " 'carolina',\n",
       " 'prix',\n",
       " 'korea',\n",
       " 'previously',\n",
       " 'rome',\n",
       " 'egypt',\n",
       " 'directly',\n",
       " 'swedish',\n",
       " 'probably',\n",
       " 'armenian',\n",
       " 'korean',\n",
       " 'alexander',\n",
       " 'santa',\n",
       " 'scott',\n",
       " 'saturn',\n",
       " 'oxford',\n",
       " 'online',\n",
       " 'recently',\n",
       " 'sweden',\n",
       " 'georgia',\n",
       " 'ohio',\n",
       " 'maria',\n",
       " 'persian',\n",
       " 'vienna',\n",
       " 'melbourne',\n",
       " 'smith',\n",
       " 'kentucky',\n",
       " 'ontario',\n",
       " 'portuguese',\n",
       " 'francisco',\n",
       " 'provence',\n",
       " 'nazi',\n",
       " 'highly',\n",
       " 'isbn',\n",
       " 'columbia',\n",
       " 'pennsylvania',\n",
       " 'linux',\n",
       " 'chris',\n",
       " 'michigan',\n",
       " 'arthur',\n",
       " 'boston',\n",
       " 'islamic',\n",
       " 'marie',\n",
       " 'finally',\n",
       " 'hungary',\n",
       " 'mayenne',\n",
       " 'sydney',\n",
       " 'andrew',\n",
       " 'massachusetts',\n",
       " 'iran',\n",
       " 'microsoft',\n",
       " 'unlike',\n",
       " 'albert',\n",
       " 'rhine',\n",
       " 'punjab',\n",
       " 'belgium',\n",
       " 'stanley',\n",
       " 'frederick',\n",
       " 'relatively',\n",
       " 'turkish',\n",
       " 'kilometres',\n",
       " 'anne',\n",
       " 'alabama',\n",
       " 'mississippi',\n",
       " 'indiana',\n",
       " 'swiss',\n",
       " 'czech',\n",
       " 'kelly',\n",
       " 'completely',\n",
       " 'kashmir',\n",
       " 'toronto',\n",
       " 'israel',\n",
       " 'kong',\n",
       " 'denmark',\n",
       " 'daniel',\n",
       " 'greece',\n",
       " 'singapore',\n",
       " 'aube',\n",
       " 'frequently',\n",
       " 'saxony',\n",
       " 'walt',\n",
       " 'azur',\n",
       " 'municipalities',\n",
       " 'alaska',\n",
       " 'jackson',\n",
       " 'julian',\n",
       " 'steve',\n",
       " 'cambridge',\n",
       " 'shortly',\n",
       " 'gregorian',\n",
       " 'largely',\n",
       " 'closely',\n",
       " 'traditionally',\n",
       " 'moscow',\n",
       " 'armenia',\n",
       " 'taylor',\n",
       " 'norway',\n",
       " 'asian',\n",
       " 'kansas',\n",
       " 'minnesota',\n",
       " 'portugal',\n",
       " 'fully',\n",
       " 'metres',\n",
       " 'hamilton',\n",
       " 'ardãƒ',\n",
       " 'respectively',\n",
       " 'arabic',\n",
       " 'caribbean',\n",
       " 'ross',\n",
       " 'argentina',\n",
       " 'finland',\n",
       " 'robinson',\n",
       " 'pierre',\n",
       " 'colorado',\n",
       " 'munich',\n",
       " 'tehsil',\n",
       " 'florence',\n",
       " 'baden',\n",
       " 'smackdown',\n",
       " 'lincoln',\n",
       " 'grammy',\n",
       " 'abbottabad',\n",
       " 'yorkshire',\n",
       " 'adams',\n",
       " 'jane',\n",
       " 'christ',\n",
       " 'aargau',\n",
       " 'walter',\n",
       " 'johann',\n",
       " 'williams',\n",
       " 'francis',\n",
       " 'dominican',\n",
       " 'montreal',\n",
       " 'hong',\n",
       " 'wilson',\n",
       " 'hitler',\n",
       " 'jews',\n",
       " 'subsequently',\n",
       " 'vietnam',\n",
       " 'muslims',\n",
       " 'christopher',\n",
       " 'benjamin',\n",
       " 'tokyo',\n",
       " 'philip',\n",
       " 'jones',\n",
       " 'specifically',\n",
       " 'danish',\n",
       " 'karl',\n",
       " 'pokãƒ',\n",
       " 'bavaria',\n",
       " 'islam',\n",
       " 'possibly',\n",
       " 'playstation',\n",
       " 'easily',\n",
       " 'fifa',\n",
       " 'subtropical',\n",
       " 'johnson',\n",
       " 'notably',\n",
       " 'hollywood',\n",
       " 'stephen',\n",
       " 'christianity',\n",
       " 'milan',\n",
       " 'wilhelm',\n",
       " 'quebec',\n",
       " 'brian',\n",
       " 'hungarian',\n",
       " 'afghanistan',\n",
       " 'detroit',\n",
       " 'geneva',\n",
       " 'iraq',\n",
       " 'neptune',\n",
       " 'samuel',\n",
       " 'arab',\n",
       " 'madrid',\n",
       " 'harrison',\n",
       " 'taiwan',\n",
       " 'missouri',\n",
       " 'sarah',\n",
       " 'historically',\n",
       " 'arizona',\n",
       " 'colour',\n",
       " 'mexican',\n",
       " 'matt',\n",
       " 'paulo',\n",
       " 'maya',\n",
       " 'anna',\n",
       " 'roger',\n",
       " 'puerto',\n",
       " 'egyptian',\n",
       " 'chile',\n",
       " 'alfred',\n",
       " 'newly',\n",
       " 'westphalia',\n",
       " 'norwegian',\n",
       " 'extremely',\n",
       " 'matthew',\n",
       " 'juan',\n",
       " 'friedrich',\n",
       " 'adam',\n",
       " 'philadelphia',\n",
       " 'muhammad',\n",
       " 'warner',\n",
       " 'graham',\n",
       " 'vancouver',\n",
       " 'vendãƒ',\n",
       " 'birmingham',\n",
       " 'entirely',\n",
       " 'friday',\n",
       " 'simon',\n",
       " 'finnish',\n",
       " 'alongside',\n",
       " 'wisconsin',\n",
       " 'pittsburgh',\n",
       " 'philippines',\n",
       " 'ferdinand',\n",
       " 'patrick',\n",
       " 'ticino',\n",
       " 'bangladesh',\n",
       " 'hawaii',\n",
       " 'tony',\n",
       " 'vaucluse',\n",
       " 'voyager',\n",
       " 'westminster',\n",
       " 'bruce',\n",
       " 'wrestlemania',\n",
       " 'beatles',\n",
       " 'jefferson',\n",
       " 'carl',\n",
       " 'monday',\n",
       " 'xbox',\n",
       " 'lewis',\n",
       " 'caesar',\n",
       " 'bobby',\n",
       " 'diego',\n",
       " 'phoenix',\n",
       " 'ardã',\n",
       " 'indonesia',\n",
       " 'billy',\n",
       " 'christians',\n",
       " 'arkansas',\n",
       " 'formally',\n",
       " 'charlotte',\n",
       " 'cuba',\n",
       " 'gaelic',\n",
       " 'catherine',\n",
       " 'viii',\n",
       " 'saxe',\n",
       " 'monroe',\n",
       " 'anthony',\n",
       " 'tennessee',\n",
       " 'miller',\n",
       " 'hebrew',\n",
       " 'howard',\n",
       " 'jordan',\n",
       " 'romans',\n",
       " 'ukraine',\n",
       " 'idaho',\n",
       " 'americans',\n",
       " 'walloon',\n",
       " 'liverpool',\n",
       " 'kennedy',\n",
       " 'indo',\n",
       " 'glasgow',\n",
       " 'darwin',\n",
       " 'urdu',\n",
       " 'douglas',\n",
       " 'davis',\n",
       " 'eric',\n",
       " 'orton',\n",
       " 'anglo',\n",
       " 'bach',\n",
       " 'edinburgh',\n",
       " 'lawrence',\n",
       " 'flanders',\n",
       " 'seattle',\n",
       " 'cena',\n",
       " 'atlanta',\n",
       " 'google',\n",
       " 'jerusalem',\n",
       " 'uranus',\n",
       " 'austen',\n",
       " 'rico',\n",
       " 'lennon',\n",
       " 'alice',\n",
       " 'iranian',\n",
       " 'franklin',\n",
       " 'barcelona',\n",
       " 'louisiana',\n",
       " 'connecticut',\n",
       " 'uefa',\n",
       " 'carlos',\n",
       " 'antonio',\n",
       " 'kevin',\n",
       " 'jeff',\n",
       " 'pakistani',\n",
       " 'russell',\n",
       " 'oliver',\n",
       " 'haiti',\n",
       " 'wagner',\n",
       " 'prussia',\n",
       " 'austin',\n",
       " 'alan',\n",
       " 'jacques',\n",
       " 'percy',\n",
       " 'clinton',\n",
       " 'celtic',\n",
       " 'nicholas',\n",
       " 'marshall',\n",
       " 'heavily',\n",
       " 'homer',\n",
       " 'arnold',\n",
       " 'hugo',\n",
       " 'annually',\n",
       " 'moore',\n",
       " 'manhattan',\n",
       " 'britannica',\n",
       " 'allen',\n",
       " 'giovanni',\n",
       " 'thailand',\n",
       " 'leeds',\n",
       " 'partly',\n",
       " 'tamil',\n",
       " 'franz',\n",
       " 'shakespeare',\n",
       " 'hans',\n",
       " 'carter',\n",
       " 'gordon',\n",
       " 'rapidly',\n",
       " 'simpson',\n",
       " 'labour',\n",
       " 'otto',\n",
       " 'hindu',\n",
       " 'hercules',\n",
       " 'zeus',\n",
       " 'miami',\n",
       " 'napoleon',\n",
       " 'athens',\n",
       " 'hesse',\n",
       " 'serie',\n",
       " 'ecliptic',\n",
       " 'atlantiques',\n",
       " 'serbia',\n",
       " 'canadiens',\n",
       " 'bengal',\n",
       " 'ernst',\n",
       " 'cleveland',\n",
       " 'forbes',\n",
       " 'jura',\n",
       " 'batista',\n",
       " 'pokã',\n",
       " 'solothurn',\n",
       " 'houston',\n",
       " 'jammu',\n",
       " 'amsterdam',\n",
       " 'baltimore',\n",
       " 'jonathan',\n",
       " 'edmund',\n",
       " 'margaret',\n",
       " 'queensland',\n",
       " 'hampshire',\n",
       " 'norse',\n",
       " 'operas',\n",
       " 'brabant',\n",
       " 'rocky',\n",
       " 'syria',\n",
       " 'commons',\n",
       " 'frankfurt',\n",
       " 'madison',\n",
       " 'overseas',\n",
       " 'romanian',\n",
       " 'kurt',\n",
       " 'josã',\n",
       " 'maritimes',\n",
       " 'augustus',\n",
       " 'shah',\n",
       " 'stuart',\n",
       " 'roosevelt',\n",
       " 'cooper',\n",
       " 'greatly',\n",
       " 'brooklyn',\n",
       " 'stewart',\n",
       " 'maryland',\n",
       " 'mcmahon',\n",
       " 'dakota',\n",
       " 'gradually',\n",
       " 'whedon',\n",
       " 'malaysia',\n",
       " 'kent',\n",
       " 'briefly',\n",
       " 'hamburg',\n",
       " 'sindh',\n",
       " 'fairly',\n",
       " 'instal',\n",
       " 'baptist',\n",
       " 'thompson',\n",
       " 'neighbour',\n",
       " 'peru',\n",
       " 'tuscany',\n",
       " 'thirteen',\n",
       " 'herbert',\n",
       " 'norman',\n",
       " 'oscar',\n",
       " 'jerry',\n",
       " 'venice',\n",
       " 'bouches',\n",
       " 'jimmy',\n",
       " 'louise',\n",
       " 'renault',\n",
       " 'francesco',\n",
       " 'holland',\n",
       " 'slovakia',\n",
       " 'indus',\n",
       " 'leonese',\n",
       " 'mont',\n",
       " 'nasa',\n",
       " 'lithuania',\n",
       " 'sony',\n",
       " 'vincent',\n",
       " 'nelson',\n",
       " 'weimar',\n",
       " 'aberdeen',\n",
       " 'gustav',\n",
       " 'clark',\n",
       " 'julius',\n",
       " 'slavic',\n",
       " 'luis',\n",
       " 'dave',\n",
       " 'naples',\n",
       " 'janeiro',\n",
       " 'reich',\n",
       " 'johnny',\n",
       " 'mccartney',\n",
       " 'barry',\n",
       " 'wright',\n",
       " 'guerrero',\n",
       " 'utah',\n",
       " 'midlands',\n",
       " 'michaels',\n",
       " 'mozart',\n",
       " 'nuremberg',\n",
       " 'luxembourg',\n",
       " 'steven',\n",
       " 'luke',\n",
       " 'pyrã',\n",
       " 'anton',\n",
       " 'suffolk',\n",
       " 'bernard',\n",
       " 'eddie',\n",
       " 'costa',\n",
       " 'maine',\n",
       " 'simpsons',\n",
       " 'arabia',\n",
       " 'gregory',\n",
       " 'oregon',\n",
       " 'emmy',\n",
       " 'gilbert',\n",
       " 'croatia',\n",
       " 'anderson',\n",
       " 'israeli',\n",
       " 'antarctica',\n",
       " 'buddhism',\n",
       " 'strongly',\n",
       " 'knowles',\n",
       " 'wayne',\n",
       " 'ivan',\n",
       " 'newfoundland',\n",
       " 'croatian',\n",
       " 'manitoba',\n",
       " 'inland',\n",
       " 'americas',\n",
       " 'donald',\n",
       " 'internationally',\n",
       " 'buenos',\n",
       " 'suceava',\n",
       " 'vendã',\n",
       " 'leipzig',\n",
       " 'persia',\n",
       " 'khyber',\n",
       " 'newton',\n",
       " 'ronald',\n",
       " 'unesco',\n",
       " 'terry',\n",
       " 'castile',\n",
       " 'thursday',\n",
       " 'barbara',\n",
       " 'successfully',\n",
       " 'somewhat',\n",
       " 'thames',\n",
       " 'isaac',\n",
       " 'orient',\n",
       " 'anglican',\n",
       " 'cameron',\n",
       " 'harvard',\n",
       " 'honour',\n",
       " 'gary',\n",
       " 'subspecies',\n",
       " 'programme',\n",
       " 'thuringia',\n",
       " 'petersburg',\n",
       " 'amazon',\n",
       " 'alps',\n",
       " 'kirby',\n",
       " 'saudi',\n",
       " 'ludwig',\n",
       " 'ryan',\n",
       " 'gabriel',\n",
       " 'victor',\n",
       " 'organise',\n",
       " 'nile',\n",
       " 'georg',\n",
       " 'antarctic',\n",
       " 'dallas',\n",
       " 'columbus',\n",
       " 'franco',\n",
       " 'charlie',\n",
       " 'apollo',\n",
       " 'harris',\n",
       " 'teau',\n",
       " 'salzburg',\n",
       " 'lucy',\n",
       " 'berkeley',\n",
       " 'exclusively',\n",
       " 'tibetan',\n",
       " 'tampa',\n",
       " 'flemish',\n",
       " 'warsaw',\n",
       " 'sierra',\n",
       " 'nazis',\n",
       " 'richmond',\n",
       " 'nevada',\n",
       " 'publicly',\n",
       " 'orleans',\n",
       " 'dublin',\n",
       " 'extratropical',\n",
       " 'sheffield',\n",
       " 'trinity',\n",
       " 'brandenburg',\n",
       " 'significantly',\n",
       " 'heinrich',\n",
       " 'santiago',\n",
       " 'hugh',\n",
       " 'murray',\n",
       " 'marco',\n",
       " 'pakhtunkhwa',\n",
       " 'campbell',\n",
       " 'similarly',\n",
       " 'josãƒ',\n",
       " 'ubuntu',\n",
       " 'kane',\n",
       " 'iceland',\n",
       " 'sebastian',\n",
       " 'ralph',\n",
       " 'nova',\n",
       " 'ukrainian',\n",
       " 'ernest',\n",
       " 'michelle',\n",
       " 'bulgaria',\n",
       " 'jason',\n",
       " 'colombia',\n",
       " 'sicily',\n",
       " 'bundesliga',\n",
       " 'buffy',\n",
       " 'raymond',\n",
       " 'harbour',\n",
       " 'easter',\n",
       " 'fourteen',\n",
       " 'mali',\n",
       " 'batman',\n",
       " 'stockholm',\n",
       " 'chakwal',\n",
       " 'soundgarden',\n",
       " 'sussex',\n",
       " 'beijing',\n",
       " 'santo',\n",
       " 'alex',\n",
       " 'panama',\n",
       " 'blackhawks',\n",
       " 'ferrari',\n",
       " 'vince',\n",
       " 'carlo',\n",
       " 'morgan',\n",
       " 'yugoslavia',\n",
       " 'manuel',\n",
       " 'tyler',\n",
       " 'emirates',\n",
       " 'saxon',\n",
       " 'bermuda',\n",
       " 'hume',\n",
       " 'prussian',\n",
       " 'chãƒ',\n",
       " 'neil',\n",
       " 'prague',\n",
       " 'anastasia',\n",
       " 'tasmania',\n",
       " 'alberta',\n",
       " 'debian',\n",
       " 'bristol',\n",
       " 'volkswagen',\n",
       " 'lanka',\n",
       " 'alexandra',\n",
       " 'lebanon',\n",
       " 'twentieth',\n",
       " 'craig',\n",
       " 'serbian',\n",
       " 'tuesday',\n",
       " 'ussr',\n",
       " 'hudson',\n",
       " 'denis',\n",
       " 'marcus',\n",
       " 'loosely',\n",
       " 'abraham',\n",
       " 'jurassic',\n",
       " 'jamaica',\n",
       " 'tommy',\n",
       " 'ottawa',\n",
       " 'tehsils',\n",
       " 'pedro',\n",
       " 'theodore',\n",
       " 'increasingly',\n",
       " 'baltic',\n",
       " 'nicolas',\n",
       " 'orlando',\n",
       " 'joan',\n",
       " 'dolj',\n",
       " 'sainte',\n",
       " 'venezuela',\n",
       " 'necessarily',\n",
       " 'norfolk',\n",
       " 'facto',\n",
       " 'partially',\n",
       " 'sega',\n",
       " 'spencer',\n",
       " 'parker',\n",
       " 'arabian',\n",
       " 'susan',\n",
       " 'santos',\n",
       " 'esperanto',\n",
       " 'wolfgang',\n",
       " 'leonard',\n",
       " 'keith',\n",
       " 'lucas',\n",
       " 'helen',\n",
       " 'medici',\n",
       " 'adolf',\n",
       " 'valencia',\n",
       " 'alexandria',\n",
       " 'avon',\n",
       " 'warren',\n",
       " 'windsor',\n",
       " 'hermann',\n",
       " 'victorian',\n",
       " 'tibet',\n",
       " 'powell',\n",
       " 'arrondissement',\n",
       " 'grande',\n",
       " 'rogers',\n",
       " 'surrey',\n",
       " 'pradesh',\n",
       " 'delhi',\n",
       " 'germanic',\n",
       " 'edgar',\n",
       " 'vegas',\n",
       " 'ethiopia',\n",
       " 'fernando',\n",
       " 'bryan',\n",
       " 'leonardo',\n",
       " 'europeans',\n",
       " 'latvia',\n",
       " 'burton',\n",
       " 'independently',\n",
       " 'devon',\n",
       " 'burma',\n",
       " 'andy',\n",
       " 'georges',\n",
       " 'chelsea',\n",
       " 'thurgau',\n",
       " 'commercially',\n",
       " 'germans',\n",
       " 'antilles',\n",
       " 'nigeria',\n",
       " 'claude',\n",
       " 'shawn',\n",
       " 'fischer',\n",
       " 'paramount',\n",
       " 'jacob',\n",
       " 'maurice',\n",
       " 'monaco',\n",
       " 'broadway',\n",
       " 'cornell',\n",
       " 'versailles',\n",
       " 'sanskrit',\n",
       " 'bruins',\n",
       " 'randy',\n",
       " 'morris',\n",
       " 'pluto',\n",
       " 'floyd',\n",
       " 'wallace',\n",
       " 'perth',\n",
       " 'collins',\n",
       " 'silva',\n",
       " 'subfamily',\n",
       " 'brighton',\n",
       " 'isabella',\n",
       " 'newcastle',\n",
       " 'sudan',\n",
       " 'lorenzo',\n",
       " 'tico',\n",
       " 'jose',\n",
       " 'luigi',\n",
       " 'aragon',\n",
       " 'forever',\n",
       " 'indians',\n",
       " 'harold',\n",
       " 'nixon',\n",
       " 'andrea',\n",
       " 'fred',\n",
       " 'hainaut',\n",
       " 'estonia',\n",
       " 'stanford',\n",
       " 'ncaa',\n",
       " 'somerset',\n",
       " 'nascar',\n",
       " 'malta',\n",
       " 'lithuanian',\n",
       " 'romeo',\n",
       " 'bart',\n",
       " 'constantine',\n",
       " 'hughes',\n",
       " 'wednesday',\n",
       " 'simultaneously',\n",
       " 'kate',\n",
       " 'organisation',\n",
       " 'calvin',\n",
       " 'waterloo',\n",
       " 'legally',\n",
       " 'properly',\n",
       " 'rudolf',\n",
       " 'calgary',\n",
       " 'hindenburg',\n",
       " 'pete',\n",
       " 'toyota',\n",
       " 'antwerp',\n",
       " 'colin',\n",
       " 'karachi',\n",
       " 'limburg',\n",
       " 'webster',\n",
       " 'keynes',\n",
       " 'azerbaijan',\n",
       " 'apparently',\n",
       " 'pablo',\n",
       " 'helsinki',\n",
       " 'luther',\n",
       " 'beyoncã',\n",
       " 'melina',\n",
       " 'kreis',\n",
       " 'slovenia',\n",
       " 'mecklenburg',\n",
       " 'boeing',\n",
       " 'monte',\n",
       " 'danny',\n",
       " 'domingo',\n",
       " 'greg',\n",
       " 'reagan',\n",
       " 'chevrolet',\n",
       " 'salvador',\n",
       " 'greeks',\n",
       " 'tanzania',\n",
       " 'slovak',\n",
       " 'indonesian',\n",
       " 'antoine',\n",
       " 'morocco',\n",
       " 'aaron',\n",
       " 'normandy',\n",
       " 'buddha',\n",
       " 'derbyshire',\n",
       " 'maxwell',\n",
       " 'pichilemu',\n",
       " 'edmonton',\n",
       " 'scala',\n",
       " 'coburg',\n",
       " 'additionally',\n",
       " 'cruz',\n",
       " 'watson',\n",
       " 'chilean',\n",
       " 'ville',\n",
       " 'ruth',\n",
       " 'cyrillic',\n",
       " 'chad',\n",
       " 'indianapolis',\n",
       " 'rica',\n",
       " 'lisa',\n",
       " 'osaka',\n",
       " 'aang',\n",
       " 'dante',\n",
       " 'gettysburg',\n",
       " 'lulu',\n",
       " 'nato',\n",
       " 'princeton',\n",
       " 'mongol',\n",
       " 'vladimir',\n",
       " 'sunderland',\n",
       " 'ming',\n",
       " 'phil',\n",
       " 'lancashire',\n",
       " 'yearly',\n",
       " 'oblast',\n",
       " 'extensively',\n",
       " 'walker',\n",
       " 'thai',\n",
       " 'owen',\n",
       " 'inca',\n",
       " 'andreas',\n",
       " 'bavarian',\n",
       " 'nineteenth',\n",
       " 'trojan',\n",
       " 'marina',\n",
       " 'nancy',\n",
       " 'rttemberg',\n",
       " 'oslo',\n",
       " 'grameen',\n",
       " 'benoit',\n",
       " 'haute',\n",
       " 'amherst',\n",
       " 'greenland',\n",
       " 'bismarck',\n",
       " 'roberto',\n",
       " 'predominantly',\n",
       " 'rhode',\n",
       " 'michel',\n",
       " 'gotha',\n",
       " 'hanover',\n",
       " 'unitary',\n",
       " 'cincinnati',\n",
       " 'stallone',\n",
       " 'todd',\n",
       " 'viola',\n",
       " 'disambiguation',\n",
       " 'yokohama',\n",
       " 'eleventh',\n",
       " 'edwards',\n",
       " 'sean',\n",
       " 'malcolm',\n",
       " 'manga',\n",
       " 'baja',\n",
       " 'albany',\n",
       " 'denver',\n",
       " 'cole',\n",
       " 'czechoslovakia',\n",
       " 'amadeus',\n",
       " 'sexually',\n",
       " 'connor',\n",
       " 'montana',\n",
       " 'congo',\n",
       " 'catholics',\n",
       " 'ipswich',\n",
       " 'felix',\n",
       " 'memphis',\n",
       " 'copenhagen']"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aoa_complement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3306"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aoa_intersect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state',\n",
       " 'bear',\n",
       " 'city',\n",
       " 'know',\n",
       " 'unite',\n",
       " 'department',\n",
       " 'play',\n",
       " 'region',\n",
       " 'commune',\n",
       " 'time',\n",
       " 'north',\n",
       " 'world',\n",
       " 'football',\n",
       " 'call',\n",
       " 'include',\n",
       " 'people',\n",
       " 'south',\n",
       " 'game',\n",
       " 'work',\n",
       " 'team']"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aoa_intersect[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2965"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([word for word in aoa_intersect if word in concrete_intersect])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in aoa_intersect:\n",
    "    word_vectors[word] = word_vectors[word] * AoA[AoA['Word']==word]['AoA_Kup_lem'].values/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.05170861e-01, -1.66753866e-03,  1.37690033e-04,  3.97680737e-02,\n",
       "       -1.94128137e-02, -4.88906987e-02,  2.66123447e-03,  5.45402169e-02,\n",
       "        1.93108455e-03,  2.63384599e-02,  1.31162666e-02, -5.93766989e-03,\n",
       "       -9.18143019e-02, -8.28372762e-02,  9.57485363e-02,  1.12751964e-02,\n",
       "       -2.93028187e-02,  2.49936208e-02,  5.48743866e-02,  7.44378790e-02,\n",
       "       -2.40801759e-02,  3.97137366e-02,  2.08030213e-02,  3.00983358e-02,\n",
       "        3.44364420e-02, -6.45714104e-02,  9.78401154e-02, -4.29256409e-02,\n",
       "       -1.73054298e-03,  1.53231639e-02,  8.28398466e-02, -1.67641304e-02,\n",
       "        2.77930945e-02,  4.93132696e-02, -1.39136473e-02,  1.48767298e-02,\n",
       "        3.11606843e-02, -1.40408454e-02,  5.28065767e-03, -7.80985802e-02,\n",
       "       -1.04491614e-01, -4.30216081e-02,  2.67536677e-02, -1.50992453e-01,\n",
       "        4.09575887e-02, -1.12339500e-02,  6.14935383e-02,  5.35262842e-03,\n",
       "       -7.40834847e-02,  2.44722292e-02, -9.22993347e-02,  5.28227314e-02,\n",
       "       -5.38969506e-03,  8.02509710e-02, -7.65443146e-02, -3.92334461e-02,\n",
       "        8.34349990e-02,  5.57681136e-02,  2.56329551e-02, -4.87071462e-03,\n",
       "       -4.72075678e-02,  1.04140490e-02, -9.99030620e-02,  2.28323066e-03,\n",
       "       -1.67115442e-02, -2.46606208e-02,  3.73994187e-02,  1.84825994e-02,\n",
       "        2.70522479e-02,  6.92991316e-02,  2.72095259e-02, -1.93700176e-02,\n",
       "        3.76823172e-02, -3.93753871e-02,  4.82711270e-02,  4.58121039e-02,\n",
       "        6.37062415e-02, -5.84561042e-02, -2.71139946e-02,  6.65446371e-02,\n",
       "        7.64024258e-02,  5.26173748e-02, -8.47942084e-02,  4.15775971e-03,\n",
       "       -3.58831957e-02, -1.22827943e-02, -8.68822336e-02, -8.54199603e-02,\n",
       "       -7.90187642e-02,  5.70657488e-04,  1.28690815e-02, -5.13709746e-02,\n",
       "        2.98149381e-02, -6.22222237e-02, -7.57568404e-02,  3.64740528e-02,\n",
       "       -5.55040734e-03,  2.06058044e-02, -1.44274794e-02, -7.10418150e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dense_features(tokenized_text,word_vectors):\n",
    "    dense_list=[]\n",
    "    words=[]\n",
    "    for _ in tokenized_text: \n",
    "        words =[word for word in _ if word in word_vectors.key_to_index]\n",
    "        \n",
    "        if len(words) >0:\n",
    "            dense_list.append(np.mean(word_vectors[words],axis=0))\n",
    "            \n",
    "        else: \n",
    "            dense_list.append(np.zeros(word_vectors.vector_size))\n",
    "            \n",
    "    return np.array(dense_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wv = generate_dense_features(tokenized_text_train,word_vectors)\n",
    "X_test_wv = generate_dense_features(tokenized_text_test,word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333414, 100)"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_wv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_wv = LogisticRegression(random_state=RANDOM_SEED,max_iter=1000).fit(X_train_wv,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.561676704177364"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,lr_wv.predict(X_test_wv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "vectorizer = TfidfVectorizer(analyzer='word',tokenizer=dummy_fun, preprocessor=dummy_fun, token_pattern=r'(?u)\\b\\w\\w+__\\([\\w\\s]*\\)')\n",
    "X_train_transform = vectorizer.fit_transform(tokenized_text_train)\n",
    "X_test_transform  = vectorizer.transform(tokenized_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120723"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a_nx',\n",
       " 'aabout',\n",
       " 'aabye',\n",
       " 'aach',\n",
       " 'aachen',\n",
       " 'aafc',\n",
       " 'aage',\n",
       " 'aaiil',\n",
       " 'aalborg',\n",
       " 'aalen',\n",
       " 'aaliyah',\n",
       " 'aaliyahs',\n",
       " 'aall',\n",
       " 'aalst',\n",
       " 'aalten',\n",
       " 'aalter',\n",
       " 'aalto',\n",
       " 'aames',\n",
       " 'aamir',\n",
       " 'aang',\n",
       " 'aangã',\n",
       " 'aapep',\n",
       " 'aarau',\n",
       " 'aarberg',\n",
       " 'aarburg',\n",
       " 'aarc',\n",
       " 'aarde',\n",
       " 'aardman',\n",
       " 'aardsma',\n",
       " 'aardvark',\n",
       " 'aardvarks',\n",
       " 'aare',\n",
       " 'aargau',\n",
       " 'aargauer',\n",
       " 'aarhus',\n",
       " 'aaron',\n",
       " 'aaroni',\n",
       " 'aarons',\n",
       " 'aarre',\n",
       " 'aarschot',\n",
       " 'aarseth',\n",
       " 'aartselaar',\n",
       " 'aarwangen',\n",
       " 'aasen',\n",
       " 'aashurah',\n",
       " 'aast',\n",
       " 'aastana',\n",
       " 'aave',\n",
       " 'ababa',\n",
       " 'ababba',\n",
       " 'ababda',\n",
       " 'abac',\n",
       " 'abacada',\n",
       " 'abaci',\n",
       " 'aback',\n",
       " 'abacus',\n",
       " 'abacuses',\n",
       " 'abad',\n",
       " 'abagnale',\n",
       " 'abahutu',\n",
       " 'abaj',\n",
       " 'abajo',\n",
       " 'abakan',\n",
       " 'abakanskoye',\n",
       " 'abal',\n",
       " 'abalo',\n",
       " 'abalone',\n",
       " 'abando',\n",
       " 'abandon',\n",
       " 'abandonded',\n",
       " 'abandoned',\n",
       " 'abandoning',\n",
       " 'abandonment',\n",
       " 'abandons',\n",
       " 'abarat',\n",
       " 'abassi',\n",
       " 'abated',\n",
       " 'abating',\n",
       " 'abattoirs',\n",
       " 'abatutsi',\n",
       " 'abauzit',\n",
       " 'abavo',\n",
       " 'abazhou',\n",
       " 'abaãºj',\n",
       " 'abba',\n",
       " 'abbado',\n",
       " 'abbadon',\n",
       " 'abbados',\n",
       " 'abbandando',\n",
       " 'abbas',\n",
       " 'abbasi',\n",
       " 'abbasid',\n",
       " 'abbasids',\n",
       " 'abbasies',\n",
       " 'abbass',\n",
       " 'abbassid',\n",
       " 'abbay',\n",
       " 'abbaye',\n",
       " 'abbe',\n",
       " 'abbeville',\n",
       " 'abbey',\n",
       " 'abbeydale',\n",
       " 'abbeys',\n",
       " 'abbiamo',\n",
       " 'abbiategrasso',\n",
       " 'abbiati',\n",
       " 'abbie',\n",
       " 'abbiss',\n",
       " 'abbondancieri',\n",
       " 'abbondanzieri',\n",
       " 'abbondio',\n",
       " 'abbot',\n",
       " 'abbotsford',\n",
       " 'abbotsinch',\n",
       " 'abbott',\n",
       " 'abbottabad',\n",
       " 'abbotts',\n",
       " 'abbr',\n",
       " 'abbrev',\n",
       " 'abbreviate',\n",
       " 'abbreviated',\n",
       " 'abbreviating',\n",
       " 'abbreviation',\n",
       " 'abbreviations',\n",
       " 'abbruzzese',\n",
       " 'abbs',\n",
       " 'abbud',\n",
       " 'abby',\n",
       " 'abbã',\n",
       " 'abbãƒ',\n",
       " 'abcd',\n",
       " 'abcs',\n",
       " 'abdacom',\n",
       " 'abdal',\n",
       " 'abdallah',\n",
       " 'abdel',\n",
       " 'abdelazar',\n",
       " 'abdelhafid',\n",
       " 'abdeljalil',\n",
       " 'abdelwahab',\n",
       " 'abdera',\n",
       " 'abderathe',\n",
       " 'abdest',\n",
       " 'abdi',\n",
       " 'abdicate',\n",
       " 'abdicated',\n",
       " 'abdicates',\n",
       " 'abdicating',\n",
       " 'abdicatio',\n",
       " 'abdication',\n",
       " 'abdirashid',\n",
       " 'abdolah',\n",
       " 'abdollah',\n",
       " 'abdomen',\n",
       " 'abdomens',\n",
       " 'abdominal',\n",
       " 'abdominis',\n",
       " 'abdou',\n",
       " 'abdoulaye',\n",
       " 'abdu',\n",
       " 'abduct',\n",
       " 'abducted',\n",
       " 'abducting',\n",
       " 'abduction',\n",
       " 'abdul',\n",
       " 'abdulahi',\n",
       " 'abdulaziz',\n",
       " 'abdullah',\n",
       " 'abdullaziz',\n",
       " 'abdun',\n",
       " 'abdurrahman',\n",
       " 'abdus',\n",
       " 'abeba',\n",
       " 'abel',\n",
       " 'abela',\n",
       " 'abelard',\n",
       " 'abelardo',\n",
       " 'abele',\n",
       " 'abelian',\n",
       " 'abelisaurid',\n",
       " 'abella',\n",
       " 'abells',\n",
       " 'abelmoschus',\n",
       " 'abelshauser',\n",
       " 'abelson',\n",
       " 'abendroth',\n",
       " 'abenobashi',\n",
       " 'abenon',\n",
       " 'abenteuer',\n",
       " 'aber',\n",
       " 'abercrombie',\n",
       " 'abercromby',\n",
       " 'aberdeen',\n",
       " 'aberdeenshire',\n",
       " 'aberdeenshires',\n",
       " 'aberdour',\n",
       " 'aberdovey',\n",
       " 'aberdyfi',\n",
       " 'aberfan',\n",
       " 'aberford',\n",
       " 'aberfoyle',\n",
       " 'abergavenny',\n",
       " 'abergement',\n",
       " 'abergynolwyn',\n",
       " 'aberlin',\n",
       " 'aberrant',\n",
       " 'aberration',\n",
       " 'aberrations',\n",
       " 'aberson',\n",
       " 'abert',\n",
       " 'abertay',\n",
       " 'aberystwyth',\n",
       " 'aberystwyththe',\n",
       " 'abet',\n",
       " 'abetted',\n",
       " 'abeyie',\n",
       " 'abgar',\n",
       " 'abgebrã',\n",
       " 'abgrenzung',\n",
       " 'abhainn',\n",
       " 'abhanga',\n",
       " 'abhangas',\n",
       " 'abhinav',\n",
       " 'abhiras',\n",
       " 'abhorrã',\n",
       " 'abhors',\n",
       " 'abid',\n",
       " 'abidal',\n",
       " 'abide',\n",
       " 'abided',\n",
       " 'abides',\n",
       " 'abidine',\n",
       " 'abiding',\n",
       " 'abidjan',\n",
       " 'abidos',\n",
       " 'abierta',\n",
       " 'abies',\n",
       " 'abigail',\n",
       " 'abilene',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'abimael',\n",
       " 'abin',\n",
       " 'abingdon',\n",
       " 'abington',\n",
       " 'abio',\n",
       " 'abiogenesis',\n",
       " 'abiotic',\n",
       " 'abiotically',\n",
       " 'abire',\n",
       " 'abisalovich',\n",
       " 'abispa',\n",
       " 'abitur',\n",
       " 'abiword',\n",
       " 'abjadi',\n",
       " 'abjads',\n",
       " 'abjuration',\n",
       " 'abkai',\n",
       " 'abkco',\n",
       " 'abkhaz',\n",
       " 'abkhazia',\n",
       " 'abkhazian',\n",
       " 'ablanedo',\n",
       " 'ablation',\n",
       " 'ablative',\n",
       " 'ablaze',\n",
       " 'able',\n",
       " 'abled',\n",
       " 'ablest',\n",
       " 'ablon',\n",
       " 'abloy',\n",
       " 'ablutions',\n",
       " 'abma',\n",
       " 'abner',\n",
       " 'abney',\n",
       " 'abnicum',\n",
       " 'abnormal',\n",
       " 'abnormalities',\n",
       " 'abnormality',\n",
       " 'abnormally',\n",
       " 'aboard',\n",
       " 'abobrãƒ',\n",
       " 'abode',\n",
       " 'abol',\n",
       " 'abolish',\n",
       " 'abolished',\n",
       " 'abolishes',\n",
       " 'abolishing',\n",
       " 'abolishment',\n",
       " 'abolition',\n",
       " 'abolitionism',\n",
       " 'abolitionist',\n",
       " 'abolitionists',\n",
       " 'abominations',\n",
       " 'abong',\n",
       " 'aboolian',\n",
       " 'aboot',\n",
       " 'aboriginal',\n",
       " 'aboriginals',\n",
       " 'aborigine',\n",
       " 'aborigines',\n",
       " 'aborted',\n",
       " 'abortifacient',\n",
       " 'abortion',\n",
       " 'abortions',\n",
       " 'abortive',\n",
       " 'abou',\n",
       " 'abound',\n",
       " 'abounding',\n",
       " 'aboutus',\n",
       " 'aboveground',\n",
       " 'abovyan',\n",
       " 'abra',\n",
       " 'abracadabra',\n",
       " 'abraham',\n",
       " 'abrahamic',\n",
       " 'abrahams',\n",
       " 'abram',\n",
       " 'abramczik',\n",
       " 'abramovich',\n",
       " 'abrams',\n",
       " 'abrantes',\n",
       " 'abrasion',\n",
       " 'abrasions',\n",
       " 'abraxas',\n",
       " 'abraãƒ',\n",
       " 'abreaction',\n",
       " 'abreu',\n",
       " 'abrictosaurus',\n",
       " 'abridged',\n",
       " 'abridging',\n",
       " 'abridgment',\n",
       " 'abril',\n",
       " 'abroad',\n",
       " 'abrogant',\n",
       " 'abrogated',\n",
       " 'abrogates',\n",
       " 'abronia',\n",
       " 'abrowse',\n",
       " 'abrsm',\n",
       " 'abrupt',\n",
       " 'abruptly',\n",
       " 'abruzzi',\n",
       " 'abruzzo',\n",
       " 'abrã',\n",
       " 'abscess',\n",
       " 'abscesses',\n",
       " 'abscissa',\n",
       " 'abscond',\n",
       " 'absconded',\n",
       " 'absecon',\n",
       " 'absence',\n",
       " 'absences',\n",
       " 'absent',\n",
       " 'absentee',\n",
       " 'absentia',\n",
       " 'absentpelagic',\n",
       " 'absinth',\n",
       " 'absinthe',\n",
       " 'absinthes',\n",
       " 'absinthium',\n",
       " 'absolut',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absolutepunk',\n",
       " 'absolution',\n",
       " 'absolutist',\n",
       " 'absolutive',\n",
       " 'absolved',\n",
       " 'absorb',\n",
       " 'absorbance',\n",
       " 'absorbances',\n",
       " 'absorbed',\n",
       " 'absorbent',\n",
       " 'absorber',\n",
       " 'absorbers',\n",
       " 'absorbing',\n",
       " 'absorbs',\n",
       " 'absorption',\n",
       " 'absorptive',\n",
       " 'abstain',\n",
       " 'abstained',\n",
       " 'abstaining',\n",
       " 'abstention',\n",
       " 'abstentionism',\n",
       " 'abstinence',\n",
       " 'abstract',\n",
       " 'abstracting',\n",
       " 'abstraction',\n",
       " 'abstractionists',\n",
       " 'abstractions',\n",
       " 'abstracts',\n",
       " 'absurd',\n",
       " 'absurde',\n",
       " 'absurdism',\n",
       " 'absurdist',\n",
       " 'absurdity',\n",
       " 'abtwil',\n",
       " 'abub',\n",
       " 'abubakari',\n",
       " 'abugida',\n",
       " 'abugidas',\n",
       " 'abuja',\n",
       " 'abukuma',\n",
       " 'abul',\n",
       " 'abuladze',\n",
       " 'abulkhair',\n",
       " 'abulm',\n",
       " 'abuls',\n",
       " 'abulã',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'abundantly',\n",
       " 'aburish',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abuses',\n",
       " 'abusing',\n",
       " 'abusive',\n",
       " 'abusively',\n",
       " 'abut',\n",
       " 'abutere',\n",
       " 'abutments',\n",
       " 'abuts',\n",
       " 'abutting',\n",
       " 'abydos',\n",
       " 'abyss',\n",
       " 'abyssal',\n",
       " 'abzekh',\n",
       " 'abãƒ',\n",
       " 'acacia',\n",
       " 'acacias',\n",
       " 'acad',\n",
       " 'acadamias',\n",
       " 'academe',\n",
       " 'academia',\n",
       " 'academic',\n",
       " 'academical',\n",
       " 'academically',\n",
       " 'academician',\n",
       " 'academics',\n",
       " 'academie',\n",
       " 'academies',\n",
       " 'academkniga',\n",
       " 'academy',\n",
       " 'acadia',\n",
       " 'acadians',\n",
       " 'acadã',\n",
       " 'acadãƒ',\n",
       " 'acamprosate',\n",
       " 'acanthaceae',\n",
       " 'acanthaclisinae',\n",
       " 'acanthocephala',\n",
       " 'acanthodii',\n",
       " 'acanthomintha',\n",
       " 'acanthomyops',\n",
       " 'acanthophis',\n",
       " 'acanthophylla',\n",
       " 'acanthostega',\n",
       " 'acapulco',\n",
       " 'acari',\n",
       " 'acarology',\n",
       " 'acaso',\n",
       " 'acasta',\n",
       " 'acca',\n",
       " 'accademia',\n",
       " 'accadian',\n",
       " 'accede',\n",
       " 'acceded',\n",
       " 'accedes',\n",
       " 'acceding',\n",
       " 'accelerate',\n",
       " 'accelerated',\n",
       " 'accelerates',\n",
       " 'accelerateur',\n",
       " 'accelerating',\n",
       " 'acceleration',\n",
       " 'accelerations',\n",
       " 'accelerator',\n",
       " 'accelerators',\n",
       " 'acceleratorsã',\n",
       " 'accelerometer',\n",
       " 'accelerometers',\n",
       " 'accends',\n",
       " 'accent',\n",
       " 'accented',\n",
       " 'accents',\n",
       " 'accentschurmann',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'acceptably',\n",
       " 'acceptance',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'acceptor',\n",
       " 'accepts',\n",
       " 'access',\n",
       " 'accessed',\n",
       " 'accesses',\n",
       " 'accessibility',\n",
       " 'accessible',\n",
       " 'accessing',\n",
       " 'accession',\n",
       " 'accessories',\n",
       " 'accessory',\n",
       " 'acchieved',\n",
       " 'acchouhouri',\n",
       " 'acciaiuoli',\n",
       " 'acciarito',\n",
       " 'accident',\n",
       " 'accidental',\n",
       " 'accidentally',\n",
       " 'accidentals',\n",
       " 'accidents',\n",
       " 'accidie',\n",
       " 'accies',\n",
       " 'acciona',\n",
       " 'accipiter',\n",
       " 'accipitridae',\n",
       " 'acciã³n',\n",
       " 'acciãƒ',\n",
       " 'acclaim',\n",
       " 'acclaimed',\n",
       " 'acclamation',\n",
       " 'acclamations',\n",
       " 'acclimatation',\n",
       " 'acclimate',\n",
       " 'acclimatisation',\n",
       " 'accolade',\n",
       " 'accolades',\n",
       " 'accomack',\n",
       " 'accommodate',\n",
       " 'accommodated',\n",
       " 'accommodates',\n",
       " 'accommodation',\n",
       " 'accommodations',\n",
       " 'accommodative',\n",
       " 'accompanied',\n",
       " 'accompanies',\n",
       " 'accompaniment',\n",
       " 'accompanist',\n",
       " 'accompany',\n",
       " 'accompanying',\n",
       " 'accomping',\n",
       " 'accomplice',\n",
       " 'accomplices',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishers',\n",
       " 'accomplishes',\n",
       " 'accomplishing',\n",
       " 'accomplishment',\n",
       " 'accomplishments',\n",
       " 'accomptant',\n",
       " 'accons',\n",
       " 'accord',\n",
       " 'accordance',\n",
       " 'accorded',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'accordion',\n",
       " 'accordionist',\n",
       " 'accordo',\n",
       " 'accords',\n",
       " 'accosted',\n",
       " 'accouchement',\n",
       " 'account',\n",
       " 'accountability',\n",
       " 'accountable',\n",
       " 'accountancy',\n",
       " 'accountant',\n",
       " 'accountants',\n",
       " 'accounted',\n",
       " 'accounting',\n",
       " 'accountn',\n",
       " 'accounts',\n",
       " 'accountsare',\n",
       " 'accous',\n",
       " 'accouterments',\n",
       " 'accoutrement',\n",
       " 'accoyer',\n",
       " 'accra',\n",
       " 'accreditation',\n",
       " 'accredited',\n",
       " 'accrediting',\n",
       " 'accreditor',\n",
       " 'accreta',\n",
       " 'accrete',\n",
       " 'accreted',\n",
       " 'accreting',\n",
       " 'accretion',\n",
       " 'accrington',\n",
       " 'accross',\n",
       " 'accrued',\n",
       " 'acculturate',\n",
       " 'accumbens',\n",
       " 'accumulate',\n",
       " 'accumulated',\n",
       " 'accumulates',\n",
       " 'accumulating',\n",
       " 'accumulation',\n",
       " 'accumulations',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accusamus',\n",
       " 'accusantium',\n",
       " 'accusation',\n",
       " 'accusations',\n",
       " 'accuse',\n",
       " 'accused',\n",
       " 'accuses',\n",
       " 'accusing',\n",
       " 'accustomed',\n",
       " 'acedia',\n",
       " 'aceldama',\n",
       " 'aceman',\n",
       " 'acephala',\n",
       " 'acequia',\n",
       " 'acer',\n",
       " 'aceraceae',\n",
       " 'aceramic',\n",
       " 'acerbic',\n",
       " 'acerbo',\n",
       " 'aces',\n",
       " 'acetaldehyde',\n",
       " 'acetaminophen',\n",
       " 'acetate',\n",
       " 'acetic',\n",
       " 'acetobacter',\n",
       " 'acetone',\n",
       " 'acetyl',\n",
       " 'acetylate',\n",
       " 'acetylation',\n",
       " 'acetylcholine',\n",
       " 'acetylene',\n",
       " 'acetylide',\n",
       " 'acetylsalicylic',\n",
       " 'acevedo',\n",
       " 'achab',\n",
       " 'achaea',\n",
       " 'achaemenid',\n",
       " 'achaius',\n",
       " 'achard',\n",
       " 'acharya',\n",
       " 'achawãƒ',\n",
       " 'ache',\n",
       " 'achebe',\n",
       " 'achelate',\n",
       " 'acheloos',\n",
       " 'achelous',\n",
       " 'achenbach',\n",
       " 'achenes',\n",
       " 'acheron',\n",
       " 'acherontia',\n",
       " 'achery',\n",
       " 'aches',\n",
       " 'acheulean',\n",
       " 'achhim',\n",
       " 'achi',\n",
       " 'achicourt',\n",
       " 'achiet',\n",
       " 'achievable',\n",
       " 'achieve',\n",
       " 'achieved',\n",
       " 'achievement',\n",
       " 'achievements',\n",
       " 'achieves',\n",
       " 'achieving',\n",
       " 'achila',\n",
       " 'achiles',\n",
       " 'achille',\n",
       " 'achillea',\n",
       " 'achilles',\n",
       " 'achillobator',\n",
       " 'achim',\n",
       " 'aching',\n",
       " 'achiote',\n",
       " 'achiral',\n",
       " 'achlorhydria',\n",
       " 'achmad',\n",
       " 'achmed',\n",
       " 'achna',\n",
       " 'achoholic',\n",
       " 'acholi',\n",
       " 'achondrite',\n",
       " 'achondroplasia',\n",
       " 'achondroplastic',\n",
       " 'achonry',\n",
       " 'achromasia',\n",
       " 'achromatopsia',\n",
       " 'achromatosis',\n",
       " 'achromia',\n",
       " 'achterhoek',\n",
       " 'achtice',\n",
       " 'achtung',\n",
       " 'achy',\n",
       " 'achzarit',\n",
       " 'achã',\n",
       " 'achãƒ',\n",
       " 'acib',\n",
       " 'acid',\n",
       " 'acidic',\n",
       " 'acidification',\n",
       " 'acidified',\n",
       " 'acidify',\n",
       " 'acidity',\n",
       " 'acidosis',\n",
       " 'acids',\n",
       " 'acinar',\n",
       " 'acis',\n",
       " 'ackerman',\n",
       " 'ackery',\n",
       " 'ackley',\n",
       " 'acklins',\n",
       " 'acknowledge',\n",
       " 'acknowledged',\n",
       " 'acknowledgement',\n",
       " 'acknowledges',\n",
       " 'acknowledging',\n",
       " 'acknowledgment',\n",
       " 'acknowledgments',\n",
       " 'ackworth',\n",
       " 'acland',\n",
       " 'aclare',\n",
       " 'acme',\n",
       " 'acmi',\n",
       " 'acne',\n",
       " 'acnielsen',\n",
       " 'acolyte',\n",
       " 'acolytes',\n",
       " 'acomyinae',\n",
       " 'acomys',\n",
       " 'aconcagua',\n",
       " 'aconite',\n",
       " 'aconitum',\n",
       " 'acorah',\n",
       " 'acorn',\n",
       " 'acorns',\n",
       " 'acosta',\n",
       " 'acou',\n",
       " 'acoustic',\n",
       " 'acoustical',\n",
       " 'acoustically',\n",
       " 'acoustician',\n",
       " 'acousticly',\n",
       " 'acoustics',\n",
       " 'acpb',\n",
       " 'acquaintance',\n",
       " 'acquaintances',\n",
       " 'acquainted',\n",
       " 'acquarossa',\n",
       " 'acqueville',\n",
       " 'acquiesce',\n",
       " 'acquiesces',\n",
       " 'acquiescing',\n",
       " 'acquieses',\n",
       " 'acquin',\n",
       " 'acquire',\n",
       " 'acquired',\n",
       " 'acquires',\n",
       " 'acquiring',\n",
       " 'acquisition',\n",
       " 'acquisitions',\n",
       " 'acquittal',\n",
       " 'acquitted',\n",
       " 'acre',\n",
       " 'acrea',\n",
       " 'acrelãƒ',\n",
       " 'acres',\n",
       " 'acrisius',\n",
       " 'acritarch',\n",
       " 'acritarchs',\n",
       " 'acrobat',\n",
       " 'acrobates',\n",
       " 'acrobatic',\n",
       " 'acrobatics',\n",
       " 'acrobaticã',\n",
       " 'acrobats',\n",
       " 'acron',\n",
       " 'acronis',\n",
       " 'acronym',\n",
       " 'acronymic',\n",
       " 'acronymous',\n",
       " 'acronyms',\n",
       " 'acropolis',\n",
       " 'acrossmanhattan',\n",
       " 'acrylic',\n",
       " 'acrymia',\n",
       " 'acst',\n",
       " 'acta',\n",
       " 'acte',\n",
       " 'acted',\n",
       " 'actes',\n",
       " 'actias',\n",
       " 'actin',\n",
       " 'acting',\n",
       " 'actinide',\n",
       " 'actinides',\n",
       " 'actinidia',\n",
       " 'actinium',\n",
       " 'actinobacteria',\n",
       " 'actinoid',\n",
       " 'actinolite',\n",
       " 'actinomorphic',\n",
       " 'actinomycetes',\n",
       " 'actinopterygii',\n",
       " 'actinopterygius',\n",
       " 'actinosporea',\n",
       " 'actinotrocha',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'actionscript',\n",
       " 'actium',\n",
       " 'actius',\n",
       " 'activate',\n",
       " 'activated',\n",
       " 'activates',\n",
       " 'activating',\n",
       " 'activation',\n",
       " 'activator',\n",
       " 'active',\n",
       " 'activebass',\n",
       " 'actively',\n",
       " 'actives',\n",
       " 'activestats',\n",
       " 'activeworlds',\n",
       " 'activex',\n",
       " 'activision',\n",
       " 'activism',\n",
       " 'activist',\n",
       " 'activists',\n",
       " 'activities',\n",
       " 'activitist',\n",
       " 'activitiy',\n",
       " 'activity',\n",
       " 'activitã',\n",
       " 'acton',\n",
       " 'actopan',\n",
       " 'actor',\n",
       " 'actors',\n",
       " 'actress',\n",
       " 'actresses',\n",
       " 'acts',\n",
       " 'actual',\n",
       " 'actuality',\n",
       " 'actualitã',\n",
       " 'actually',\n",
       " 'actuaries',\n",
       " 'actuated',\n",
       " 'actuations',\n",
       " 'actus',\n",
       " 'acuatic',\n",
       " 'acuca',\n",
       " 'acuity',\n",
       " 'aculeata',\n",
       " 'aculeatus',\n",
       " 'acupressure',\n",
       " 'acupuncture',\n",
       " 'acura',\n",
       " 'acute',\n",
       " 'acutely',\n",
       " 'acuteness',\n",
       " 'acutus',\n",
       " 'acyclic',\n",
       " 'acyl',\n",
       " 'adachi',\n",
       " 'adad',\n",
       " 'adage',\n",
       " 'adages',\n",
       " 'adagh',\n",
       " 'adagio',\n",
       " 'adair',\n",
       " 'adairville',\n",
       " 'adak',\n",
       " 'adal',\n",
       " 'adalbert',\n",
       " 'adam',\n",
       " 'adama',\n",
       " 'adamantine',\n",
       " 'adamantium',\n",
       " 'adamey',\n",
       " 'adaminaby',\n",
       " 'adamite',\n",
       " 'adamkus',\n",
       " 'adamlarina',\n",
       " 'adamle',\n",
       " 'adams',\n",
       " 'adamski',\n",
       " 'adamson',\n",
       " 'adamsville',\n",
       " 'adana',\n",
       " 'adapiformes',\n",
       " 'adapt',\n",
       " 'adaptability',\n",
       " 'adaptable',\n",
       " 'adaptation',\n",
       " 'adaptations',\n",
       " 'adapted',\n",
       " 'adapter',\n",
       " 'adapters',\n",
       " 'adapting',\n",
       " 'adaption',\n",
       " 'adaptions',\n",
       " 'adaptive',\n",
       " 'adaptively',\n",
       " 'adaptor',\n",
       " 'adapts',\n",
       " 'adas',\n",
       " 'adasaurus',\n",
       " 'adashim',\n",
       " 'adastra',\n",
       " 'adav',\n",
       " 'adda',\n",
       " 'addai',\n",
       " 'addakhil',\n",
       " 'addams',\n",
       " 'addax',\n",
       " 'added',\n",
       " 'addenbrooke',\n",
       " 'addendum',\n",
       " 'adder',\n",
       " 'adderley',\n",
       " 'adders',\n",
       " 'addicks',\n",
       " 'addict',\n",
       " 'addicted',\n",
       " 'addiction',\n",
       " 'addictions',\n",
       " 'addictive',\n",
       " 'addicts',\n",
       " 'adding',\n",
       " 'addington',\n",
       " 'addis',\n",
       " 'addiscombe',\n",
       " 'addison',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'additionally',\n",
       " 'additionals',\n",
       " 'additions',\n",
       " 'additive',\n",
       " 'additively',\n",
       " 'additives',\n",
       " 'addon',\n",
       " 'address',\n",
       " 'addressability',\n",
       " 'addressable',\n",
       " 'addressed',\n",
       " 'addresses',\n",
       " 'addressing',\n",
       " 'adds',\n",
       " 'adegboyega',\n",
       " 'adegem',\n",
       " 'adel',\n",
       " 'adela',\n",
       " 'adelaide',\n",
       " 'adelante',\n",
       " 'adelbert',\n",
       " 'adelboden',\n",
       " 'adele',\n",
       " 'adelekan',\n",
       " 'adelheid',\n",
       " 'adeline',\n",
       " 'adelir',\n",
       " 'adell',\n",
       " 'adella',\n",
       " 'adelomyrmex',\n",
       " 'adelong',\n",
       " 'adelphi',\n",
       " 'adelphotheos',\n",
       " 'adelsheim',\n",
       " 'ademar',\n",
       " 'ademir',\n",
       " 'ademola',\n",
       " 'aden',\n",
       " 'adenauer',\n",
       " 'adenine',\n",
       " 'adenoidectomy',\n",
       " 'adenoids',\n",
       " 'adenoma',\n",
       " 'adenosine',\n",
       " 'adephaga',\n",
       " 'adept',\n",
       " 'adequality',\n",
       " 'adequate',\n",
       " 'adequately',\n",
       " 'aderbal',\n",
       " 'ades',\n",
       " 'adesa',\n",
       " 'adetokunbo',\n",
       " 'adeus',\n",
       " 'adha',\n",
       " 'adhaerens',\n",
       " 'adhan',\n",
       " 'adhana',\n",
       " 'adhd',\n",
       " 'adhemar',\n",
       " 'adhere',\n",
       " 'adhered',\n",
       " ...]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<333414x120723 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2896838 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_bow = LogisticRegression(random_state=RANDOM_SEED,max_iter=1000).fit(X_train_transform,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6584686997624589"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,lr_bow.predict(X_test_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word = set(word_vectors.index_to_key) #around 6k words in the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4031"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_word.intersection(concreteset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03942373,  1.7966397 , -0.14200447, -0.3486227 ,  0.03800169,\n",
       "       -0.131528  , -0.35020527, -0.41108254,  1.6974189 ,  0.05236067,\n",
       "       -0.00799712,  0.7041458 ,  0.03987895, -0.15628862, -0.01195999,\n",
       "       -0.5276748 , -0.28911754, -0.59118384, -0.29640502, -0.05531655,\n",
       "        0.05208854,  1.2201724 ,  0.05189531, -1.7594199 ,  0.1870284 ,\n",
       "        0.26337144,  0.03394307,  0.18608569, -0.55623055, -0.96030605,\n",
       "       -0.70980805,  0.70162207, -1.1859473 , -0.61812186, -0.6310784 ,\n",
       "       -0.5761929 , -0.23991399,  0.7104913 ,  1.6777844 , -0.02619001,\n",
       "        1.5803056 , -0.43163323, -0.37865758, -0.46269822,  1.1120914 ,\n",
       "       -0.24402891,  0.1181486 , -1.052571  , -0.37906688,  1.0918018 ,\n",
       "        0.18887055,  0.6383479 ,  0.99249554,  1.1353667 ,  0.57656294,\n",
       "        0.21096966,  0.22929311, -0.43283883,  0.50271827,  0.54883325,\n",
       "        0.30768827,  0.03781867, -0.242916  , -1.5159138 , -0.63193476,\n",
       "        0.3270656 ,  1.6821493 , -0.28726235,  0.28426522,  0.38750395,\n",
       "       -0.5363247 , -0.6901583 ,  0.88967055,  0.3410842 ,  0.5208195 ,\n",
       "       -0.68426925, -0.32523647, -0.61977863, -1.2543977 , -0.9816912 ,\n",
       "        0.6012322 , -0.72917604, -0.4487168 ,  0.06676707, -1.5257605 ,\n",
       "        0.24243657, -1.5760837 , -1.0886368 , -0.17732583, -0.38102052,\n",
       "       -0.35411668,  0.7078996 ,  0.9520063 , -0.44247362,  0.4765347 ,\n",
       "       -1.008189  ,  0.3118885 ,  0.71582496,  0.7199179 ,  0.00837577],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors['live']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "word_list = []\n",
    "for word in model_word: \n",
    "    word_list.append((word,lemmatizer.lemmatize(word.lower())))\n",
    "df = pd.DataFrame(word_list,columns=['Original','word'])\n",
    "df = df.merge(AoA,left_on='word',right_on='Word',how='left')\n",
    "df = df[['Original','word','Perc_known','AoA_Kup_lem']]\n",
    "word_not_matched = set(df[df['Perc_known'].isnull()].word.values)\n",
    "\n",
    "for i in range(len(df)):   \n",
    "    if df['word'][i][0] in set(('0','1','2','3','4','5','6','7','8','9')) or len(df['word'][i])==1:\n",
    "        df['AoA_Kup_lem'][i] = 3\n",
    "mean_value = df['AoA_Kup_lem'].mean()\n",
    "df['AoA_Kup_lem'].fillna(value=mean_value,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>word</th>\n",
       "      <th>Perc_known</th>\n",
       "      <th>AoA_Kup_lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3094</th>\n",
       "      <td>weapon</td>\n",
       "      <td>weapon</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Original    word  Perc_known  AoA_Kup_lem\n",
       "3094   weapon  weapon         1.0         6.95"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.loc[df['Original']==['troops','weapons']]\n",
    "df[df['Original'].isin(['troops','weapon'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_perc_known(tokenized_text,df):\n",
    "    avg_perc_know=None\n",
    "    perc_know_list=[]\n",
    "    for _ in tokenized_text: \n",
    "        words =[word for word in _ if word in word_vectors.key_to_index]\n",
    "        \n",
    "        if len(words) >0:\n",
    "            avg_perc_know = np.mean(df[df['Original'].isin(words)]['AoA_Kup_lem'])\n",
    "            perc_know_list.append(avg_perc_know)\n",
    "        else: \n",
    "            \n",
    "            perc_know_list.append(0)\n",
    "            \n",
    "    return perc_know_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(X_train_wv)\n",
    "df_train['year'] = generate_perc_known(tokenized_text_train,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(X_test_wv)\n",
    "df_test['year'] = generate_perc_known(tokenized_text_test,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.163066</td>\n",
       "      <td>0.066503</td>\n",
       "      <td>0.007967</td>\n",
       "      <td>-0.368197</td>\n",
       "      <td>-0.475971</td>\n",
       "      <td>0.174799</td>\n",
       "      <td>-0.226500</td>\n",
       "      <td>0.288741</td>\n",
       "      <td>-0.101118</td>\n",
       "      <td>0.251682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.343301</td>\n",
       "      <td>0.449110</td>\n",
       "      <td>-0.301583</td>\n",
       "      <td>-0.318929</td>\n",
       "      <td>-0.027628</td>\n",
       "      <td>-0.003120</td>\n",
       "      <td>0.640888</td>\n",
       "      <td>0.395134</td>\n",
       "      <td>-0.211103</td>\n",
       "      <td>7.319698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.098105</td>\n",
       "      <td>-0.697004</td>\n",
       "      <td>-0.067849</td>\n",
       "      <td>0.073167</td>\n",
       "      <td>0.001977</td>\n",
       "      <td>-0.519177</td>\n",
       "      <td>-0.064798</td>\n",
       "      <td>-0.384014</td>\n",
       "      <td>0.359658</td>\n",
       "      <td>-0.080730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100243</td>\n",
       "      <td>-0.152842</td>\n",
       "      <td>0.018108</td>\n",
       "      <td>-0.616458</td>\n",
       "      <td>0.208961</td>\n",
       "      <td>0.239500</td>\n",
       "      <td>-0.078117</td>\n",
       "      <td>0.907243</td>\n",
       "      <td>0.644744</td>\n",
       "      <td>8.900953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.608009</td>\n",
       "      <td>-0.270855</td>\n",
       "      <td>-0.351858</td>\n",
       "      <td>-1.324698</td>\n",
       "      <td>0.509448</td>\n",
       "      <td>0.466696</td>\n",
       "      <td>-0.869674</td>\n",
       "      <td>0.316894</td>\n",
       "      <td>-0.832663</td>\n",
       "      <td>0.482958</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.804097</td>\n",
       "      <td>-1.260673</td>\n",
       "      <td>-0.484280</td>\n",
       "      <td>-1.026836</td>\n",
       "      <td>-0.381989</td>\n",
       "      <td>0.006748</td>\n",
       "      <td>0.651532</td>\n",
       "      <td>0.502151</td>\n",
       "      <td>-1.543706</td>\n",
       "      <td>7.385000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.231419</td>\n",
       "      <td>-0.460309</td>\n",
       "      <td>-0.321846</td>\n",
       "      <td>-0.401228</td>\n",
       "      <td>-1.299778</td>\n",
       "      <td>-0.461486</td>\n",
       "      <td>0.002258</td>\n",
       "      <td>-0.175611</td>\n",
       "      <td>0.296010</td>\n",
       "      <td>0.373852</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068769</td>\n",
       "      <td>0.134842</td>\n",
       "      <td>0.026607</td>\n",
       "      <td>0.200088</td>\n",
       "      <td>0.376173</td>\n",
       "      <td>0.175164</td>\n",
       "      <td>-0.239718</td>\n",
       "      <td>0.463941</td>\n",
       "      <td>-0.541556</td>\n",
       "      <td>8.971588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.155188</td>\n",
       "      <td>0.110082</td>\n",
       "      <td>0.749716</td>\n",
       "      <td>-0.211680</td>\n",
       "      <td>-0.294006</td>\n",
       "      <td>-0.928232</td>\n",
       "      <td>0.095029</td>\n",
       "      <td>0.326077</td>\n",
       "      <td>0.020296</td>\n",
       "      <td>0.458989</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.496064</td>\n",
       "      <td>0.562254</td>\n",
       "      <td>-0.161042</td>\n",
       "      <td>-0.556670</td>\n",
       "      <td>-0.152797</td>\n",
       "      <td>0.216482</td>\n",
       "      <td>-0.109737</td>\n",
       "      <td>1.134926</td>\n",
       "      <td>-0.073294</td>\n",
       "      <td>7.939948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83349</th>\n",
       "      <td>-0.212178</td>\n",
       "      <td>-0.577913</td>\n",
       "      <td>0.233901</td>\n",
       "      <td>-0.283749</td>\n",
       "      <td>-0.250686</td>\n",
       "      <td>-0.740940</td>\n",
       "      <td>-0.073741</td>\n",
       "      <td>0.163121</td>\n",
       "      <td>-0.268991</td>\n",
       "      <td>0.569778</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062439</td>\n",
       "      <td>0.181861</td>\n",
       "      <td>-0.294118</td>\n",
       "      <td>0.674152</td>\n",
       "      <td>-0.312687</td>\n",
       "      <td>-0.416863</td>\n",
       "      <td>0.006465</td>\n",
       "      <td>0.296494</td>\n",
       "      <td>0.144787</td>\n",
       "      <td>7.846061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83350</th>\n",
       "      <td>0.083994</td>\n",
       "      <td>-0.119798</td>\n",
       "      <td>0.014636</td>\n",
       "      <td>-0.046240</td>\n",
       "      <td>-0.176528</td>\n",
       "      <td>-0.371178</td>\n",
       "      <td>-0.049741</td>\n",
       "      <td>-0.063575</td>\n",
       "      <td>0.069346</td>\n",
       "      <td>0.097987</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.106163</td>\n",
       "      <td>0.598239</td>\n",
       "      <td>-0.423099</td>\n",
       "      <td>-0.277646</td>\n",
       "      <td>0.249423</td>\n",
       "      <td>0.238795</td>\n",
       "      <td>-0.084238</td>\n",
       "      <td>0.325800</td>\n",
       "      <td>-0.371009</td>\n",
       "      <td>7.653076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83351</th>\n",
       "      <td>-0.027579</td>\n",
       "      <td>-0.583053</td>\n",
       "      <td>-0.212853</td>\n",
       "      <td>0.064448</td>\n",
       "      <td>-0.001676</td>\n",
       "      <td>-0.386104</td>\n",
       "      <td>-0.194504</td>\n",
       "      <td>0.125628</td>\n",
       "      <td>0.087920</td>\n",
       "      <td>0.013556</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087076</td>\n",
       "      <td>0.200042</td>\n",
       "      <td>0.022237</td>\n",
       "      <td>0.865286</td>\n",
       "      <td>0.345294</td>\n",
       "      <td>0.206362</td>\n",
       "      <td>-0.050420</td>\n",
       "      <td>0.287032</td>\n",
       "      <td>-0.024188</td>\n",
       "      <td>6.618984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83352</th>\n",
       "      <td>0.150752</td>\n",
       "      <td>-0.344787</td>\n",
       "      <td>0.016055</td>\n",
       "      <td>-0.438976</td>\n",
       "      <td>0.105028</td>\n",
       "      <td>-0.072180</td>\n",
       "      <td>-0.229091</td>\n",
       "      <td>0.010541</td>\n",
       "      <td>-0.065317</td>\n",
       "      <td>0.162644</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090400</td>\n",
       "      <td>-0.352687</td>\n",
       "      <td>-0.262663</td>\n",
       "      <td>-0.028436</td>\n",
       "      <td>0.180446</td>\n",
       "      <td>-0.053098</td>\n",
       "      <td>0.068634</td>\n",
       "      <td>-0.034959</td>\n",
       "      <td>0.074879</td>\n",
       "      <td>7.009195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83353</th>\n",
       "      <td>0.798341</td>\n",
       "      <td>-0.436330</td>\n",
       "      <td>-0.095135</td>\n",
       "      <td>-0.875586</td>\n",
       "      <td>-0.725843</td>\n",
       "      <td>-0.983312</td>\n",
       "      <td>-0.478156</td>\n",
       "      <td>0.076523</td>\n",
       "      <td>-0.534273</td>\n",
       "      <td>0.518388</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020779</td>\n",
       "      <td>0.168365</td>\n",
       "      <td>-0.001870</td>\n",
       "      <td>-0.459175</td>\n",
       "      <td>-1.014081</td>\n",
       "      <td>-0.339258</td>\n",
       "      <td>0.264108</td>\n",
       "      <td>0.849447</td>\n",
       "      <td>-0.063408</td>\n",
       "      <td>8.322109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83354 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0      0.163066  0.066503  0.007967 -0.368197 -0.475971  0.174799 -0.226500   \n",
       "1      0.098105 -0.697004 -0.067849  0.073167  0.001977 -0.519177 -0.064798   \n",
       "2      0.608009 -0.270855 -0.351858 -1.324698  0.509448  0.466696 -0.869674   \n",
       "3     -0.231419 -0.460309 -0.321846 -0.401228 -1.299778 -0.461486  0.002258   \n",
       "4     -0.155188  0.110082  0.749716 -0.211680 -0.294006 -0.928232  0.095029   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "83349 -0.212178 -0.577913  0.233901 -0.283749 -0.250686 -0.740940 -0.073741   \n",
       "83350  0.083994 -0.119798  0.014636 -0.046240 -0.176528 -0.371178 -0.049741   \n",
       "83351 -0.027579 -0.583053 -0.212853  0.064448 -0.001676 -0.386104 -0.194504   \n",
       "83352  0.150752 -0.344787  0.016055 -0.438976  0.105028 -0.072180 -0.229091   \n",
       "83353  0.798341 -0.436330 -0.095135 -0.875586 -0.725843 -0.983312 -0.478156   \n",
       "\n",
       "              7         8         9  ...        91        92        93  \\\n",
       "0      0.288741 -0.101118  0.251682  ...  0.343301  0.449110 -0.301583   \n",
       "1     -0.384014  0.359658 -0.080730  ...  0.100243 -0.152842  0.018108   \n",
       "2      0.316894 -0.832663  0.482958  ... -0.804097 -1.260673 -0.484280   \n",
       "3     -0.175611  0.296010  0.373852  ... -0.068769  0.134842  0.026607   \n",
       "4      0.326077  0.020296  0.458989  ... -0.496064  0.562254 -0.161042   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "83349  0.163121 -0.268991  0.569778  ... -0.062439  0.181861 -0.294118   \n",
       "83350 -0.063575  0.069346  0.097987  ... -0.106163  0.598239 -0.423099   \n",
       "83351  0.125628  0.087920  0.013556  ... -0.087076  0.200042  0.022237   \n",
       "83352  0.010541 -0.065317  0.162644  ... -0.090400 -0.352687 -0.262663   \n",
       "83353  0.076523 -0.534273  0.518388  ... -0.020779  0.168365 -0.001870   \n",
       "\n",
       "             94        95        96        97        98        99      year  \n",
       "0     -0.318929 -0.027628 -0.003120  0.640888  0.395134 -0.211103  7.319698  \n",
       "1     -0.616458  0.208961  0.239500 -0.078117  0.907243  0.644744  8.900953  \n",
       "2     -1.026836 -0.381989  0.006748  0.651532  0.502151 -1.543706  7.385000  \n",
       "3      0.200088  0.376173  0.175164 -0.239718  0.463941 -0.541556  8.971588  \n",
       "4     -0.556670 -0.152797  0.216482 -0.109737  1.134926 -0.073294  7.939948  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "83349  0.674152 -0.312687 -0.416863  0.006465  0.296494  0.144787  7.846061  \n",
       "83350 -0.277646  0.249423  0.238795 -0.084238  0.325800 -0.371009  7.653076  \n",
       "83351  0.865286  0.345294  0.206362 -0.050420  0.287032 -0.024188  6.618984  \n",
       "83352 -0.028436  0.180446 -0.053098  0.068634 -0.034959  0.074879  7.009195  \n",
       "83353 -0.459175 -1.014081 -0.339258  0.264108  0.849447 -0.063408  8.322109  \n",
       "\n",
       "[83354 rows x 101 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=RANDOM_SEED,max_iter=1000).fit(df_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58372723564556"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,lr.predict(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_bow = DummyClassifier(strategy='uniform',random_state=RANDOM_SEED).fit(X_train_transform,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5011277203253593"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, dummy_bow.predict(X_test_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_wv = DummyClassifier(strategy='uniform',random_state=RANDOM_SEED).fit(X_train_wv,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5011277203253593"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,dummy_wv.predict(X_test_wv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_bow = LogisticRegression(random_state=RANDOM_SEED,max_iter=1000).fit(X_train_transform,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6465916452719725"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,lr_bow.predict(X_test_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_wv = LogisticRegression(random_state=RANDOM_SEED,max_iter=1000).fit(X_train_wv,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5640041269765098"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,lr_wv.predict(X_test_wv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_bow = RandomForestClassifier(n_estimators=500,max_depth=5,random_state=RANDOM_SEED).fit(X_train_transform,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6416968591789236"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,rf_bow.predict(X_test_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_wv = RandomForestClassifier(n_estimators=100,max_depth=5,random_state=RANDOM_SEED).fit(X_train_wv,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,rf_wv.predict(X_test_wv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2,random_state=RANDOM_SEED).fit(X_train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = pd.DataFrame({'cluster':kmeans.labels_,'y_label':y_train,'text':X_train})\n",
    "cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Topic Modeling - Consider NMF to create a document-topic matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.stem.porter import *\n",
    "def lemmatize_stemming(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    #Un-hash next line to use stemming\n",
    "    #return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "    #Un-hash next line to NOT use stemming\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            #Un-hash next line to use stemming\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            #Un-hash next line to NOT use stemming\n",
    "            #result.append(token)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There is manuscript evidence that Austen continued to work on these pieces as late as the period 1809 Ã¢ '' 11 , and that her niece and nephew , Anna and James Edward Austen , made further additions as late as 1814 .\""
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['original_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['manuscript',\n",
       " 'evidence',\n",
       " 'austen',\n",
       " 'continue',\n",
       " 'work',\n",
       " 'piece',\n",
       " 'late',\n",
       " 'period',\n",
       " 'niece',\n",
       " 'nephew',\n",
       " 'anna',\n",
       " 'jam',\n",
       " 'edward',\n",
       " 'austen',\n",
       " 'additions',\n",
       " 'late']"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(df['original_text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will run about 2 minutes\n",
    "processed_docs = [preprocess(text) for text in df['original_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x1567b848520>"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "#bow_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "416768"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will run 10 minutes\n",
    "#lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "#                                   num_topics = 8, \n",
    "#                                   id2word = dictionary,                                    \n",
    "#                                   passes = 10,\n",
    "#                                   workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for idx, topic in lda_model.print_topics(-1):\n",
    "#    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "#    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
