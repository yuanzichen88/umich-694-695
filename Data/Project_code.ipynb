{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import altair as alt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/zhouwei/opt/anaconda3/lib/python3.8/site-packages (4.1.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/zhouwei/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/zhouwei/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.19.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/zhouwei/opt/anaconda3/lib/python3.8/site-packages (from gensim) (6.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zhouwei/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED=694"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('WikiLarge_Train.csv')\n",
    "len(df[df['label']==1])/len(df) # the dataset label is well balanced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['original_text']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "dale_chall = pd.read_csv('dale_chall.txt',delimiter='\\t',header=None,names=['word'])\n",
    "dale = set(dale_chall['word'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhouwei/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['absent', 'accept', 'accident', 'account', 'ache', 'aching', 'acorn', 'acre', 'across', 'act', 'acts', 'add', 'address', 'admire', 'adventure', 'afar', 'afternoon', 'afterward', 'afterwards', 'age', 'aged', 'ago', 'agree', 'ah', 'ahead', 'aid', 'aim', 'air', 'airfield', 'airplane', 'airport', 'airship', 'airy', 'alarm', 'alike', 'alive', 'alley', 'alligator', 'allow', 'almost', 'alone', 'along', 'america', 'american', 'april', 'august', 'christmas', 'december', 'ding', 'dong', 'english', 'eyed', 'fashioned', 'february', 'french', 'friday', 'indian', 'january', 'july', 'june', 'looking', 'monday', 'mr', 'mrs', 'negro', 'november', 'october', 'saturday', 'september', 'states', 'sunday', 'thanksgiving', 'thursday', 'tuesday', 'united', 'wednesday', 'wow'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "stop_words = list(set(stopwords.words('english'))|dale)\n",
    "vectorizer = TfidfVectorizer(min_df=5,stop_words=stop_words,ngram_range=(1,2))\n",
    "X_train_transform = vectorizer.fit_transform(X_train)\n",
    "X_test_transform  = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '00 00',\n",
       " '00 04',\n",
       " '00 10',\n",
       " '00 30',\n",
       " '00 cdt',\n",
       " '00 edt',\n",
       " '00 households',\n",
       " '00 lrb',\n",
       " '00 on16',\n",
       " '00 pm',\n",
       " '00 rrb',\n",
       " '00 utc',\n",
       " '000',\n",
       " '000 00',\n",
       " '000 000',\n",
       " '000 001',\n",
       " '000 10',\n",
       " '000 100',\n",
       " '000 11',\n",
       " '000 12',\n",
       " '000 13',\n",
       " '000 150',\n",
       " '000 1989',\n",
       " '000 20',\n",
       " '000 200',\n",
       " '000 2001',\n",
       " '000 2005',\n",
       " '000 2006',\n",
       " '000 2008',\n",
       " '000 22',\n",
       " '000 24',\n",
       " '000 25',\n",
       " '000 30',\n",
       " '000 35',\n",
       " '000 40',\n",
       " '000 400',\n",
       " '000 500',\n",
       " '000 600',\n",
       " '000 70',\n",
       " '000 80',\n",
       " '000 according',\n",
       " '000 acre',\n",
       " '000 acres',\n",
       " '000 applicants',\n",
       " '000 armenian',\n",
       " '000 articles',\n",
       " '000 bangladesh',\n",
       " '000 bc',\n",
       " '000 bce',\n",
       " '000 births',\n",
       " '000 bp',\n",
       " '000 buildings',\n",
       " '000 burma',\n",
       " '000 casualties',\n",
       " '000 chinese',\n",
       " '000 chitral',\n",
       " '000 copies',\n",
       " '000 deaths',\n",
       " '000 described',\n",
       " '000 employees',\n",
       " '000 ethnic',\n",
       " '000 euros',\n",
       " '000 event',\n",
       " '000 fans',\n",
       " '000 fatalities',\n",
       " '000 ft',\n",
       " '000 haiti',\n",
       " '000 hectares',\n",
       " '000 highest',\n",
       " '000 homeless',\n",
       " '000 homes',\n",
       " '000 horses',\n",
       " '000 hours',\n",
       " '000 houses',\n",
       " '000 inhabitants',\n",
       " '000 injured',\n",
       " '000 jews',\n",
       " '000 july',\n",
       " '000 kilometers',\n",
       " '000 kilometres',\n",
       " '000 km',\n",
       " '000 km2',\n",
       " '000 kmâ',\n",
       " '000 largest',\n",
       " '000 letters',\n",
       " '000 liters',\n",
       " '000 lived',\n",
       " '000 lrb',\n",
       " '000 m2',\n",
       " '000 main',\n",
       " '000 members',\n",
       " '000 meters',\n",
       " '000 metropolitan',\n",
       " '000 miles',\n",
       " '000 minimize',\n",
       " '000 missing',\n",
       " '000 mâ',\n",
       " '000 pages',\n",
       " '000 passengers',\n",
       " '000 per',\n",
       " '000 philadelphia',\n",
       " '000 population',\n",
       " '000 pounds',\n",
       " '000 registered',\n",
       " '000 reported',\n",
       " '000 residents',\n",
       " '000 roma',\n",
       " '000 rrb',\n",
       " '000 soldiers',\n",
       " '000 speakers',\n",
       " '000 species',\n",
       " '000 specimens',\n",
       " '000 spectators',\n",
       " '000 sq',\n",
       " '000 students',\n",
       " '000 th',\n",
       " '000 times',\n",
       " '000 tonnes',\n",
       " '000 tons',\n",
       " '000 total',\n",
       " '000 troops',\n",
       " '000 united',\n",
       " '000 units',\n",
       " '000 usd',\n",
       " '000 vehicles',\n",
       " '000 victims',\n",
       " '000 visitors',\n",
       " '000 votes',\n",
       " '000 within',\n",
       " '000 workers',\n",
       " '000 worshippers',\n",
       " '000 years',\n",
       " '0000',\n",
       " '0000 utc',\n",
       " '001',\n",
       " '002',\n",
       " '003',\n",
       " '004',\n",
       " '005',\n",
       " '006',\n",
       " '0061',\n",
       " '0061 lrb',\n",
       " '007',\n",
       " '008',\n",
       " '009',\n",
       " '01',\n",
       " '01 02',\n",
       " '01 03',\n",
       " '01 13',\n",
       " '01 28',\n",
       " '01 pablo',\n",
       " '010',\n",
       " '012',\n",
       " '014',\n",
       " '016',\n",
       " '018',\n",
       " '0198605757',\n",
       " '0198605757 religious',\n",
       " '02',\n",
       " '02 28',\n",
       " '02 rrb',\n",
       " '021',\n",
       " '024',\n",
       " '025',\n",
       " '026',\n",
       " '027',\n",
       " '028',\n",
       " '03',\n",
       " '03 00',\n",
       " '03 lrb',\n",
       " '03 rrb',\n",
       " '030',\n",
       " '034',\n",
       " '038',\n",
       " '039',\n",
       " '04',\n",
       " '04 00',\n",
       " '04 09',\n",
       " '04 10',\n",
       " '04 commonly',\n",
       " '04 miles',\n",
       " '04 rrb',\n",
       " '044',\n",
       " '046',\n",
       " '047',\n",
       " '049',\n",
       " '05',\n",
       " '05 lockout',\n",
       " '05 rrb',\n",
       " '050',\n",
       " '052',\n",
       " '054',\n",
       " '056',\n",
       " '057',\n",
       " '06',\n",
       " '06 km',\n",
       " '06 rrb',\n",
       " '0600',\n",
       " '0600 utc',\n",
       " '061',\n",
       " '062',\n",
       " '063',\n",
       " '064',\n",
       " '068',\n",
       " '07',\n",
       " '07 01',\n",
       " '07 30',\n",
       " '07 km',\n",
       " '070',\n",
       " '071',\n",
       " '072',\n",
       " '073',\n",
       " '073 km',\n",
       " '076',\n",
       " '078',\n",
       " '08',\n",
       " '08 01',\n",
       " '08 almost',\n",
       " '08 contained',\n",
       " '08 gretna',\n",
       " '08 kit',\n",
       " '08 nhl',\n",
       " '08 rrb',\n",
       " '08 virtually',\n",
       " '080',\n",
       " '081',\n",
       " '082',\n",
       " '084',\n",
       " '086',\n",
       " '086 lrb',\n",
       " '0868050121',\n",
       " '088',\n",
       " '09',\n",
       " '09 09',\n",
       " '09 nhl',\n",
       " '09 retrieved',\n",
       " '09 rrb',\n",
       " '090',\n",
       " '092',\n",
       " '092629',\n",
       " '092629 81',\n",
       " '0932',\n",
       " '099',\n",
       " '0e',\n",
       " '0e altitude',\n",
       " '0n',\n",
       " '0n 68',\n",
       " '0n 72',\n",
       " '0n 73',\n",
       " '10',\n",
       " '10 00',\n",
       " '10 000',\n",
       " '10 0e',\n",
       " '10 10',\n",
       " '10 100',\n",
       " '10 11',\n",
       " '10 12',\n",
       " '10 14',\n",
       " '10 15',\n",
       " '10 16',\n",
       " '10 1612',\n",
       " '10 1791',\n",
       " '10 18',\n",
       " '10 1836',\n",
       " '10 1838',\n",
       " '10 1842',\n",
       " '10 1855',\n",
       " '10 1890',\n",
       " '10 1895',\n",
       " '10 1897',\n",
       " '10 1908',\n",
       " '10 1913',\n",
       " '10 1919',\n",
       " '10 1927',\n",
       " '10 1930',\n",
       " '10 1932',\n",
       " '10 1933',\n",
       " '10 1940',\n",
       " '10 1942',\n",
       " '10 1952',\n",
       " '10 1956',\n",
       " '10 1957',\n",
       " '10 1962',\n",
       " '10 1963',\n",
       " '10 1965',\n",
       " '10 1967',\n",
       " '10 1968',\n",
       " '10 1969',\n",
       " '10 1970',\n",
       " '10 1971',\n",
       " '10 1972',\n",
       " '10 1973',\n",
       " '10 1974',\n",
       " '10 1977',\n",
       " '10 1980',\n",
       " '10 1981',\n",
       " '10 1983',\n",
       " '10 1984',\n",
       " '10 1986',\n",
       " '10 1988',\n",
       " '10 1989',\n",
       " '10 1990',\n",
       " '10 1991',\n",
       " '10 1992',\n",
       " '10 1995',\n",
       " '10 1997',\n",
       " '10 20',\n",
       " '10 200',\n",
       " '10 2000',\n",
       " '10 2001',\n",
       " '10 2002',\n",
       " '10 2003',\n",
       " '10 2004',\n",
       " '10 2005',\n",
       " '10 2006',\n",
       " '10 2007',\n",
       " '10 2008',\n",
       " '10 2009',\n",
       " '10 2010',\n",
       " '10 25',\n",
       " '10 30',\n",
       " '10 300',\n",
       " '10 40',\n",
       " '10 50',\n",
       " '10 500',\n",
       " '10 80',\n",
       " '10 actually',\n",
       " '10 alone',\n",
       " '10 april',\n",
       " '10 august',\n",
       " '10 bc',\n",
       " '10 bermuda',\n",
       " '10 billion',\n",
       " '10 british',\n",
       " '10 candidate',\n",
       " '10 cantons',\n",
       " '10 cm',\n",
       " '10 command',\n",
       " '10 conference',\n",
       " '10 copies',\n",
       " '10 cylinders',\n",
       " '10 days',\n",
       " '10 december',\n",
       " '10 districts',\n",
       " '10 english',\n",
       " '10 european',\n",
       " '10 february',\n",
       " '10 finishes',\n",
       " '10 ft',\n",
       " '10 games',\n",
       " '10 hits',\n",
       " '10 hours',\n",
       " '10 hygiea',\n",
       " '10 january',\n",
       " '10 july',\n",
       " '10 june',\n",
       " '10 kilometers',\n",
       " '10 kilometres',\n",
       " '10 km',\n",
       " '10 km2',\n",
       " '10 lrb',\n",
       " '10 meters',\n",
       " '10 metres',\n",
       " '10 miles',\n",
       " '10 minus',\n",
       " '10 minutes',\n",
       " '10 months',\n",
       " '10 named',\n",
       " '10 ndash',\n",
       " '10 nhl',\n",
       " '10 november',\n",
       " '10 october',\n",
       " '10 per',\n",
       " '10 pm',\n",
       " '10 points',\n",
       " '10 quarters',\n",
       " '10 rounds',\n",
       " '10 rrb',\n",
       " '10 seconds',\n",
       " '10 september',\n",
       " '10 singles',\n",
       " '10 slower',\n",
       " '10 success',\n",
       " '10 successful',\n",
       " '10 teams',\n",
       " '10 things',\n",
       " '10 times',\n",
       " '10 uncertainty',\n",
       " '10 units',\n",
       " '10 unofficial',\n",
       " '10 videos',\n",
       " '10 volume',\n",
       " '10 weekend',\n",
       " '10 years',\n",
       " '100',\n",
       " '100 000',\n",
       " '100 1000',\n",
       " '100 200',\n",
       " '100 500',\n",
       " '100 ad',\n",
       " '100 airplay',\n",
       " '100 asians',\n",
       " '100 bc',\n",
       " '100 billion',\n",
       " '100 books',\n",
       " '100 career',\n",
       " '100 centavos',\n",
       " '100 characters',\n",
       " '100 cm',\n",
       " '100 copies',\n",
       " '100 counties',\n",
       " '100 countries',\n",
       " '100 days',\n",
       " '100 dollars',\n",
       " '100 females',\n",
       " '100 greatest',\n",
       " '100 hp',\n",
       " '100 influential',\n",
       " '100 kilometers',\n",
       " '100 km',\n",
       " '100 largest',\n",
       " '100 lrb',\n",
       " '100 mb',\n",
       " '100 meters',\n",
       " '100 metres',\n",
       " '100 mi',\n",
       " '100 miles',\n",
       " '100 mph',\n",
       " '100 pregnancies',\n",
       " '100 records',\n",
       " '100 rrb',\n",
       " '100 seconds',\n",
       " '100 selling',\n",
       " '100 sexiest',\n",
       " '100 singles',\n",
       " '100 sites',\n",
       " '100 songs',\n",
       " '100 species',\n",
       " '100 years',\n",
       " '1000',\n",
       " '1000 2000',\n",
       " '1000 ad',\n",
       " '1000 bc',\n",
       " '1000 km',\n",
       " '1000 lrb',\n",
       " '1000 rrb',\n",
       " '1000 species',\n",
       " '1000 times',\n",
       " '1000 years',\n",
       " '10000',\n",
       " '1000th',\n",
       " '1001',\n",
       " '1004',\n",
       " '1004 rrb',\n",
       " '1009',\n",
       " '100th',\n",
       " '100th celebration',\n",
       " '101',\n",
       " '101 dalmatians',\n",
       " '101 lrb',\n",
       " '1010',\n",
       " '1011',\n",
       " '1011 971',\n",
       " '1012',\n",
       " '1013',\n",
       " '1014',\n",
       " '1015',\n",
       " '1016',\n",
       " '102',\n",
       " '102 026',\n",
       " '102 dalmatians',\n",
       " '102 males',\n",
       " '102 rrb',\n",
       " '1020',\n",
       " '1022',\n",
       " '1024',\n",
       " '1025',\n",
       " '1027',\n",
       " '1029',\n",
       " '1029 703',\n",
       " '103',\n",
       " '103 communes',\n",
       " '1030',\n",
       " '1031',\n",
       " '1031 rrb',\n",
       " '1032',\n",
       " '1034',\n",
       " '1035',\n",
       " '1036',\n",
       " '1036 ganymed',\n",
       " '1037',\n",
       " '1039',\n",
       " '104',\n",
       " '104 rrb',\n",
       " '1040',\n",
       " '1040 rrb',\n",
       " '1040s',\n",
       " '1041',\n",
       " '1042',\n",
       " '1045',\n",
       " '1045 rrb',\n",
       " '1046',\n",
       " '1046 bc',\n",
       " '1048',\n",
       " '105',\n",
       " '105 hp',\n",
       " '105 miles',\n",
       " '105 mph',\n",
       " '105 rrb',\n",
       " '1050',\n",
       " '1051',\n",
       " '1053',\n",
       " '1053 rrb',\n",
       " '1056',\n",
       " '1057',\n",
       " '106',\n",
       " '1060',\n",
       " '1063',\n",
       " '1066',\n",
       " '1066 ndash',\n",
       " '1066 rrb',\n",
       " '1067',\n",
       " '1068',\n",
       " '1069',\n",
       " '106th',\n",
       " '107',\n",
       " '1070',\n",
       " '1072',\n",
       " '1073',\n",
       " '1077',\n",
       " '108',\n",
       " '1081',\n",
       " '1086',\n",
       " '1088',\n",
       " '1089',\n",
       " '109',\n",
       " '109 140',\n",
       " '1090',\n",
       " '1092',\n",
       " '1093',\n",
       " '1094',\n",
       " '1094 rrb',\n",
       " '1096',\n",
       " '1099',\n",
       " '1099 rrb',\n",
       " '10th',\n",
       " '10th anniversary',\n",
       " '10th annual',\n",
       " '10th century',\n",
       " '10th edition',\n",
       " '10th prime',\n",
       " '11',\n",
       " '11 00',\n",
       " '11 000',\n",
       " '11 10',\n",
       " '11 11',\n",
       " '11 12',\n",
       " '11 13',\n",
       " '11 15',\n",
       " '11 16',\n",
       " '11 1772',\n",
       " '11 1812',\n",
       " '11 1854',\n",
       " '11 1857',\n",
       " '11 1858',\n",
       " '11 1899',\n",
       " '11 1904',\n",
       " '11 1917',\n",
       " '11 1918',\n",
       " '11 1920',\n",
       " '11 1924',\n",
       " '11 1926',\n",
       " '11 1931',\n",
       " '11 1933',\n",
       " '11 1936',\n",
       " '11 1946',\n",
       " '11 1948',\n",
       " '11 1949',\n",
       " '11 1953',\n",
       " '11 1961',\n",
       " '11 1962',\n",
       " '11 1963',\n",
       " '11 1965',\n",
       " '11 1967',\n",
       " '11 1971',\n",
       " '11 1972',\n",
       " '11 1973',\n",
       " '11 1974',\n",
       " '11 1976',\n",
       " '11 1978',\n",
       " '11 1979',\n",
       " '11 1980',\n",
       " '11 1981',\n",
       " '11 1983',\n",
       " '11 1984',\n",
       " '11 1987',\n",
       " '11 1989',\n",
       " '11 1990',\n",
       " '11 1991',\n",
       " '11 1993',\n",
       " '11 1994',\n",
       " '11 1995',\n",
       " '11 1997',\n",
       " '11 1999',\n",
       " '11 20',\n",
       " '11 2001',\n",
       " '11 2002',\n",
       " '11 2003',\n",
       " '11 2004',\n",
       " '11 2005',\n",
       " '11 2006',\n",
       " '11 2007',\n",
       " '11 2008',\n",
       " '11 2009',\n",
       " '11 2010',\n",
       " '11 2011',\n",
       " '11 25',\n",
       " '11 29',\n",
       " '11 40',\n",
       " '11 59',\n",
       " '11 april',\n",
       " '11 attacks',\n",
       " '11 august',\n",
       " '11 british',\n",
       " '11 david',\n",
       " '11 days',\n",
       " '11 december',\n",
       " '11 february',\n",
       " '11 games',\n",
       " '11 goals',\n",
       " '11 hours',\n",
       " '11 january',\n",
       " '11 july',\n",
       " '11 june',\n",
       " '11 km',\n",
       " '11 lrb',\n",
       " '11 miles',\n",
       " '11 months',\n",
       " '11 ndash',\n",
       " '11 november',\n",
       " '11 october',\n",
       " '11 penumbral',\n",
       " '11 portable',\n",
       " '11 provinces',\n",
       " '11 rrb',\n",
       " '11 seasons',\n",
       " '11 september',\n",
       " '11 slower',\n",
       " '11 standard',\n",
       " '11 years',\n",
       " '110',\n",
       " '110 000',\n",
       " '110 hp',\n",
       " '110 km',\n",
       " '110 kw',\n",
       " '110 rrb',\n",
       " '110 years',\n",
       " '1100',\n",
       " '1100 days',\n",
       " '1100 rrb',\n",
       " '1105',\n",
       " '1107',\n",
       " '111',\n",
       " '1115',\n",
       " '1118',\n",
       " '111th',\n",
       " '112',\n",
       " '112 titsingh',\n",
       " '1120',\n",
       " '1120 809',\n",
       " '1122',\n",
       " '1123',\n",
       " '1125',\n",
       " '1125 rrb',\n",
       " '113',\n",
       " '113 years',\n",
       " '1130',\n",
       " '1130 rrb',\n",
       " '1131',\n",
       " '1133',\n",
       " '1137',\n",
       " '1138',\n",
       " '1139',\n",
       " '114',\n",
       " '114 ura',\n",
       " '114 years',\n",
       " '1142',\n",
       " '1143',\n",
       " '115',\n",
       " '115 billion',\n",
       " '115 mph',\n",
       " '115 years',\n",
       " '1150',\n",
       " '1151',\n",
       " '1151 rrb',\n",
       " '1152',\n",
       " '1153',\n",
       " '1154',\n",
       " '1158',\n",
       " '1159',\n",
       " '116',\n",
       " '1160',\n",
       " '1165',\n",
       " '1166',\n",
       " '1167',\n",
       " '117',\n",
       " '117 diesel',\n",
       " '117 rrb',\n",
       " '1170',\n",
       " '1173',\n",
       " '1174',\n",
       " '1177',\n",
       " '1178',\n",
       " '1179',\n",
       " '118',\n",
       " '118 69',\n",
       " '118 rrb',\n",
       " '1180',\n",
       " '1180 rrb',\n",
       " '1183',\n",
       " '1185',\n",
       " '1185 rrb',\n",
       " '1186',\n",
       " '1187',\n",
       " '1189',\n",
       " '119',\n",
       " '1192',\n",
       " '1192 1333',\n",
       " '1192 kamakura',\n",
       " '1195',\n",
       " '1196',\n",
       " '1198',\n",
       " '1199',\n",
       " '119th',\n",
       " '11th',\n",
       " '11th 12th',\n",
       " '11th centuries',\n",
       " '11th century',\n",
       " '11th edition',\n",
       " '11th largest',\n",
       " '12',\n",
       " '12 00',\n",
       " '12 000',\n",
       " '12 0e',\n",
       " '12 13',\n",
       " '12 14',\n",
       " '12 16',\n",
       " '12 17',\n",
       " '12 18',\n",
       " '12 1804',\n",
       " '12 1806',\n",
       " '12 1836',\n",
       " '12 1837',\n",
       " '12 1849',\n",
       " '12 1872',\n",
       " '12 1909',\n",
       " '12 1915',\n",
       " '12 1924',\n",
       " '12 1925',\n",
       " '12 1929',\n",
       " '12 1932',\n",
       " '12 1934',\n",
       " '12 1938',\n",
       " '12 1939',\n",
       " '12 1940',\n",
       " '12 1945',\n",
       " '12 1946',\n",
       " '12 1947',\n",
       " '12 1948',\n",
       " '12 1950',\n",
       " '12 1961',\n",
       " '12 1963',\n",
       " '12 1966',\n",
       " '12 1967',\n",
       " '12 1968',\n",
       " '12 1970',\n",
       " '12 1971',\n",
       " '12 1972',\n",
       " '12 1977',\n",
       " '12 1978',\n",
       " '12 1979',\n",
       " '12 1981',\n",
       " '12 1985',\n",
       " '12 1987',\n",
       " '12 1989',\n",
       " '12 1992',\n",
       " '12 1993',\n",
       " '12 1994',\n",
       " '12 1995',\n",
       " '12 1997',\n",
       " '12 1999',\n",
       " '12 2001',\n",
       " '12 2003',\n",
       " '12 2004',\n",
       " '12 2005',\n",
       " '12 2006',\n",
       " '12 2007',\n",
       " '12 2008',\n",
       " '12 2009',\n",
       " '12 2010',\n",
       " '12 21',\n",
       " '12 23',\n",
       " '12 25',\n",
       " '12 30',\n",
       " '12 31',\n",
       " '12 392',\n",
       " '12 45',\n",
       " '12 500',\n",
       " '12 april',\n",
       " '12 august',\n",
       " '12 category',\n",
       " '12 chapters',\n",
       " '12 cm',\n",
       " '12 conference',\n",
       " '12 counties',\n",
       " '12 countries',\n",
       " '12 days',\n",
       " '12 december',\n",
       " '12 eggs',\n",
       " '12 ep',\n",
       " '12 episode',\n",
       " '12 expresses',\n",
       " '12 february',\n",
       " '12 ft',\n",
       " '12 games',\n",
       " '12 goals',\n",
       " '12 hours',\n",
       " '12 january',\n",
       " '12 july',\n",
       " '12 june',\n",
       " '12 km',\n",
       " '12 lrb',\n",
       " '12 meters',\n",
       " '12 mexico',\n",
       " '12 mi',\n",
       " '12 miles',\n",
       " '12 minutes',\n",
       " '12 months',\n",
       " '12 ndash',\n",
       " '12 november',\n",
       " '12 october',\n",
       " '12 rounds',\n",
       " '12 rrb',\n",
       " '12 schools',\n",
       " '12 seasons',\n",
       " '12 senior',\n",
       " '12 september',\n",
       " '12 slower',\n",
       " '12 songs',\n",
       " '12 teams',\n",
       " '12 times',\n",
       " '12 titles',\n",
       " '12 tops',\n",
       " '12 weeks',\n",
       " '12 years',\n",
       " '120',\n",
       " '120 000',\n",
       " '120 1960s',\n",
       " '120 km',\n",
       " '120 lrb',\n",
       " '120 metres',\n",
       " '120 miles',\n",
       " '120 mph',\n",
       " '120 years',\n",
       " '1200',\n",
       " '1200 rrb',\n",
       " '1200 times',\n",
       " '1200 utc',\n",
       " '1200 years',\n",
       " '1201',\n",
       " '1202',\n",
       " '12033',\n",
       " '12033 series',\n",
       " '1204',\n",
       " '1205',\n",
       " '1206',\n",
       " '1207',\n",
       " '1207 rrb',\n",
       " '1208',\n",
       " '1209',\n",
       " '1209 1229',\n",
       " '121',\n",
       " '121 hermione',\n",
       " '121 rrb',\n",
       " '1210',\n",
       " '1212',\n",
       " '1213',\n",
       " '1214',\n",
       " '1214 rrb',\n",
       " '1215',\n",
       " '1215 rrb',\n",
       " '1216',\n",
       " '1217',\n",
       " '1218',\n",
       " '1219',\n",
       " '121st',\n",
       " '122',\n",
       " '122 km',\n",
       " '1220',\n",
       " '1220 1258',\n",
       " '1220 rrb',\n",
       " '1221',\n",
       " '1221 amor',\n",
       " '1222',\n",
       " '1223',\n",
       " '1225',\n",
       " '1225 rrb',\n",
       " '1226',\n",
       " '1227',\n",
       " '1229',\n",
       " '1229 rrb',\n",
       " '123',\n",
       " '123 parks',\n",
       " '1230',\n",
       " '1230 rrb',\n",
       " '1232',\n",
       " '1234',\n",
       " '1235',\n",
       " '1236',\n",
       " '1237',\n",
       " '1237 rrb',\n",
       " '1238',\n",
       " '1239',\n",
       " '124',\n",
       " '124 lrb',\n",
       " '124 rrb',\n",
       " '1241',\n",
       " '1242',\n",
       " '1243',\n",
       " '1244',\n",
       " '1245',\n",
       " '1246',\n",
       " '1249',\n",
       " '125',\n",
       " '125 000',\n",
       " '125 km',\n",
       " '125 lrb',\n",
       " '125 mph',\n",
       " '125 years',\n",
       " '1250',\n",
       " '1252',\n",
       " '1257',\n",
       " '1258',\n",
       " '1259',\n",
       " '126',\n",
       " '1260',\n",
       " '1260 1264',\n",
       " '1260 alfonso',\n",
       " '1261',\n",
       " '1263',\n",
       " '1264',\n",
       " '1265',\n",
       " '1266',\n",
       " '1267',\n",
       " '1268',\n",
       " '1269',\n",
       " '127',\n",
       " '127 rrb',\n",
       " '1270',\n",
       " '1270 rrb',\n",
       " '1272',\n",
       " '1272 rrb',\n",
       " '1273',\n",
       " '1274',\n",
       " '1276',\n",
       " '1276 rrb',\n",
       " '1278',\n",
       " '1279',\n",
       " '1279 rrb',\n",
       " '128',\n",
       " '128 192',\n",
       " '128 bits',\n",
       " '128 lrb',\n",
       " '128 mb',\n",
       " '1280',\n",
       " '1282',\n",
       " '1284',\n",
       " '1285',\n",
       " '1286',\n",
       " '1288',\n",
       " '1289',\n",
       " '1289 rrb',\n",
       " '129',\n",
       " '129 rrb',\n",
       " '129 seats',\n",
       " '1290',\n",
       " '1290 rrb',\n",
       " '1293',\n",
       " '1294',\n",
       " '1294 lrb',\n",
       " '1295',\n",
       " '1295 rrb',\n",
       " '1296',\n",
       " '1297',\n",
       " '1298',\n",
       " '1298 rrb',\n",
       " '1299',\n",
       " '12lda28c',\n",
       " '12lda28c cylinder',\n",
       " ...]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "552c332e67cc43f197f3a78317f5ee8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=333414.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9d34ba5a624c6fa290ab8e56b95ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=83354.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_text_train=[]\n",
    "tokenized_text_test=[]\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "for text in tqdm(X_train):\n",
    "    tokens_in_text = word_tokenize(text)\n",
    "    tokens_in_text = [word for word in tokens_in_text if word.lower() not in stopWords]\n",
    "    tokenized_text_train.append(tokens_in_text)\n",
    "    \n",
    "for text in tqdm(X_test):\n",
    "    tokens_in_text = word_tokenize(text)\n",
    "    tokens_in_text = [word for word in tokens_in_text if word.lower() not in stopWords]\n",
    "    tokenized_text_test.append(tokens_in_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14256286, 24263135)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(vector_size=100,window=2,min_count=100,seed= RANDOM_SEED,workers=4)\n",
    "model.build_vocab(tokenized_text_train)\n",
    "model.train(tokenized_text_train,total_examples=model.corpus_count,epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dense_features(tokenized_text,word_vectors):\n",
    "    dense_list=[]\n",
    "    words=[]\n",
    "    for _ in tokenized_text: \n",
    "        words =[word for word in _ if word in word_vectors.key_to_index]\n",
    "        \n",
    "        if len(words) >0:\n",
    "            dense_list.append(np.mean(word_vectors[words],axis=0))\n",
    "            \n",
    "        else: \n",
    "            dense_list.append(np.zeros(word_vectors.vector_size))\n",
    "            \n",
    "    return np.array(dense_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wv = generate_dense_features(tokenized_text_train,word_vectors)\n",
    "X_test_wv = generate_dense_features(tokenized_text_test,word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Word's Difficulty Considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"couldn't\",\n",
       " 'dump',\n",
       " 'burst',\n",
       " 'guard',\n",
       " 'pork',\n",
       " 'proud',\n",
       " 'rat',\n",
       " 'toward',\n",
       " 'runner',\n",
       " 'proper',\n",
       " 'firearm',\n",
       " 'army',\n",
       " 'still',\n",
       " 'manger',\n",
       " 'minute',\n",
       " 'rice',\n",
       " 'firecracker',\n",
       " 'workman',\n",
       " 'cover',\n",
       " 'join',\n",
       " 'balloon',\n",
       " 'ought',\n",
       " 'couch',\n",
       " 'drag',\n",
       " 'flutter',\n",
       " 'hind',\n",
       " 'loaf',\n",
       " 'pint',\n",
       " 'could',\n",
       " 'sure',\n",
       " 'frighten',\n",
       " 'crossing',\n",
       " 'us',\n",
       " 'cardboard',\n",
       " 'feather',\n",
       " 'sign',\n",
       " 'heart',\n",
       " 'rug',\n",
       " 'click',\n",
       " 'tape',\n",
       " 'lost',\n",
       " 'apiece',\n",
       " 'pull',\n",
       " 'canoe',\n",
       " 'cake',\n",
       " 'elder',\n",
       " 'magic',\n",
       " 'April',\n",
       " 'shaking',\n",
       " 'taught',\n",
       " 'has',\n",
       " 'weaken',\n",
       " 'billboard',\n",
       " 'kick',\n",
       " 'reader',\n",
       " 'interesting',\n",
       " 'unhappy',\n",
       " 'sly',\n",
       " 'soil',\n",
       " 'defense',\n",
       " 'earn',\n",
       " 'wildcat',\n",
       " 'coast',\n",
       " 'quarter',\n",
       " 'necktie',\n",
       " 'visit',\n",
       " 'both',\n",
       " 'pave',\n",
       " 'running',\n",
       " 'doorbell',\n",
       " 'thimble',\n",
       " 'Monday',\n",
       " 'sir',\n",
       " 'sleeve',\n",
       " 'mailman',\n",
       " 'jacks',\n",
       " 'shadow',\n",
       " 'instead',\n",
       " 'term',\n",
       " 'month',\n",
       " 'cowardly',\n",
       " 'law',\n",
       " 'scream',\n",
       " 'third',\n",
       " 'jellyfish',\n",
       " 'wore',\n",
       " 'possible',\n",
       " 'reach',\n",
       " 'vessel',\n",
       " 'twenty',\n",
       " 'say',\n",
       " 'sail',\n",
       " 'at',\n",
       " 'fresh',\n",
       " 'caterpillar',\n",
       " 'meat',\n",
       " 'shiny',\n",
       " 'flight',\n",
       " 'beg',\n",
       " 'sling',\n",
       " 'fisherman',\n",
       " \"we've\",\n",
       " 'lives',\n",
       " 'elf',\n",
       " 'silver',\n",
       " 'guy',\n",
       " 'flat',\n",
       " 'blame',\n",
       " 'suck',\n",
       " 'son',\n",
       " 'bunny',\n",
       " 'six',\n",
       " 'hey',\n",
       " 'sigh',\n",
       " 'stem',\n",
       " 'blew',\n",
       " 'bib',\n",
       " 'bud',\n",
       " 'fiddle',\n",
       " 'locomotive',\n",
       " 'lose',\n",
       " 'wag',\n",
       " 'catsup',\n",
       " 'ninety',\n",
       " 'hoof',\n",
       " 'went',\n",
       " 'false',\n",
       " 'count',\n",
       " 'pigeon',\n",
       " 'desk',\n",
       " 'hopeful',\n",
       " 'steeple',\n",
       " 'sandwich',\n",
       " 'drown',\n",
       " 'deposit',\n",
       " 'star',\n",
       " 'afterward ',\n",
       " 'Mrs.',\n",
       " 'blacksmith',\n",
       " 'hose',\n",
       " 'poem',\n",
       " 'sore',\n",
       " 'store',\n",
       " 'business',\n",
       " 'carriage',\n",
       " 'rumble',\n",
       " 'hang',\n",
       " 'branch',\n",
       " 'pat',\n",
       " 'ice',\n",
       " 'customer',\n",
       " 'arrived',\n",
       " 'dresser',\n",
       " 'obey',\n",
       " 'because',\n",
       " 'doll',\n",
       " 'hundred',\n",
       " 'sweetness',\n",
       " 'riddle',\n",
       " 'mountain',\n",
       " 'aloud',\n",
       " 'carpenter',\n",
       " 'goose',\n",
       " 'eastern',\n",
       " 'unfinished',\n",
       " 'hasty',\n",
       " 'question',\n",
       " 'cabbage',\n",
       " 'catfish',\n",
       " 'throat',\n",
       " 'bit',\n",
       " 'million',\n",
       " 'dream',\n",
       " 'ladies',\n",
       " 'hen',\n",
       " 'none',\n",
       " 'gasoline',\n",
       " 'sin',\n",
       " 'bracelet',\n",
       " 'nowhere',\n",
       " 'thrown',\n",
       " 'acre ',\n",
       " 'smell',\n",
       " 'hug',\n",
       " 'mother',\n",
       " 'dangerous',\n",
       " 'banker',\n",
       " 'kind',\n",
       " 'rid',\n",
       " 'thin',\n",
       " 'hall',\n",
       " 'given',\n",
       " 'owner',\n",
       " 'everything',\n",
       " 'blessing',\n",
       " \"I've\",\n",
       " 'lash',\n",
       " 'part',\n",
       " 'artist',\n",
       " 'crop',\n",
       " 'sweat',\n",
       " 'upon',\n",
       " 'torn',\n",
       " 'eyebrow',\n",
       " 'aid ',\n",
       " 'enjoy',\n",
       " 'dove',\n",
       " 'milkman',\n",
       " 'wayside',\n",
       " 'painful',\n",
       " 'playmate',\n",
       " 'seek',\n",
       " 'library',\n",
       " 'pink',\n",
       " 'stable',\n",
       " 'bum',\n",
       " 'knives',\n",
       " 'ten',\n",
       " 'stood',\n",
       " 'take',\n",
       " 'off',\n",
       " 'willing',\n",
       " 'beard',\n",
       " 'beer',\n",
       " 'orange',\n",
       " 'stung',\n",
       " 'mighty',\n",
       " 'upward',\n",
       " 'cookie',\n",
       " 'wide',\n",
       " 'dish',\n",
       " 'thought',\n",
       " 'near',\n",
       " 'cry',\n",
       " 'overnight',\n",
       " 'shy',\n",
       " 'puff',\n",
       " 'upset',\n",
       " 'lunch',\n",
       " 'thunder',\n",
       " 'sick',\n",
       " 'stork',\n",
       " 'gun',\n",
       " 'badge',\n",
       " 'policeman',\n",
       " 'report',\n",
       " 'penny',\n",
       " 'diver',\n",
       " 'begged',\n",
       " 'sword',\n",
       " 'homely',\n",
       " 'crazy',\n",
       " 'watermelon',\n",
       " 'butterscotch',\n",
       " 'sled',\n",
       " 'beefsteak',\n",
       " 'snowflake',\n",
       " \"we'll\",\n",
       " 'canal',\n",
       " 'finger',\n",
       " 'breathe',\n",
       " 'narrow',\n",
       " 'tailor',\n",
       " 'ho',\n",
       " 'roar',\n",
       " 'morning',\n",
       " 'fur',\n",
       " 'cheat',\n",
       " 'several',\n",
       " 'worker',\n",
       " 'address ',\n",
       " 'silly',\n",
       " 'oar',\n",
       " 'wooden',\n",
       " 'garbage',\n",
       " 'send',\n",
       " 'thankful',\n",
       " 'service',\n",
       " 'healthy',\n",
       " 'following',\n",
       " 'patch',\n",
       " 'sleep',\n",
       " 'early',\n",
       " 'drain',\n",
       " 'hers',\n",
       " 'print',\n",
       " 'became',\n",
       " 'dart',\n",
       " 'scrub',\n",
       " 'cellar',\n",
       " 'swell',\n",
       " 'ticket',\n",
       " 'much',\n",
       " 'luck',\n",
       " 'old-fashioned',\n",
       " 'royal',\n",
       " 'this',\n",
       " 'cozy',\n",
       " 'sudden',\n",
       " 'rough',\n",
       " 'teaspoon',\n",
       " 'gallon',\n",
       " 'rabbit',\n",
       " 'blackberry',\n",
       " 'view',\n",
       " 'chain',\n",
       " 'broad',\n",
       " 'pea',\n",
       " 'plenty',\n",
       " 'Indian',\n",
       " 'dairy',\n",
       " 'squirrel',\n",
       " 'November',\n",
       " 'sank',\n",
       " 'cereal',\n",
       " 'Negro',\n",
       " 'far',\n",
       " 'schoolboy',\n",
       " 'copper',\n",
       " 'each',\n",
       " 'spoil',\n",
       " 'hush',\n",
       " 'lot',\n",
       " 'hilly',\n",
       " 'will',\n",
       " 'ore',\n",
       " 'chin',\n",
       " 'rack',\n",
       " 'fruit',\n",
       " 'flashlight',\n",
       " 'salt',\n",
       " 'peppermint',\n",
       " 'night',\n",
       " 'bill',\n",
       " 'shot',\n",
       " 'learn',\n",
       " 'sunrise',\n",
       " 'believe',\n",
       " 'crash',\n",
       " 'Christmas',\n",
       " 'skin',\n",
       " 'western',\n",
       " 'uniform',\n",
       " 'many',\n",
       " 'rain',\n",
       " 'rock',\n",
       " 'help',\n",
       " 'see',\n",
       " 'wise',\n",
       " 'onion',\n",
       " 'forehead',\n",
       " 'daily',\n",
       " 'check',\n",
       " 'rattle',\n",
       " 'sissy',\n",
       " 'owl',\n",
       " 'free',\n",
       " 'land',\n",
       " 'trunk',\n",
       " 'review',\n",
       " \"they'll\",\n",
       " 'ditch',\n",
       " 'voice',\n",
       " 'queer',\n",
       " 'bran',\n",
       " 'lumber',\n",
       " 'church',\n",
       " 'above ',\n",
       " 'bay',\n",
       " 'speech',\n",
       " 'eagle',\n",
       " 'everywhere',\n",
       " 'youngster',\n",
       " 'butter',\n",
       " 'redbreast',\n",
       " 'engineer',\n",
       " 'squeak',\n",
       " 'lawn',\n",
       " 'bullet',\n",
       " \"didn't\",\n",
       " 'nut',\n",
       " 'candle',\n",
       " 'icy',\n",
       " 'light',\n",
       " 'fancy',\n",
       " 'due',\n",
       " 'ink',\n",
       " 'ate',\n",
       " 'them',\n",
       " 'slowly',\n",
       " 'ache ',\n",
       " 'connect',\n",
       " 'out',\n",
       " 'shop',\n",
       " 'stick',\n",
       " 'gentleman',\n",
       " \"you'll\",\n",
       " 'United',\n",
       " 'sweetheart',\n",
       " 'middle',\n",
       " \"hasn't\",\n",
       " 'climb',\n",
       " 'living',\n",
       " 'radish',\n",
       " 'Sunday',\n",
       " 'tardy',\n",
       " 'cup',\n",
       " 'stand',\n",
       " 'quickly',\n",
       " 'swallow',\n",
       " 'net',\n",
       " 'baseball',\n",
       " 'corner',\n",
       " 'select',\n",
       " 'umbrella',\n",
       " 'steak',\n",
       " 'boss',\n",
       " 'wheat',\n",
       " 'harp',\n",
       " 'gentlemen',\n",
       " 'excuse',\n",
       " 'rainy',\n",
       " 'fret',\n",
       " 'nail',\n",
       " 'cute',\n",
       " 'plant',\n",
       " 'cause',\n",
       " 'fry',\n",
       " 'cradle',\n",
       " 'mostly',\n",
       " 'dame',\n",
       " 'magazine',\n",
       " 'herself',\n",
       " 'appear',\n",
       " 'chair',\n",
       " 'hunger',\n",
       " 'bathtub',\n",
       " 'garage',\n",
       " 'campfire',\n",
       " 'fever',\n",
       " 'cool',\n",
       " 'dandy',\n",
       " 'it',\n",
       " 'cost',\n",
       " 'pay',\n",
       " 'cooking',\n",
       " 'joy',\n",
       " 'snuff',\n",
       " 'bookkeeper',\n",
       " 'race',\n",
       " 'swimming',\n",
       " 'checkers',\n",
       " 'goodbye',\n",
       " 'postman',\n",
       " 'hour',\n",
       " 'clever',\n",
       " 'remain',\n",
       " 'hare',\n",
       " 'washer',\n",
       " \"I'm\",\n",
       " 'toe',\n",
       " 'slippery',\n",
       " 'rocket',\n",
       " 'camel',\n",
       " 'yourself',\n",
       " 'cupful',\n",
       " 'inch',\n",
       " 'cow',\n",
       " 'music',\n",
       " 'dried',\n",
       " 'aboard',\n",
       " 'fight',\n",
       " 'seam',\n",
       " 'skip',\n",
       " 'praise',\n",
       " \"you're\",\n",
       " 'skate',\n",
       " 'daisy',\n",
       " 'short',\n",
       " 'return',\n",
       " 'correct',\n",
       " 'defend',\n",
       " 'robin',\n",
       " 'tried',\n",
       " 'borrow',\n",
       " 'graveyard',\n",
       " 'swat',\n",
       " 'mice',\n",
       " 'peach',\n",
       " 'trade',\n",
       " 'big',\n",
       " 'lesson',\n",
       " 'punish',\n",
       " 'belt',\n",
       " 'outlaw',\n",
       " 'woke',\n",
       " 'gallop',\n",
       " 'even',\n",
       " 'hurrah',\n",
       " 'hook',\n",
       " 'together',\n",
       " 'ear',\n",
       " 'sprinkle',\n",
       " 'goody',\n",
       " 'rubbed',\n",
       " 'carelessness',\n",
       " 'hitch',\n",
       " \"that's\",\n",
       " 'direction',\n",
       " 'feast',\n",
       " 'crowded',\n",
       " 'explain',\n",
       " 'lonely',\n",
       " 'wagon',\n",
       " 'back',\n",
       " 'stock',\n",
       " 'govern',\n",
       " 'under',\n",
       " 'slate',\n",
       " 'plow',\n",
       " 'tune',\n",
       " 'learned',\n",
       " 'dive',\n",
       " 'wrapped',\n",
       " 'led',\n",
       " 'cannon',\n",
       " 'weak',\n",
       " 'whenever',\n",
       " 'lantern',\n",
       " 'made',\n",
       " 'answer',\n",
       " 'colored',\n",
       " 'blast',\n",
       " 'slid',\n",
       " 'squash',\n",
       " 'birth',\n",
       " 'cut',\n",
       " 'shears',\n",
       " 'roast',\n",
       " 'move',\n",
       " 'remember',\n",
       " 'quit',\n",
       " 'like',\n",
       " 'ran',\n",
       " 'limp',\n",
       " 'always',\n",
       " 'glance',\n",
       " 'lick',\n",
       " 'sorrow',\n",
       " 'know',\n",
       " 'lamp',\n",
       " 'frown',\n",
       " 'acts ',\n",
       " 'classroom',\n",
       " 'close',\n",
       " 'dirt',\n",
       " 'copy',\n",
       " 'holder',\n",
       " 'some',\n",
       " 'stall',\n",
       " 'tunnel',\n",
       " 'week',\n",
       " 'marble',\n",
       " 'name',\n",
       " 'pitch',\n",
       " 'end',\n",
       " 'fourth',\n",
       " 'leader',\n",
       " 'accident ',\n",
       " 'on',\n",
       " 'neighborhood',\n",
       " 'stripes',\n",
       " 'open',\n",
       " 'smoke',\n",
       " 'neighbor',\n",
       " 'nickel',\n",
       " 'tongue',\n",
       " 'truly',\n",
       " 'switch',\n",
       " 'late',\n",
       " 'rule',\n",
       " 'plug',\n",
       " 'cork',\n",
       " 'honor',\n",
       " 'pineapple',\n",
       " 'she',\n",
       " 'snapping',\n",
       " \"I'd\",\n",
       " 'winner',\n",
       " 'dying',\n",
       " 'sleigh',\n",
       " 'fib',\n",
       " 'horse',\n",
       " 'busy',\n",
       " 'kneel',\n",
       " 'mistake',\n",
       " \"isn't\",\n",
       " 'follow',\n",
       " 'ask',\n",
       " 'round',\n",
       " 'buggy',\n",
       " 'another',\n",
       " 'chicken',\n",
       " 'grasshopper',\n",
       " 'sole',\n",
       " 'nod',\n",
       " 'hog',\n",
       " 'badly',\n",
       " 'tablecloth',\n",
       " 'errand',\n",
       " 'towards',\n",
       " 'afar ',\n",
       " 'my',\n",
       " 'knife',\n",
       " 'mouth',\n",
       " 'greet',\n",
       " 'satin',\n",
       " 'grease',\n",
       " 'blank',\n",
       " 'suffer',\n",
       " 'doorstep',\n",
       " 'noon',\n",
       " 'pepper',\n",
       " 'hatchet',\n",
       " 'toy',\n",
       " 'sap',\n",
       " 'drub',\n",
       " 'least',\n",
       " 'onward',\n",
       " 'coin',\n",
       " 'stamp',\n",
       " 'ox',\n",
       " 'dark',\n",
       " 'dock',\n",
       " 'cuff',\n",
       " 'flies',\n",
       " \"it's\",\n",
       " 'most',\n",
       " 'uncle',\n",
       " 'keen',\n",
       " 'nerve',\n",
       " 'poke',\n",
       " 'gain',\n",
       " 'base',\n",
       " 'frost',\n",
       " 'path',\n",
       " 'chipmunk',\n",
       " 'me',\n",
       " 'whip',\n",
       " 'dozen',\n",
       " 'extra',\n",
       " 'prove',\n",
       " 'or',\n",
       " 'tire',\n",
       " 'brick',\n",
       " 'gown',\n",
       " 'thirty',\n",
       " 'brother',\n",
       " 'pistol',\n",
       " 'bedroom',\n",
       " 'well',\n",
       " \"who'll\",\n",
       " 'grab',\n",
       " 'mew',\n",
       " 'platform',\n",
       " 'letting',\n",
       " 'overturn',\n",
       " 'taste',\n",
       " 'counter',\n",
       " 'beneath',\n",
       " 'spank',\n",
       " 'dawn',\n",
       " 'held',\n",
       " 'sat',\n",
       " 'fellow',\n",
       " 'kindly',\n",
       " 'bug',\n",
       " 'test',\n",
       " 'lend',\n",
       " 'sip',\n",
       " 'hope',\n",
       " 'seed',\n",
       " 'he',\n",
       " 'east',\n",
       " 'pair',\n",
       " 'lean',\n",
       " 'twig',\n",
       " 'river',\n",
       " 'soon',\n",
       " 'useful',\n",
       " 'pussycat',\n",
       " 'gone',\n",
       " 'hot',\n",
       " 'clothing',\n",
       " 'wicked',\n",
       " 'story',\n",
       " 'spirit',\n",
       " 'lonesome',\n",
       " 'boxes',\n",
       " 'painter',\n",
       " 'touch',\n",
       " 'deal',\n",
       " 'silent',\n",
       " 'golden',\n",
       " 'footprint',\n",
       " 'rag',\n",
       " 'helper',\n",
       " 'remind',\n",
       " 'hood',\n",
       " 'tear',\n",
       " 'pound',\n",
       " 'lime',\n",
       " 'strap',\n",
       " 'vine',\n",
       " 'wrong',\n",
       " 'chilly',\n",
       " 'fireworks',\n",
       " 'only',\n",
       " 'shall',\n",
       " 'sixteen',\n",
       " 'beautiful',\n",
       " 'note',\n",
       " 'work',\n",
       " 'hell',\n",
       " 'dig',\n",
       " 'yet',\n",
       " 'laugh',\n",
       " 'gift',\n",
       " 'around',\n",
       " 'home',\n",
       " 'winter',\n",
       " 'far-off',\n",
       " 'favor',\n",
       " 'barber',\n",
       " 'clump',\n",
       " 'house',\n",
       " 'sandy',\n",
       " 'built',\n",
       " 'prick',\n",
       " 'curve',\n",
       " 'bought',\n",
       " 'chatter',\n",
       " 'broke',\n",
       " 'fairy',\n",
       " 'pleasant',\n",
       " 'course',\n",
       " 'rich',\n",
       " 'wall',\n",
       " 'postage',\n",
       " 'thief',\n",
       " 'straight',\n",
       " 'sun',\n",
       " 'party',\n",
       " 'cranberry',\n",
       " 'wheel',\n",
       " 'churn',\n",
       " 'rubbish',\n",
       " 'coffeepot',\n",
       " 'very',\n",
       " 'baa',\n",
       " 'turtle',\n",
       " 'but',\n",
       " 'delight',\n",
       " 'remove',\n",
       " 'grape',\n",
       " 'tap',\n",
       " 'tight',\n",
       " 'chart',\n",
       " 'trace',\n",
       " 'depend',\n",
       " 'which',\n",
       " 'so',\n",
       " 'drawing',\n",
       " 'canary',\n",
       " 'stitch',\n",
       " 'downtown',\n",
       " 'surface',\n",
       " 'hasten',\n",
       " 'forgive',\n",
       " 'dead',\n",
       " 'until',\n",
       " 'dare',\n",
       " 'driven',\n",
       " 'number',\n",
       " 'wave',\n",
       " 'ashes',\n",
       " 'haystack',\n",
       " 'silk',\n",
       " 'dull',\n",
       " 'room',\n",
       " 'baking',\n",
       " 'soak',\n",
       " 'steamboat',\n",
       " 'shovel',\n",
       " 'croak',\n",
       " 'till',\n",
       " 'roller',\n",
       " 'bulb',\n",
       " 'cab',\n",
       " 'sill',\n",
       " 'dust',\n",
       " 'forth',\n",
       " 'line',\n",
       " 'liking',\n",
       " 'lace',\n",
       " 'repay',\n",
       " 'share',\n",
       " 'soup',\n",
       " 'bounce',\n",
       " 'spill',\n",
       " 'told',\n",
       " 'watch',\n",
       " 'ago ',\n",
       " 'beauty',\n",
       " 'ranch',\n",
       " 'savings',\n",
       " 'school',\n",
       " 'wiggle',\n",
       " 'wood',\n",
       " 'clown',\n",
       " 'apartment',\n",
       " 'grove',\n",
       " 'slap',\n",
       " 'throw',\n",
       " 'organ',\n",
       " 'knew',\n",
       " 'maypole',\n",
       " 'spring',\n",
       " 'cart',\n",
       " 'deaf',\n",
       " 'elephant',\n",
       " 'jail',\n",
       " 'barely',\n",
       " 'song',\n",
       " 'paint',\n",
       " 'vote',\n",
       " 'Thursday',\n",
       " 'invite',\n",
       " 'something',\n",
       " 'anyhow',\n",
       " 'less',\n",
       " 'attend',\n",
       " 'below',\n",
       " 'cage',\n",
       " 'bun',\n",
       " 'deer',\n",
       " 'and',\n",
       " 'man',\n",
       " 'sang',\n",
       " 'cone',\n",
       " 'strip',\n",
       " 'pack',\n",
       " 'fat',\n",
       " 'pants',\n",
       " 'cooler',\n",
       " 'tablespoon',\n",
       " 'shore',\n",
       " 'cheek',\n",
       " 'lead',\n",
       " 'lovely',\n",
       " 'bedbug',\n",
       " 'oats',\n",
       " 'beggar',\n",
       " 'wrote',\n",
       " 'peck',\n",
       " 'eighth',\n",
       " 'destroy',\n",
       " 'coal',\n",
       " 'was',\n",
       " 'fare',\n",
       " 'lie',\n",
       " 'polite',\n",
       " 'one',\n",
       " 'thread',\n",
       " 'custard',\n",
       " 'horseshoe',\n",
       " 'neither',\n",
       " 'sting',\n",
       " 'word',\n",
       " 'ape',\n",
       " 'quilt',\n",
       " 'swing',\n",
       " 'loaves',\n",
       " 'sink',\n",
       " 'drop',\n",
       " 'joyful',\n",
       " 'game',\n",
       " 'cord',\n",
       " 'comfort',\n",
       " 'lad',\n",
       " 'call',\n",
       " 'lemon',\n",
       " 'coach',\n",
       " 'surprise',\n",
       " 'any',\n",
       " 'happy',\n",
       " 'am',\n",
       " 'gear',\n",
       " 'whirl',\n",
       " 'choose',\n",
       " 'lord',\n",
       " 'hive',\n",
       " 'field',\n",
       " 'cent',\n",
       " 'clap',\n",
       " 'girl',\n",
       " 'skirt',\n",
       " 'cheer',\n",
       " 'sing',\n",
       " 'single',\n",
       " 'hate',\n",
       " 'wild',\n",
       " 'bean',\n",
       " 'fort',\n",
       " 'grocery',\n",
       " 'same',\n",
       " 'palace',\n",
       " \"shan't\",\n",
       " 'roll',\n",
       " 'rejoice',\n",
       " 'sickness',\n",
       " 'coffee',\n",
       " 'tablet',\n",
       " 'tank',\n",
       " 'yell',\n",
       " 'tiptoe',\n",
       " 'iron',\n",
       " 'housetop',\n",
       " 'judge',\n",
       " 'kid',\n",
       " 'hillside',\n",
       " 'nor',\n",
       " 'alike ',\n",
       " 'scare',\n",
       " 'dear',\n",
       " 'groan',\n",
       " 'crept',\n",
       " 'oak',\n",
       " 'through',\n",
       " 'clock',\n",
       " 'scab',\n",
       " 'misspell',\n",
       " 'grandpa',\n",
       " 'sitting',\n",
       " 'howl',\n",
       " 'movies',\n",
       " 'slide',\n",
       " 'junior',\n",
       " 'conductor',\n",
       " 'among',\n",
       " 'stopped',\n",
       " 'were',\n",
       " 'toadstool',\n",
       " 'present',\n",
       " 'haul',\n",
       " 'shave',\n",
       " 'good',\n",
       " 'square',\n",
       " 'wring',\n",
       " 'carve',\n",
       " 'hotel',\n",
       " 'unless',\n",
       " 'blow',\n",
       " 'bar',\n",
       " 'door',\n",
       " 'bold',\n",
       " 'window',\n",
       " 'day',\n",
       " 'dollar',\n",
       " 'mask',\n",
       " 'playhouse',\n",
       " 'angel',\n",
       " 'dad',\n",
       " 'purse',\n",
       " 'yarn',\n",
       " 'steer',\n",
       " 'valuable',\n",
       " 'tack',\n",
       " 'battle',\n",
       " 'live',\n",
       " 'stack',\n",
       " 'fence',\n",
       " 'eat',\n",
       " 'notice',\n",
       " 'crab',\n",
       " 'tow',\n",
       " 'task',\n",
       " 'tired',\n",
       " 'pure',\n",
       " 'humble',\n",
       " 'dancing',\n",
       " 'carpet',\n",
       " 'pan',\n",
       " 'elbow',\n",
       " 'cross',\n",
       " 'barn',\n",
       " 'change',\n",
       " 'pupil',\n",
       " 'case',\n",
       " 'an',\n",
       " ...}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Basic english words\n",
    "dale_chall = pd.read_csv('dale_chall.txt',delimiter='\\t',header=None,names=['word'])\n",
    "dale = set(dale_chall['word'].values)\n",
    "dale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concreteness rating\n",
    "concrete_df = pd.read_csv('Concreteness_ratings_Brysbaert_et_al_BRM.txt',delimiter='\\t')\n",
    "concreteset=(concrete_df['Word'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Alternative.spelling</th>\n",
       "      <th>Freq_pm</th>\n",
       "      <th>Dom_PoS_SUBTLEX</th>\n",
       "      <th>Nletters</th>\n",
       "      <th>Nphon</th>\n",
       "      <th>Nsyll</th>\n",
       "      <th>Lemma_highest_PoS</th>\n",
       "      <th>AoA_Kup</th>\n",
       "      <th>Perc_known</th>\n",
       "      <th>AoA_Kup_lem</th>\n",
       "      <th>Perc_known_lem</th>\n",
       "      <th>AoA_Bird_lem</th>\n",
       "      <th>AoA_Bristol_lem</th>\n",
       "      <th>AoA_Cort_lem</th>\n",
       "      <th>AoA_Schock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>20415.27</td>\n",
       "      <td>Article</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>2.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aardvark</td>\n",
       "      <td>aardvark</td>\n",
       "      <td>0.41</td>\n",
       "      <td>Noun</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>aardvark</td>\n",
       "      <td>9.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>9.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abacus</td>\n",
       "      <td>abacus</td>\n",
       "      <td>0.24</td>\n",
       "      <td>Noun</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>abacus</td>\n",
       "      <td>8.69</td>\n",
       "      <td>0.65</td>\n",
       "      <td>8.69</td>\n",
       "      <td>0.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abacuses</td>\n",
       "      <td>abacuses</td>\n",
       "      <td>0.02</td>\n",
       "      <td>Noun</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>abacus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.69</td>\n",
       "      <td>0.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abalone</td>\n",
       "      <td>abalone</td>\n",
       "      <td>0.51</td>\n",
       "      <td>Verb</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>abalone</td>\n",
       "      <td>12.23</td>\n",
       "      <td>0.72</td>\n",
       "      <td>12.23</td>\n",
       "      <td>0.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word Alternative.spelling   Freq_pm Dom_PoS_SUBTLEX  Nletters  Nphon  \\\n",
       "0         a                    a  20415.27         Article         1      1   \n",
       "1  aardvark             aardvark      0.41            Noun         8      7   \n",
       "2    abacus               abacus      0.24            Noun         6      6   \n",
       "3  abacuses             abacuses      0.02            Noun         8      9   \n",
       "4   abalone              abalone      0.51            Verb         7      7   \n",
       "\n",
       "   Nsyll Lemma_highest_PoS  AoA_Kup  Perc_known  AoA_Kup_lem  Perc_known_lem  \\\n",
       "0      1                 a     2.89        1.00         2.89            1.00   \n",
       "1      2          aardvark     9.89        1.00         9.89            1.00   \n",
       "2      3            abacus     8.69        0.65         8.69            0.65   \n",
       "3      4            abacus      NaN         NaN         8.69            0.65   \n",
       "4      4           abalone    12.23        0.72        12.23            0.72   \n",
       "\n",
       "   AoA_Bird_lem  AoA_Bristol_lem  AoA_Cort_lem  AoA_Schock  \n",
       "0          3.16              NaN           NaN         NaN  \n",
       "1           NaN              NaN           NaN         NaN  \n",
       "2           NaN              NaN           NaN         NaN  \n",
       "3           NaN              NaN           NaN         NaN  \n",
       "4           NaN              NaN           NaN         NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AoA\n",
    "#Perc_known_lem, AoA_Kup_lem\n",
    "AoA = pd.read_csv('AoA_51715_words.csv',encoding = 'unicode_escape')\n",
    "AoA_set = set(AoA['Word'].values)\n",
    "AoA.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word = set(word_vectors.index_to_key) #around 6k words in the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2623"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_word.intersection(concreteset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-e7a08ad5c3a7>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['AoA_Kup_lem'][i] = 3\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "word_list = []\n",
    "for word in model_word: \n",
    "    word_list.append((word,lemmatizer.lemmatize(word.lower())))\n",
    "df = pd.DataFrame(word_list,columns=['Original','word'])\n",
    "df = df.merge(AoA,left_on='word',right_on='Word',how='left')\n",
    "df = df[['Original','word','Perc_known','AoA_Kup_lem']]\n",
    "word_not_matched = set(df[df['Perc_known'].isnull()].word.values)\n",
    "\n",
    "for i in range(len(df)):   \n",
    "    if df['word'][i][0] in set(('0','1','2','3','4','5','6','7','8','9')) or len(df['word'][i])==1:\n",
    "        df['AoA_Kup_lem'][i] = 3\n",
    "mean_value = df['AoA_Kup_lem'].mean()\n",
    "df['AoA_Kup_lem'].fillna(value=mean_value,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>word</th>\n",
       "      <th>Perc_known</th>\n",
       "      <th>AoA_Kup_lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>troops</td>\n",
       "      <td>troop</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3521</th>\n",
       "      <td>weapon</td>\n",
       "      <td>weapon</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Original    word  Perc_known  AoA_Kup_lem\n",
       "1828   troops   troop         1.0         8.35\n",
       "3521   weapon  weapon         1.0         6.95"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.loc[df['Original']==['troops','weapons']]\n",
    "df[df['Original'].isin(['troops','weapon'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_perc_known(tokenized_text,df):\n",
    "    avg_perc_know=None\n",
    "    perc_know_list=[]\n",
    "    for _ in tokenized_text: \n",
    "        words =[word for word in _ if word in word_vectors.key_to_index]\n",
    "        \n",
    "        if len(words) >0:\n",
    "            avg_perc_know = np.mean(df[df['Original'].isin(words)]['AoA_Kup_lem'])\n",
    "            perc_know_list.append(avg_perc_know)\n",
    "        else: \n",
    "            \n",
    "            perc_know_list.append(0)\n",
    "            \n",
    "    return perc_know_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f1f90c910490>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_wv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_perc_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-a3d8efc84271>\u001b[0m in \u001b[0;36mgenerate_perc_known\u001b[0;34m(tokenized_text, df)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mavg_perc_know\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Original'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AoA_Kup_lem'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mperc_know_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_perc_know\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36misin\u001b[0;34m(self, values)\u001b[0m\n\u001b[1;32m   4683\u001b[0m         \u001b[0mName\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0manimal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4684\u001b[0m         \"\"\"\n\u001b[0;32m-> 4685\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4686\u001b[0m         return self._constructor(result, index=self.index).__finalize__(\n\u001b[1;32m   4687\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"isin\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36misin\u001b[0;34m(comps, values)\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0mcomps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_train = pd.DataFrame(X_train_wv)\n",
    "df_train['year'] = generate_perc_known(tokenized_text_train,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(X_test_wv)\n",
    "df_test['year'] = generate_perc_known(tokenized_text_test,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=RANDOM_SEED,max_iter=1000).fit(df_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,lr.predict(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_bow = DummyClassifier(strategy='uniform',random_state=RANDOM_SEED).fit(X_train_transform,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5011277203253593"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, dummy_bow.predict(X_test_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_wv = DummyClassifier(strategy='uniform',random_state=RANDOM_SEED).fit(X_train_wv,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5011277203253593"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,dummy_wv.predict(X_test_wv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_bow = LogisticRegression(random_state=RANDOM_SEED,max_iter=1000).fit(X_train_transform,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6690260815317801"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,lr_bow.predict(X_test_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_wv = LogisticRegression(random_state=RANDOM_SEED,max_iter=1000).fit(X_train_wv,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6191424526717374"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,lr_wv.predict(X_test_wv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ridge = RidgeClassifier(alpha=0.5,random_state=RANDOM_SEED,max_iter=1000).fit(X_train_transform,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6544376994505363"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,clf_ridge.predict(X_test_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ridge_wv = RidgeClassifier(alpha=0.5,random_state=RANDOM_SEED,max_iter=1000).fit(X_train_wv,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6191304556469995"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,clf_ridge_wv.predict(X_test_wv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoosting Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, \n",
    "                                 random_state=0).fit(X_train_transform, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6165510953283586"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,clf_gb.predict(X_test_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-e5fa78671d93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m clf_gb_wv = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, \n\u001b[0m\u001b[1;32m      2\u001b[0m                                  random_state=0).fit(X_train_wv, y_train)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         n_stages = self._fit_stages(\n\u001b[0m\u001b[1;32m    499\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m             \u001b[0;31m# fit next stage of trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             raw_predictions = self._fit_stage(\n\u001b[0m\u001b[1;32m    556\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m                 random_state, X_idx_sorted, X_csc, X_csr)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0m\u001b[1;32m    212\u001b[0m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1240\u001b[0m         \"\"\"\n\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1243\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf_gb_wv = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, \n",
    "                                 random_state=0).fit(X_train_wv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,clf_gb.predict(X_test_wv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_bow = RandomForestClassifier(n_estimators=500,max_depth=5,random_state=RANDOM_SEED).fit(X_train_transform,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6257408162775632"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,rf_bow.predict(X_test_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-8b7de881e9fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrf_wv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRANDOM_SEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_wv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[1;32m    387\u001b[0m                              \u001b[0;34m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'threads'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                 delayed(_parallel_build_trees)(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    864\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    166\u001b[0m                                                         indices=indices)\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    888\u001b[0m         \"\"\"\n\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m    891\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rf_wv = RandomForestClassifier(n_estimators=100,max_depth=5,random_state=RANDOM_SEED).fit(X_train_wv,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,rf_wv.predict(X_test_wv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=5,random_state=RANDOM_SEED)\n",
    "W = nmf.fit_transform(X_train_transform)\n",
    "H = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333414, 50)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_test = nmf.transform(X_test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: commune pas,france,department,region,nord,nord pas,calais department,pas calais,calais,pas\n",
      "Topic 2: lrb known,lrb called,km rrb,lrb died,german,km,ndash,rrb lrb,lrb,rrb\n",
      "Topic 3: department,region,north,aisne,aisne department,department north,north france,picardie aisne,region picardie,picardie\n",
      "Topic 4: united kingdom,county iowa,kentucky,kentucky united,city iowa,iowa,iowa united,united states,states,united\n",
      "Topic 5: references reading,references websites,list,references external,links,external links,external,notes references,notes,references\n",
      "Topic 6: basse,basse normandie,normandie,calvados department,calvados,northwest,normandie calvados,region basse,department northwest,northwest france\n",
      "Topic 7: brazilian football,player plays,player currently,lrb born,rrb japanese,japanese football,japanese,football player,football,player\n",
      "Topic 8: region france,la,aisne commune,romania,county romania,sur,le,saint,commune region,commune\n",
      "Topic 9: department,sarthe department,sarthe,region,region pays,pays la,la loire,pays,loire,la\n",
      "Topic 10: department,region,gironde department,aquitaine,southwest,gironde,region aquitaine,aquitaine gironde,department southwest,southwest france\n",
      "Topic 11: links,ian,various,available,internet,articles,website,information,references websites,websites\n",
      "Topic 12: age,death,died,century,old,king,early,later,life,years\n",
      "Topic 13: city state,state,city iowa,iowa united,iowa,largest city,largest,capital city,capital,city\n",
      "Topic 14: france,department,department picardy,picardy northern,picardy,aisne department,aisne,commune aisne,northern,northern france\n",
      "Topic 15: canton ticino,ticino,aargau switzerland,canton aargau,aargau,municipality district,municipality,switzerland,canton,district\n",
      "Topic 16: brazil,brazil national,team national,football,national football,football team,played,national team,national,team\n",
      "Topic 17: web,special,template,list,talk pages,talk,user,wikipedia,page,pages\n",
      "Topic 18: period,founded,important,ancient,century,modern,museum,natural,natural history,history\n",
      "Topic 19: city county,town,county iowa,state,county romania,county county,romania,county seat,seat,county\n",
      "Topic 20: south wales,new south,new jersey,jersey,new zealand,zealand,york city,new york,york,new\n",
      "Topic 21: km,america,area,england,london,north,west,town,east,south\n",
      "Topic 22: largest,germany,second,wrestling,championship,war ii,ii,world war,war,world\n",
      "Topic 23: different,make,term used,commonly used,use,refer,commonly,word,term,used\n",
      "Topic 24: 2006,000,area,total,2000 census,2000,census,2007 population,2007,population\n",
      "Topic 25: province hainaut,hainaut,walloon,located belgian,belgian province,municipality,belgian,municipality located,province,located\n",
      "Topic 26: february,august,april,september,march,july,january,footballer,lrb born,born\n",
      "Topic 27: rhône alpes,alpes,department,france,commune ain,department eastern,eastern france,eastern,ain department,ain\n",
      "Topic 28: northwestern france,region,calvados department,basse,basse normandie,normandie,calvados,department basse,commune calvados,normandie region\n",
      "Topic 29: form,small,type,rrb called,area,book,group,usually,lrb called,called\n",
      "Topic 30: julian,calendar rrb,rrb common,link,common year,year starting,starting,common,calendar,year\n",
      "Topic 31: department,france,south western,aquitaine,western france,western,gironde department,gironde,department aquitaine,commune gironde\n",
      "Topic 32: million people,people lrb,million,people live,live,people living,living,000 people,000,people\n",
      "Topic 33: better known,better,commonly known,commonly,rrb known,species,lrb known,best known,best,known\n",
      "Topic 34: folk,music director,song,pop,director,festival,group,video,music video,music\n",
      "Topic 35: founded,california,high school,public,high,state university,college,school,state,university\n",
      "Topic 36: small,flowering,family moved,members,moved,plants,member,genus,species,family\n",
      "Topic 37: game boy,japan,boy,mario,nintendo,games,series,video game,video,game\n",
      "Topic 38: based,club career,career statistics,club plays,statistics,career,plays,football,football club,club\n",
      "Topic 39: aube dã,department north,france,aube department,commune aube,north central,central france,aube,north,central\n",
      "Topic 40: dã,che department,department southern,commune ardã,france,ardã,ardã che,che,southern france,southern\n",
      "Topic 41: television series,american actor,actress,singer,actor,series,television,film,rrb american,american\n",
      "Topic 42: cyclone,depression,atlantic hurricane,hurricane season,atlantic,season,tropical storm,hurricane,storm,tropical\n",
      "Topic 43: short time,champion,magazine,time lrb,use,short,period,long time,long,time\n",
      "Topic 44: country,president,state,official website,government,official languages,website,languages,official language,official\n",
      "Topic 45: written,english language,means,spoken,use,french,languages,word,english,language\n",
      "Topic 46: symbol,element,rrb capital,chemical,rrb city,number,rrb lrb,rrb,lrb,lrb rrb\n",
      "Topic 47: members,alternative rock,alternative,band formed,metal band,formed,metal,rock band,rock,band\n",
      "Topic 48: rivers,located,flows,lake,valley,tributary,water,named,near,river\n",
      "Topic 49: league lrb,ice,major,professional,national hockey,nhl,hockey league,season,hockey,league\n",
      "Topic 50: 2007,2006,studio,number,2008,song,second,single,released,album\n"
     ]
    }
   ],
   "source": [
    "words = np.array(vectorizer.get_feature_names())\n",
    "for i, topic in enumerate(H):\n",
    "     print(\"Topic {}: {}\".format(i + 1, \",\".join([str(x) for x in words[topic.argsort()[-10:]]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tm = LogisticRegression(random_state=RANDOM_SEED,max_iter=1000).fit(W,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5311802672937112"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,lr_tm.predict(W_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=100, n_iter=5,random_state=RANDOM_SEED)\n",
    "X_LSI_train = svd.fit_transform(X_train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333414, 100)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_LSI_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_LSI_test = svd.transform(X_test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_lsi = LogisticRegression(random_state=RANDOM_SEED,max_iter=1000).fit(X_LSI_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5800201550015596"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,lr_lsi.predict(X_LSI_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
